{
    "Out-of-Sample Validation (DL 06)": [
        {
            "start_time": 0,
            "end_time": 30,
            "transcription": " In machine learning, like in science, the way to evaluate a model is to make new predictions and test them on new data. So in this video, we'll discuss how to make the best use of our data to experimentally validate and improve our machine learning models. Everything we'll be discussing falls under the heading of model selection, which is about picking among different machine learning approaches, which one will be best for your problem."
        },
        {
            "start_time": 30,
            "end_time": 60,
            "transcription": " The first step towards choosing a model is to determine what options are even available for solving a given problem, which leads us to the notion of a model's hypothesis space. For a given machine learning model, the hypothesis space is the types of functions it could represent. Remember that when we train a machine learning model, we are trying to choose"
        },
        {
            "start_time": 60,
            "end_time": 90,
            "transcription": " among many possible functions, the one that best represents our data. And so the hypothesis space describes the sort of functions that could be the result of our training. This is always constrained by the problem we are solving. In particular, the input representation we have chosen for our data may specify the dimension or other aspects of the model and the type of output required by our problem"
        },
        {
            "start_time": 90,
            "end_time": 120,
            "transcription": " constrains the possible functions, for example by requiring them to be discrete in the case of classification or continuous for regression. But different machine learning algorithms and models will also impose constraints on what type of mapping between the inputs and the outputs might be learned. For example, when we looked at single neuron models, the sigmoid classifier always identified a linear"
        },
        {
            "start_time": 120,
            "end_time": 150,
            "transcription": " and that's a very rigid constraint on the sort of function that could map the inputs to the outputs. If we wanted a nonlinear decision boundary, there are other types of model available. For example, a neural network with more nodes, but the same number of inputs and the same number of outputs could still perform classification on three-dimensional intervals."
        },
        {
            "start_time": 150,
            "end_time": 180,
            "transcription": " inputs, but could also divide the space of inputs in more complicated ways than is possible with a single neuron. So thinking about the hypothesis space of a particular machine learning model can serve as an important starting point to deciding whether it's appropriate to the problem that we're solving. But once we've chosen the type of model or the machine learning algorithm that we want to use, there remain a lot of"
        },
        {
            "start_time": 180,
            "end_time": 210,
            "transcription": " aspects of the model that can be tuned. First of all, whenever we're doing machine learning, we have the parameters of the model that we will train using our data set. That's in fact the whole point of machine learning that we don't know exactly what function we want, but we can adjust the parameters based on the data. And in the case of the single neuron models that we've seen so far, the parameters that got updated by the training"
        },
        {
            "start_time": 210,
            "end_time": 240,
            "transcription": " were the weights and the bias of that single neuron. If we had a larger neural net, the parameters would be similar, but now we would have weights and a bias for every neuron in the network. But it's also generally the case that some aspects of the model we're using are in principle tunable, but are not actually being updated when we train with gradient descent. In the single neuron models we've seen, this includes"
        },
        {
            "start_time": 240,
            "end_time": 270,
            "transcription": " the step size saying how far to go in the minus gradient direction. And in other contexts, this is also known as the learning rate. And when we run gradient descent, it generally takes many iterations to converge to a good model. But we can think of the number of iterations for which we will run gradient descent as another hyperparameter, something that we could tune, but is not being directly decided by the training."
        },
        {
            "start_time": 270,
            "end_time": 300,
            "transcription": " As our models get more and more complex, we will have more and more hyperparameters available. As soon as we start thinking about neural networks, we'll have to choose the number and connectivity of nodes in the network. And when we're talking about nodes that aren't directly producing the output of the network, we may have a choice of which activation function they should use. And over the course of the semester, we'll encounter many more hyperparameters of deep learning models."
        },
        {
            "start_time": 300,
            "end_time": 330,
            "transcription": " models, some of which will have very large effects, and others of which will have effects that depend greatly on the particular problem we're solving. So if we have many possible options for how to set the hyper parameters, or what class of model to choose, we would like to have a systematic way to explore those options, and to experimentally test which ones are more effective. And this now lets us define the"
        },
        {
            "start_time": 330,
            "end_time": 360,
            "transcription": " model selection problem, which is about choosing between different plausible models, which one will be best at representing our data set, and this is closely related to the idea of hyperparameter tuning, which is about selecting the values of the hyperparameters that will best represent the data. And my phrasing here is deliberate because these are fundamentally the same problem. There may not always be"
        },
        {
            "start_time": 360,
            "end_time": 390,
            "transcription": " a clear difference between what constitutes a hyperparameter and what constitutes a different class of model. Sometimes when we talk about different architectures for deep learning, we'll have different ways of organizing the nodes in a neural network, and we'll think of those as different classes of model, but other times we'll vary the number and connectivity of nodes and think of that as a hyperparameter"
        },
        {
            "start_time": 390,
            "end_time": 420,
            "transcription": " of a particular model. And so in both of these problems, what we're doing is really just trying to figure out among the available options, which will be most effective for our particular problem. And the key idea, and perhaps the most important concept in all of machine learning, is that we can answer these questions experimentally by separating a training set from a test set. If we divide our data set, including both the input"
        },
        {
            "start_time": 420,
            "end_time": 450,
            "transcription": " and the output examples into one portion that we will train on and a separate portion that we will not train on, then we can use that portion that we held back for out-of-sample validation. And so if we want to decide between two different options for a hyperparameter like the learning rate, we can try training our model on the training set using each of those different parameters,"
        },
        {
            "start_time": 450,
            "end_time": 480,
            "transcription": " And then test the resulting model using data that it was not trained on. And this is so important because it allows us to experiment on how well our models will generalize to data they haven't seen before. Wanting our models to generalize is important because if we never expected to see new data, there would be no point in doing machine learning in the first place. Our entire goal is to come up with a model that can"
        },
        {
            "start_time": 480,
            "end_time": 510,
            "transcription": " make good predictions on new data, and so to make sure that we're able to do that, we had better use some of our data to test that our model succeeds in that goal. One of the most obvious, but also most important ways, that generalization comes up is in terms of avoiding overfitting. Overfitting is the idea that a model can be trained to be too specific to a particular"
        },
        {
            "start_time": 510,
            "end_time": 540,
            "transcription": " data set. Here we have a data set with one-dimensional inputs and one-dimensional outputs represented by the black points, and we have two different candidate models trained on the data set. If our only goal were to accurately represent the training set, the blue model would be perfect because it hits every single one of our data points, while the red model has a fair bit of error on almost all of the data. But"
        },
        {
            "start_time": 540,
            "end_time": 570,
            "transcription": " But if we were to guess how well we think these models will generalize to a new data point that they haven't seen before, it seems more intuitive to me that the red model might actually be a better prediction than the blue model that has curved way beyond the points it's seen just to fit them exactly. But that intuition that the red model might generalize better"
        },
        {
            "start_time": 570,
            "end_time": 600,
            "transcription": " better than the blue one, is way less valuable than an actual experiment that demonstrates which of these is better at predicting data it hasn't seen. So in this case, instead of training on the entire data set, we could randomly pick a couple of points to leave out of the training. And then after we run our training algorithm on all but those points, we can ask which model got closer on the point"
        },
        {
            "start_time": 600,
            "end_time": 630,
            "transcription": " we held back. A good rule of thumb for how much data to leave out is an 80-20 split. There will absolutely be problems and models where we want to do some other split, but this is usually a good starting point. And we can also compare not just two specific models, but many different possible values for the hyperparameters that we think might make a difference. Sometimes"
        },
        {
            "start_time": 630,
            "end_time": 660,
            "transcription": " times it makes sense to systematically explore all of the possible values of a hyperparameter. But if there are multiple hyperparameters we think might be important, then exploring all possible combinations can be incredibly time consuming. So the approach that I recommend is to begin by figuring out the plausible range for each hyperparameter. Suppose that you started with a step size or learning rate of 0.01, but you're not sure if changing the learning rate"
        },
        {
            "start_time": 660,
            "end_time": 690,
            "transcription": " is going to make a big difference. Well, the first thing I would do is try some drastic changes to the learning rate. If you change the learning rate by a factor of 10 in either direction and don't see much change in the results, that's a good sign that you don't need to be exploring fine gradations of the learning rate within that range. But it may also mean that you want to try variations by even more orders of magnitude and pushing the parameter in"
        },
        {
            "start_time": 690,
            "end_time": 720,
            "transcription": " until it breaks, and the training does something dramatically different, is often a good way to establish the bounds that it makes sense to even try to explore. But if we think the interaction of several different hyperparameters is going to be important, then just varying them one at a time may not be enough to produce the best model. In which case, my advice is to first establish the range for each parameter, and then"
        },
        {
            "start_time": 720,
            "end_time": 750,
            "transcription": " randomly choose from within that range. The advantage of randomization over systematic search is that it allows you to explore a more diverse range of possible values more quickly, and this can be helpful in narrowing in the plausible parameter values before you make a finer-grained search in the areas you think are important. But if we end up trying out tons of different values for several different hyperparameters,"
        },
        {
            "start_time": 750,
            "end_time": 780,
            "transcription": " There's a danger that we'll actually start overfitting to the testing set because the choice of hyperparameter is now being optimized based on the predictions on the held-out set. And if that's the case, it may make sense to split the data even further, which might lead us to split into train, validate, and test sets, which will allow us to"
        },
        {
            "start_time": 780,
            "end_time": 810,
            "transcription": " us to set our parameters based on the training set and our hyper parameters based on the validation set and still have a test set available to make sure that our model generalizes. Finally, there may be some cases where we're not happy with always validating on the same subset of the data, even if we randomized which data went into the training set and which was for validation. And in that case, we can actually use our entire data set"
        },
        {
            "start_time": 810,
            "end_time": 840,
            "transcription": " for training and for validation by averaging our performance across several different test train splits. This is the idea of cross-validation, where I have illustrated here four-fold cross-validation. This means splitting the data into four equal-size subsets, and then we would train the model on three-fourths of the data and test it on the remaining one."
        },
        {
            "start_time": 840,
            "end_time": 870,
            "transcription": " one-fourth, and then we would do the same thing holding out each of the other fourths of the data, and we can average the performance across those different test sets to make sure that the generalization performance we observed wasn't specific to one particular subset of the data. But cross-validation can also be expensive because it requires us retraining the same model several times on"
        },
        {
            "start_time": 870,
            "end_time": 895.326625,
            "transcription": " different data sets. And so often it will suffice to just split our data into training and test or training validation and test sets. But any of these approaches to model selection would be an enormous improvement over the starting point of training on all the available data and having nothing left over to test whether the model can generalize."
        }
    ],
    "Neural Network Backpropagation (DL 08)": [
        {
            "start_time": 0,
            "end_time": 30,
            "transcription": " In this video, we're going to derive the backpropagation algorithm, which performs a step of stochastic gradient descent for training a neural network. The algorithm for this update proceeds in three steps. First, we use a feed forward pass to make a prediction on some data point. The predicted outputs determine the loss, and we next perform a backward pass to compute partial derivative"
        },
        {
            "start_time": 30,
            "end_time": 60,
            "transcription": " of that loss, and a final pass through the network modifies its parameters using the partial derivatives to update the weights and biases. For much more information on the forward pass, see our previous video where we talked in depth about how to compute all of the activations. And we said in that video that we needed to store each of the activations we computed because they would contribute to the partial derivatives"
        },
        {
            "start_time": 60,
            "end_time": 90,
            "transcription": " calculations coming later. So now I want to talk about how we compute the partial derivatives and how we use them to update the parameters. In the last video we defined a neural network loss function where for each data point and each of the network's outputs we computed a squared error. We then summed those squared errors over the output nodes and averaged across all"
        },
        {
            "start_time": 90,
            "end_time": 120,
            "transcription": " all of the points in the data set. Our task now in performing gradient descent is to determine how the weights of the network influence that loss. And for computing those partial derivatives, the key step turns out to be computing a quantity that we'll call delta for each of the output and hidden layer nodes in the network. For any of our"
        },
        {
            "start_time": 120,
            "end_time": 150,
            "transcription": " computing neurons, we define delta to be the partial derivative of the loss with respect to the weighted sum of inputs at that neuron. This xi is what gets passed into the activation function for neuron i. And we compute this quantity on the backward pass because it will determine the weight updates we perform for"
        },
        {
            "start_time": 150,
            "end_time": 180,
            "transcription": " every edge coming into that neuron. But since our loss consists of a sum over many data points, it will also help to think about the partial derivative of the loss on an individual data point. And if we think about the relationship between the partial derivative of the loss on one data point and the partial derivative of the loss on the whole data set, since the data set loss is an"
        },
        {
            "start_time": 180,
            "end_time": 210,
            "transcription": " average over all of the data points. When we take a derivative of this sum, we will get a sum of derivatives. And so the derivative of the loss on the data set is just an average of the derivatives of the loss on all of the data points. And so when performing the"
        },
        {
            "start_time": 210,
            "end_time": 240,
            "transcription": " our backwards pass on data point j, we want to compute the delta for data point j on every hidden and output neuron. Beginning with a neuron in the output layer, we should think about how the weighted sum of inputs for this neuron affects the loss function. So here I am deriving the partial derivative of the loss with respect to the"
        },
        {
            "start_time": 240,
            "end_time": 270,
            "transcription": " respect to x12 for data point j. Since the loss on data point j is a sum over the outputs, we'll move the derivative inside the sum and then apply the chain rule to the squared function. When we apply the chain rule"
        },
        {
            "start_time": 270,
            "end_time": 300,
            "transcription": " here, we get the derivative of the outside, so 2 times target minus activation, times the derivative of the inside. So this term here represents the derivative with respect to x12 of the inside, that is the target minus activation. But when we think about this derivative with respect to x12, the target is not affected by the"
        },
        {
            "start_time": 300,
            "end_time": 330,
            "transcription": " this variable, and so it is a constant, and the activation of the output node differs for the terms in the sum. First it's a 11, and then it's a 12, but for the other nodes like 11, the input to node 12 has no effect on their activation, so this variable is also a constant that goes to 0,"
        },
        {
            "start_time": 330,
            "end_time": 360,
            "transcription": " for all of the terms in the sum except for the term corresponding to our current output. For the term in this sum corresponding to node 12, the derivative of the inside will be minus the derivative of the activation with respect to x12. I'll factor that minus sign out front."
        },
        {
            "start_time": 360,
            "end_time": 390,
            "transcription": " Since we multiplied by zero for the terms in the sum corresponding to all the other outputs, we're left with only terms that depend on this node, and we now have something that looks an awful lot like what we derive for one neuron, minus two times the error times the derivative of the activation function."
        },
        {
            "start_time": 390,
            "end_time": 420,
            "transcription": " And because in the last video, we described the derivative of each of our activation functions in terms of the activation. And since we stored the activation of each neuron on our forward pass, we can describe the derivative of the activation function evaluated at the stored activation function"
        },
        {
            "start_time": 420,
            "end_time": 450,
            "transcription": " activation of this node on data point J. This same approach lets us calculate the deltas for all of the output layer neurons, but then we need to continue our backward pass and calculate deltas for hidden layer neurons. And the key insight here is that for a hidden layer neuron, it affects the output through each"
        },
        {
            "start_time": 450,
            "end_time": 480,
            "transcription": " of the neurons at the next layer that it feeds into. Here I have drawn the computational paths by which x8 influences the weighted sum of inputs at the next layer. It passes through the activation function for neuron 8, and then it gets multiplied by weights before taking part in the sum of inputs to produce x11 and x8."
        },
        {
            "start_time": 480,
            "end_time": 510,
            "transcription": " If we proceed backwards through the network, applying the chain rule, we can start from the partial derivatives we've already computed at this layer, and the partial derivative of the loss with respect to x8 will be a sum over its contribution through each of these paths."
        },
        {
            "start_time": 510,
            "end_time": 540,
            "transcription": " So here we are summing over all of the nodes in the next layer, the contribution that node eight makes through that path, and that contribution comes from applying the chain rule. We already know the partial derivative of the loss with respect to x11, and if we want to go further back, we multiply the derivative of"
        },
        {
            "start_time": 540,
            "end_time": 570,
            "transcription": " of the outside times the derivative of the inside. And so now we need to find the partial derivative of x 11 with respect to x eight. And we can break that down using our intermediate variable, which is the activation A eight. Since A eight participates in a weighted sum of inputs to produce x 11, the derivative of x 11 with respect to A eight"
        },
        {
            "start_time": 570,
            "end_time": 600,
            "transcription": " will have constants for all of the other terms in the sum. And for the term in the sum corresponding to a8, we'll get a derivative of this weight. And with one final application of the chain rule, we will multiply by the partial derivative of a8 with respect to x8."
        },
        {
            "start_time": 600,
            "end_time": 630,
            "transcription": " That simply comes from our known derivative of the activation function evaluated on the activation for the current data point. And since we're working on data point J, really all of these variables for inputs and activations should have a superscript J. And we now have a formula"
        },
        {
            "start_time": 630,
            "end_time": 660,
            "transcription": " for the delta at a hidden layer node. It's a sum over the nodes in the next layer of the next layer delta times the weight from the current node to that next layer node times the activation derivative for the hidden node. And what's really great about this"
        },
        {
            "start_time": 660,
            "end_time": 690,
            "transcription": " formula is that it wasn't specific to the second to last layer of the network. Everything I said about neuron 8 also applies to 9 and 10, but it similarly applies to neurons 5, 6, and 7. And this is why we are able to compute the deltas in a single backwards pass. Once we know the deltas for the last layer, we can calculate the"
        },
        {
            "start_time": 690,
            "end_time": 720,
            "transcription": " deltas for the layer before by a sum over the next layer deltas times the weights times the derivative of the current layer nodes. And that process will proceed backwards through the network until we have a delta for every hidden neuron. Now that we know how to calculate the deltas for the output layer and for the hidden layer, we'd like to use this to actually"
        },
        {
            "start_time": 720,
            "end_time": 750,
            "transcription": " produce the gradient vector for the parameters, that is, to find the partial derivatives for the weights and biases. And we haven't calculated any of those partial derivatives yet, but it turns out that the deltas were the hard part, and from here, getting the partial derivatives for the parameters is quite simple. If we think back to the partial derivatives that we came up with for a single neuron model,"
        },
        {
            "start_time": 750,
            "end_time": 780,
            "transcription": " the derivative for each weight differed only by the activation we were multiplying by. And the way we've set up the deltas, they summarize everything except for the activation we need to multiply by. Using the delta for node 11, to get the derivative for the weight from 8 to 11, we need just one more application of the chain rule. And that is applied to"
        },
        {
            "start_time": 780,
            "end_time": 810,
            "transcription": " to this summation. And since the only part of this summation that depends on the weight from eight to 11 is the term activation eight times the weight from eight, all of the other terms in the summation go away. And the derivative of this weight is just the delta times this activation. So O for data point J,"
        },
        {
            "start_time": 810,
            "end_time": 840,
            "transcription": " the partial derivative for the weight from k to l is just the delta we calculated for l times the activation we saved for k. And just like in the single neuron model, the corresponding bias partial derivative drops the activation and it's just equal to the delta. Now that we have formulas for the partial derivative of the weights and biases"
        },
        {
            "start_time": 840,
            "end_time": 870,
            "transcription": " on a single data point, we can get the partial derivative of the loss for the entire data set by simply averaging over all of the data points. And likewise, the bias partial derivative is an average over the deltas for all of the data points. Now in principle, we can take a gradient descent step"
        },
        {
            "start_time": 870,
            "end_time": 900,
            "transcription": " by computing the deltas and activations for every data point, using them to get the partial derivatives for each parameter. And then we can take a gradient descent step by subtracting a learning rate times the partial derivative from each parameter. But if we have a very large data set, which is frequently the case in deep learning, it can take a very long time to"
        },
        {
            "start_time": 900,
            "end_time": 930,
            "transcription": " calculate the deltas for every single data point and average them. And so instead of performing exact gradient descent, what we'll often do instead is stochastic gradient descent. And the idea is, if we randomly sample a subset of the data points, then we can compute the average loss on that subset. And we can use the"
        },
        {
            "start_time": 930,
            "end_time": 960,
            "transcription": " gradient of the mean error on that subset as an approximation of the gradient of the loss on the entire data set. And so if we average our partial derivatives not over the entire data set, but instead over a random sample, then we'll get an approximation of the gradient and can move in roughly the right direction and hopefully still reduce error, but"
        },
        {
            "start_time": 960,
            "end_time": 990,
            "transcription": " do so much more quickly. And so we can perform a stochastic gradient descent step by computing our derivatives using a sample from the data set. And our update pass through the network will change each of the parameters"
        },
        {
            "start_time": 990,
            "end_time": 1020,
            "transcription": " by eta times the average partial derivative computed on the sample. And so in summary, the back propagation algorithm performs a stochastic gradient descent update"
        },
        {
            "start_time": 1020,
            "end_time": 1050,
            "transcription": " by choosing a random sample from the data set and the best way to choose that random sample is to shuffle the data set and then group it into batches. Then on each data point computing the activations with a feed forward pass and the deltas with a backwards pass. Then using the activations and the deltas we can calculate"
        },
        {
            "start_time": 1050,
            "end_time": 1080,
            "transcription": " the partial derivative for each data point, and then we can average over our batch to determine the update that we perform to each weight and each bias in the network. This gives us one step of stochastic gradient descent, and so we'll need to do this many times with many different batches sampled from the data set in order to minimize the loss and train our neural network on"
        },
        {
            "start_time": 1080,
            "end_time": 1081.481,
            "transcription": " dataset."
        }
    ],
    "Recurrent Neural Networks (DL 17)": [
        {
            "start_time": 0,
            "end_time": 30,
            "transcription": " In the last lecture, we talked about embeddings, which we can train to convert words into vectorized representations with a large but not crazy number of dimensions. And now we'd like to use those embeddings as a starting point for building machine learning systems that can operate on text data. And for some simple tasks like"
        },
        {
            "start_time": 30,
            "end_time": 60,
            "transcription": " classifying the sentiment of a product review, it might suffice to break up the document into words, embed each word, and then pass that sequence of embeddings as input to a neural network. But with text data, there are many much more challenging tasks that we might be interested in, like conversational replies or machine translation. And for those sorts of problems, we're going to need to do something"
        },
        {
            "start_time": 60,
            "end_time": 90,
            "transcription": " more sophisticated. So as our running example here, we will use the task of predicting the next word in a sentence, given the word so far. This is harder than just classifying text as positive or negative reviews, but it's also much easier than machine translation. Thinking for a moment about how we want to set up neural networks in general,"
        },
        {
            "start_time": 90,
            "end_time": 120,
            "transcription": " general to operate on any text processing task, there are in a sense two broad approaches. One extreme would be to give an entire document as input to a neural network, and the other extreme would be to give a single word as input to a neural network. If we're operating on entire documents, that gives us a lot fewer training examples to work with,"
        },
        {
            "start_time": 120,
            "end_time": 150,
            "transcription": " And we also have to deal with the fact that different documents that we might be trying to operate on could be of wildly varying sizes. But if we try to operate on one word at a time, then our network will be missing out on all of the surrounding context that is important to understanding a particular word's meaning."
        },
        {
            "start_time": 150,
            "end_time": 180,
            "transcription": " meaning in a sentence or a document, and this causes problems if we're trying to do translations or trying to represent concepts that don't map one to one onto words. And so both of these broad approaches make the learning problem way too difficult. In one case, because we're trying to operate on way too much data at a time and not getting enough learning feedback, and in the other case, because we're trying to operate on way too"
        },
        {
            "start_time": 180,
            "end_time": 210,
            "transcription": " data at a time and not having the information that we need to make good inferences. And so each of the methods that we will actually consider will in fact be some sort of compromise between these two extremes. The first approach we'll consider achieves this compromise by operating on one word at a time, but setting up the architecture of the network so that it can be"
        },
        {
            "start_time": 210,
            "end_time": 240,
            "transcription": " can remember some of its previous inputs and therefore retain some of the important context. So thinking in terms of our predict the next word problem, how the heck can we set up the architecture of a network to give it memory of inputs that it was given in the past? Well, the basic idea is to feed the networks"
        },
        {
            "start_time": 240,
            "end_time": 270,
            "transcription": " output back to its input. And so then if we give the neural network the first word of the sentence, it can make a guess about the next word. But then when we give it the second word of the sentence, it will also receive as input its own activations on the first word in the sentence. And so those activations can serve as a summary of what it saw before. When it outputs its guess for the third word, that will"
        },
        {
            "start_time": 270,
            "end_time": 300,
            "transcription": " be fed back in along with the actual third word when it is trying to guess the fourth word. And so the output or the activations of a hidden layer at the end of the network can serve as the summary of all of the previous words of the sentence. And the result is what's known as a recurrent neural network. To reason about recurrent neural networks, it helps to unroll the"
        },
        {
            "start_time": 300,
            "end_time": 330,
            "transcription": " them over time. Each of these boxes represents the same neural network but at different points in time. Since we're inputting the different words one at a time and feeding the network's output back to its input, we represent feeding the output as these arrows forward from time one to time two. And on each time step, the network receives a word as input"
        },
        {
            "start_time": 330,
            "end_time": 360,
            "transcription": " and tries to predict what the next word will be. And so the target for each of these time steps is the same as the input for the next time step. Note that all of the inputs will be vectors from our embedding, but the outputs can't actually be embedding vectors because we need to allow the network to express uncertainty over"
        },
        {
            "start_time": 360,
            "end_time": 390,
            "transcription": " different possible outputs. And so the outputs will actually be in our one-hot dictionary encoding. This might seem rather complicated, but in terms of computing the predictions, it's not really all that bad. We're giving an embedded word as input, and we're concatenating that with the previous activations from the last time step. The tricky part"
        },
        {
            "start_time": 390,
            "end_time": 420,
            "transcription": " and the reason for unwinding the time steps is computing the gradients. Broadly speaking, to take a gradient descent step, we want to figure out how do the weights within the neural network influence the error on each of the words in the sentence. So our gradient will be an average of the losses on each of the different outputs"
        },
        {
            "start_time": 420,
            "end_time": 450,
            "transcription": " at different time steps, but that by itself is not all that different from averaging loss gradients over a batch as we've been doing before. The insidious part is that for later time steps, the weights of the network influenced the loss not just through how they were applied to the input at that time step, but also how those weights affected the"
        },
        {
            "start_time": 450,
            "end_time": 480,
            "transcription": " inputs from all of the previous time steps because that information gets carried all the way into the eventual computation of output at a later time step. So to do a gradient descent step, we want to average the loss over all of these time steps. But to compute the effect that the neural networks weights had on the loss at a particular time step, we have to compute not all"
        },
        {
            "start_time": 480,
            "end_time": 510,
            "transcription": " only how the weights affected the input from that time step, but also how the repeated applications of weights affected the earlier time steps and their contribution to error made on this time step. I'm not going to go through a full derivation of those gradients, but I will post a reading that does. But one thing that's noteworthy here is that recurrent neural networks"
        },
        {
            "start_time": 510,
            "end_time": 540,
            "transcription": " often deliberately use sigmoid or 10H activation functions because those tend to be prone to the vanishing gradient problem and with a vanishing gradient problem we don't expect gradients to propagate too far backwards through the network and so we get things less wrong if we cut off these gradient propagations after a"
        },
        {
            "start_time": 540,
            "end_time": 570,
            "transcription": " relatively small number of steps. But that inability to propagate information backwards through a large number of time steps is part of the reason that I've been talking about this in terms of sentences and not documents. And for exactly this reason of gradients not being able to propagate very far backwards, plain recurrent neural networks are not very good at"
        },
        {
            "start_time": 570,
            "end_time": 595.4060625,
            "transcription": " solving tasks that require a lot of context and therefore long-term memory. And so the topic for next time will be a variant of recurrent neural networks that are designed specifically to achieve longer-term memory and therefore be much more effective at text and language processing tasks."
        }
    ],
    "Residual Networks and Skip Connections (DL 15)": [
        {
            "start_time": 0,
            "end_time": 30,
            "transcription": " Deep neural networks are incredibly powerful, but unfortunately can be very hard to train. In part, that's because with more and more parameters, we need more and more data, so each epoch of training takes longer. But independent of the data set, the depth of the network can be an obstacle to training. When we have success training shallow neural networks, we tend to see a large decrease in the loss"
        },
        {
            "start_time": 30,
            "end_time": 60,
            "transcription": " in the first few epochs of training. But if we build a deep neural network out of dense or convolutional layers, the loss graph often looks more like this. Where we can run an awful lot of training with very little decrease in the loss, and remember each epoch of training takes longer when we have an enormous data set. And so this can be a big obstacle to practical training of deep networks."
        },
        {
            "start_time": 60,
            "end_time": 90,
            "transcription": " To figure out why training gets so much slower as networks get deeper, let's think about what happens when we pass data through a very deep neural network. When we initialize a neural network, all of the layers have randomly chosen weights. And so when we pass an input through each layer of the network, the activations are being multiplied by a random weight matrix."
        },
        {
            "start_time": 90,
            "end_time": 120,
            "transcription": " And if we do this many, many times, then by the time the data gets to the output layer, almost none of what we're producing is actual signal related to the input, we've essentially scrambled that input into random noise. Then when we perform an update, we'll compute the loss on that output and propagate it back through the network. But that loss may not be very informative."
        },
        {
            "start_time": 120,
            "end_time": 150,
            "transcription": " at the layers late in the network because their inputs were mostly random noise. And when we pass gradients back through the network, each layer multiplies the deltas by its weight matrix. And so by the time we get back to the early layers that had inputs with some meaningful connection to the data, the gradients have also been scrambled by many multiplications of random"
        },
        {
            "start_time": 150,
            "end_time": 180,
            "transcription": " and weight matrices. And so the updates that we do to the later layers aren't very meaningful because their inputs have been scrambled and don't mean very much, and the updates we do to the early layers aren't very meaningful because their gradients have been scrambled and don't mean very much. And so this is why we tend to see not much improvement for an awful lot of training time as gradient descent essentially wanders around"
        },
        {
            "start_time": 180,
            "end_time": 210,
            "transcription": " So what can we do about this? Well, we'd like to create some way for the data to arrive at the later layers of the network and make their inputs more meaningful, and also for loss gradients to arrive at the early layers of the network and make their updates more meaningful. And we can achieve both of those goals using skip connections. The idea of skip connections is to"
        },
        {
            "start_time": 210,
            "end_time": 240,
            "transcription": " to group up layers of the network into blocks, and for each block, have data go both through and around. Within each block, the layers will pass their data forward normally, but between blocks will have a new type of connection. And that connection works by"
        },
        {
            "start_time": 240,
            "end_time": 270,
            "transcription": " Combining the input to the block with the output from the block giving two different paths for data to follow one that goes through the block and one that goes around We then need some way of combining the output of the block with the input"
        },
        {
            "start_time": 270,
            "end_time": 300,
            "transcription": " to the block, and we want this to be a simple function that passes gradients along undisturbed, and for that we have a couple of choices. We can either take the tensor of inputs and the tensor of outputs and add them element-wise, or we can concatenate those tensors."
        },
        {
            "start_time": 300,
            "end_time": 330,
            "transcription": " is now a network that is built out of residual blocks. And we hope that these skip connections will accelerate training and make our loss graph look more like this. So why is it reasonable to expect that networks built out of residual blocks will be easier to train? Well, they have a couple of big advantages."
        },
        {
            "start_time": 330,
            "end_time": 360,
            "transcription": " The first big advantage of a residual block is that the function it is computing is augmenting the existing data. Since the input is being passed along and will still be available to later layers in the network, the job of this layer is no longer to figure out every"
        },
        {
            "start_time": 360,
            "end_time": 390,
            "transcription": " important about the input that needs to be passed along, but rather to figure out what information it can add on top of the input to make subsequent processing easier. And this sort of function tends to be much easier to learn because the block doesn't have to start by figuring out what information the input contains. Instead, the block starts by passing along all of the input and barely modifying it."
        },
        {
            "start_time": 390,
            "end_time": 420,
            "transcription": " If we're using concatenation, then the input is passed along unchanged in addition to whatever the block outputs. But even if we're using addition, when we initialize these layers with random weights, those weights tend to be small and centered around zero, meaning that what we add is initially close to zero, and so initially we will be passing along the information relatively unchanged."
        },
        {
            "start_time": 420,
            "end_time": 450,
            "transcription": " And so each block has a simpler task to learn, and each block has access to better information to learn from. The second big advantage is that we now have much shorter paths for the gradients to follow to get to each layer of the network. Since each block has one path that goes through its layers and one path that goes around them,"
        },
        {
            "start_time": 450,
            "end_time": 480,
            "transcription": " The gradients will flow along both of those paths. And this means that any layer in the network now has a relatively short path by which loss gradients can arrive and usefully update what that layer is computing. And therefore, in the initial epochs of training, when the information coming"
        },
        {
            "start_time": 480,
            "end_time": 510,
            "transcription": " along the path that goes through all of the layers is relatively uninformative. We can still get useful updates out of these shorter paths and get decreasing loss right away. Plus over time, as the later blocks in the network start computing more and more useful functions, the information coming along this path will get more and more informative. And so our gradients can"
        },
        {
            "start_time": 510,
            "end_time": 540,
            "transcription": " and continue to decrease and outperform a shallow network. So these first two advantages of simplifying the function we need to learn and speeding up the propagation of gradient information are the main motivations behind building residual networks. But residual blocks turn out to have an additional advantage of modularity. Since each of these blocks have essentially the same structure"
        },
        {
            "start_time": 540,
            "end_time": 570,
            "transcription": " And since we are able to short circuit around those blocks in the initial training, it's relatively easy to add more blocks and build a deeper, more powerful network. And so when you read about residual networks in a paper or see an implementation in a model zoo, you'll often find several different variations on the same network with different numbers of blocks"
        },
        {
            "start_time": 570,
            "end_time": 600,
            "transcription": " to achieve different trade-offs between the resources needed to train the network and the power of the resulting model. Because of all this, residual network architectures are extremely powerful, and many of the same ideas have also been incorporated into the other network architectures we will study soon, but there are a few concerns we need to keep in mind when thinking about how to set up a residual network."
        },
        {
            "start_time": 600,
            "end_time": 630,
            "transcription": " If we want to combine the inputs and outputs of a residual block by addition or concatenation, then we need to think about how do the shapes of those layers match up."
        },
        {
            "start_time": 630,
            "end_time": 660,
            "transcription": " activation vectors, like with dense layers, then we might have different numbers of neurons in the different layers. And some mismatch is inevitable, unless we are willing to have the shape of every layer in our network be dictated by the shape of the input representation. So if we are adding our inputs to the output of the first block, we either need some operation that will reshape the input"
        },
        {
            "start_time": 660,
            "end_time": 690,
            "transcription": " or we need to only add the inputs to a part of that block's output. This gets even more complicated if our blocks contain convolutional layers, where we definitely don't want to constrain the depth of all our later layers based on the color channels in the input. And we also need to make sure that the height and width of our convolutional layers can be"
        },
        {
            "start_time": 690,
            "end_time": 720,
            "transcription": " combined with the height and width of the input. And so when we're using residual blocks, we will usually want to have some special case for how we are getting the shape of the input to match up with the shape for the first block. And then we want to make sure that subsequent blocks have a common shape so that this sort of problem is minimized."
        },
        {
            "start_time": 720,
            "end_time": 750,
            "transcription": " Another concern is that if we use concatenation repeatedly to combine the outputs of many blocks, then we get a very, very large activation tensor and an explosion in the number of parameters. So even if the shapes match up well enough that we can do concatenation, the more times that we can"
        },
        {
            "start_time": 750,
            "end_time": 780,
            "transcription": " we can coordinate the bigger this activation tensor gets. And since subsequent layers will have weights coming from each of these activations, we can end up with an explosion in the number of parameters if we overuse concatenation. And so if we are using lots of residual blocks, we should prefer addition over concatenation. And so you'll rarely see more"
        },
        {
            "start_time": 780,
            "end_time": 810,
            "transcription": " than one or two skip connections that use concatenation in a single network. And so when we're building a large residual network, we'll generally use addition for most of the skip connections. And we'll try and have most of the blocks use a consistent shape so that we can match up the inputs and the outputs."
        },
        {
            "start_time": 810,
            "end_time": 840,
            "transcription": " But it's worth spending a little extra time to think about how can we achieve this goal when we're using convolutional layers."
        },
        {
            "start_time": 840,
            "end_time": 870,
            "transcription": " can do to simplify convolutional blocks is use one by one strides and appropriate padding to ensure that the height and width of the image is preserved. And that way we know that when we add the input and output tensors for a block, at least the height and width dimensions will match up."
        },
        {
            "start_time": 870,
            "end_time": 900,
            "transcription": " But if we also want the channel's dimension to match up, we can achieve that using one by one convolutions. Well, one by one convolutions might seem kind of silly since they get their inputs from only one pixel of the image. But let's draw an illustration of what such a neuron would compute."
        },
        {
            "start_time": 900,
            "end_time": 930,
            "transcription": " If we perform convolution with a one by one kernel, then we'll get an output block with the same height and width as the input block. And each of the neurons in the output will get its inputs from just one pixel of the input."
        },
        {
            "start_time": 930,
            "end_time": 960,
            "transcription": " get inputs from the entire depth of that pixel's channel, whether that's the three color channels of an input image or the several different neurons that computed simple functions on the same window from the previous layer. But by setting the number of filters or channels for our one by one convolution, we can specify the depth of our output layer and each of"
        },
        {
            "start_time": 960,
            "end_time": 990,
            "transcription": " of these neurons will get its input from every neuron across the depth of the previous layer. And so we can use this to either increase or decrease the depth. And so if we think by analogy to dense layers, adding this one by one convolution step is as though we inserted a single dense layer along this path to make the input and output shapes match up. There are lots of other variations on residual"
        },
        {
            "start_time": 990,
            "end_time": 1019.75075,
            "transcription": " networks and how to build residual blocks, many of which include other types of layers that might do normalization or might pass data through multiple paths within a block. And so I encourage you to read about different types of residual architectures, but all of them follow this common theme that achieves much better performance in terms of our ability to train deep neural networks."
        }
    ],
    "Better Activation & Loss for Classification\uff1a Softmax & Categorical Crossentropy (DL 09)": [
        {
            "start_time": 0,
            "end_time": 30,
            "transcription": " When we use a neural network for multi-class classification, sigmoid neurons kind of suck. When we only had two classes, we could use a single neuron to output 0 or 1. But if we have several labels like 0 through 9 for classifying handwritten digits, or potentially way more labels for harder problems, we need a different representation. One of the most common approaches is to encode the labels as a one-hot vector, where we have a dimension"
        },
        {
            "start_time": 30,
            "end_time": 60,
            "transcription": " for each label, and only one at a time should be switched on. For example, here we have a five-dimensional vector indicating that there were five possible labels, and for this particular point, dimension four is switched on, meaning that this point had the fourth label. In principle, an output layer made up of sigmoid neurons could be trained to produce this sort of output, but in practice there are a couple of problems."
        },
        {
            "start_time": 60,
            "end_time": 90,
            "transcription": " issue illustrated here is that it's quite possible for the sigmoid layer to output lots of relatively large values. In this case, 0.91 is the largest activation, which might indicate that we are predicting label 4, but we have an activation that's almost as large for label 3 and other activations that are also quite far from 0. And so if our neural network gave this sort of output, it would be rather hard to interpret the prediction."
        },
        {
            "start_time": 90,
            "end_time": 120,
            "transcription": " What we'd really like is for the output layer to produce zeros and ones, or failing that for it to produce something that we can interpret as confidence in different possible labels. The other problem here comes when we try to train this sigmoid output layer. If our target is this one-hot vector, then we are trying, when we perform gradient descent, to push this activation toward one and all of the others"
        },
        {
            "start_time": 120,
            "end_time": 150,
            "transcription": " towards zero. But if we calculate the deltas for some of these neurons, we get the surprising result that the neuron that is more wrong has a smaller delta. This happens because the derivative of the sigmoid function, illustrated in blue here, gets very small when we get far away from an input of zero. That means that even though the"
        },
        {
            "start_time": 150,
            "end_time": 180,
            "transcription": " for this dimension is large because the function is so flat out here, the derivative is small, and so we're not able to make a large update by gradient descent. And so if a sigmoid output layer is confidently wrong, that's very hard to fix. A similar issue known as the vanishing gradient problem also comes up when we use sigmoid activations for hidden neurons, and we'll talk much more about vanishing gradients in a future video."
        },
        {
            "start_time": 180,
            "end_time": 210,
            "transcription": " In this video, I want to demonstrate an alternative combination of output activations and loss function that will solve both of these problems with a sigmoid output layer. Our new activation is known as a softmax, and it gets its name from behaving a little bit like finding a maximum. Softmax activations are computed on an entire layer rather than individually for each neuron. And this activation function"
        },
        {
            "start_time": 210,
            "end_time": 240,
            "transcription": " only makes sense as the output layer for a classification problem. We should avoid using softmax activations for hidden layers. To compute softmax activations, we take the weighted sum of inputs xi for each node i in the output layer. We then divide them all by the sum of all those exponentiated inputs. The result is that raising e to each of the"
        },
        {
            "start_time": 240,
            "end_time": 270,
            "transcription": " inputs will magnify differences between the inputs, and then dividing by the sum will normalize all of the activations so that they add up to 1. This has the result of pushing the largest input towards 1 and pushing the other inputs closer to 0, hence the similarity to max, which would choose just the single largest. When we use a softmax"
        },
        {
            "start_time": 270,
            "end_time": 300,
            "transcription": " layer that tends to produce outputs that are much more interpretable as a prediction. Because we will tend to get output vectors that are much more likely to have a single value close to 1 and a lot of values close to 0. Also, since the values sum to 1, we can sometimes interpret them as the network's confidence in each of the possible predictions. But to make use of Softmax activations, we want to combine"
        },
        {
            "start_time": 300,
            "end_time": 330,
            "transcription": " with a loss function that gives us effective gradient descent updates. And the best candidate for that turns out to be categorical cross entropy. We define cross entropy loss as minus the sum over the outputs of the target for that neuron times the natural log of its activation. But if we think for a moment about what this computes, if we're using one-hot vectors for our targets,"
        },
        {
            "start_time": 330,
            "end_time": 360,
            "transcription": " most of the dimensions will be zero, except for a one on the correct label. And so, most of the terms in this sum will be zero, and it reduces to minus the log of the activation on the neuron that should have an output of one. We can call that neuron where we want the output to be one, t star. It's the one that we are actually targeting with our one hot."
        },
        {
            "start_time": 360,
            "end_time": 390,
            "transcription": " vector, and we can think about the gradient of this loss function for both the neurons where we want the output to be 1 and for the others where we want the output to be 0. To use this combination of categorical cross entropy loss and softmax activation, we need to figure out how to compute the deltas for all of the output nodes, which will then serve as the starting point for"
        },
        {
            "start_time": 390,
            "end_time": 420,
            "transcription": " backpropagation. So let's start by thinking about the partial derivative of the loss with respect to the activations. Since our loss function looks very different for t star than for any of the other output nodes, we'll look separately at partial derivatives for t star and for the other neurons. And so first we want to figure out how does the loss change with respect to the"
        },
        {
            "start_time": 420,
            "end_time": 450,
            "transcription": " activation of the node whose label we want to be one. Well, that labels activation appears right here, and so we are taking the derivative of this function with respect to its input. And the derivative of the natural log is 1 over x. So we get minus 1 over the activation, but in a moment it will help to have plugged in our formula for that activation. So let's do that now."
        },
        {
            "start_time": 450,
            "end_time": 480,
            "transcription": " And we get minus the inverse of this fraction with t star plugged in for the node. On the other hand, the derivative of the loss with respect to any of the other nodes is just zero because those activations don't appear in the loss formula. And this is why categorical cross entropy only works with an activation function"
        },
        {
            "start_time": 480,
            "end_time": 510,
            "transcription": " function like softmax, because if the delta for a node depended only on its own activation, then we would be performing no updates at all to any of the nodes where we wanted to output zero. But in the case of softmax, the activation for any neuron depends on the inputs to all of the neurons. And so even though we only have a non-zero partial derivative for this"
        },
        {
            "start_time": 510,
            "end_time": 540,
            "transcription": " we will still get non-zero deltas for all of the inputs. So now we need to figure out how do each of these inputs affect the activation of the t star neuron, which means determining the partial derivative of the t star activation with respect to both the t star input and each of the other inputs. So in both cases,"
        },
        {
            "start_time": 540,
            "end_time": 570,
            "transcription": " we're taking the derivative of this activation, but in the first case we're focusing on the variable that appears both on the top and in the sum, and in the second case we're looking at one of the other variables that only appears in the denominator. For this derivative we need to apply the quotient rule. So the derivative of this quotient is a prime b minus b prime a over b squared, where a prime means the"
        },
        {
            "start_time": 570,
            "end_time": 600,
            "transcription": " derivative of e to the xi with respect to xi, while b prime means the derivative of this sum with respect to the xi that appears in one of its terms. End of note, both terms on the top have an e to the xt star that we can factor out."
        },
        {
            "start_time": 600,
            "end_time": 630,
            "transcription": " For some other node k, the derivative of the sum will still be e to the xk, but the derivative of the top will now be zero because this is a constant with respect to xk. Now to find the actual deltas for the output layer, we want the partial derivative of the loss with respect"
        },
        {
            "start_time": 630,
            "end_time": 660,
            "transcription": " Each of those inputs affects all of the activations, but then only the activation for t star affects the loss. So when we apply the chain rule to the loss function, we end up with the partial derivative of the loss with respect to the activation of t star times the partial derivative of that activation with respect to the activation of t star."
        },
        {
            "start_time": 660,
            "end_time": 690,
            "transcription": " to each of the layers inputs. When we write out this product for the t-star node, we see lots of things we can cancel. The e to the x t star in the denominator cancels with the one that we can factor out from the difference"
        },
        {
            "start_time": 690,
            "end_time": 720,
            "transcription": " And the sum in this numerator cancels with one of the sums in the denominator. And we're left with the minus sign out front, and then these two terms in the numerator that we can split up and simplify further. And here we have, as the first term,"
        },
        {
            "start_time": 720,
            "end_time": 750,
            "transcription": " exactly the activation for the t star node minus a second term that simplifies to 1. And so as long as we have already computed and saved this activation on the forward pass, the delta for this node is incredibly easy to calculate. And it turns out that we'll get lots of the same simplifications when we work out the partial derivatives for the other node."
        },
        {
            "start_time": 750,
            "end_time": 780,
            "transcription": " neurons. We'll start by copying over these terms that we calculated before. When we start canceling terms here, we first lose both of the minus signs, and our sum in the numerator can cancel with one from the denominator. And our e to the x t star terms can"
        },
        {
            "start_time": 780,
            "end_time": 810,
            "transcription": " and also cancel. And what we're left with is exactly the formula for the activation of node k. So what we've derived here are formulas for computing the deltas for the node where the target was one and all of the other output layer nodes."
        },
        {
            "start_time": 810,
            "end_time": 840,
            "transcription": " And if we now think about how these deltas will perform as part of a gradient descent update, for the neuron where we wanted the target to be 1, we get a negative value for delta, and that value will be closer to 0, the closer the activation gets to 1. While for the others where the output should be 0, we get a"
        },
        {
            "start_time": 840,
            "end_time": 870,
            "transcription": " positive delta that gets smaller as the activation gets closer to zero. This means that when we take a step in the minus gradient direction, we will be increasing the activation where we need it to be one and decreasing all of the others. And the amount that we change those activations by will be proportional to the size of the error that those neurons made in our"
        },
        {
            "start_time": 870,
            "end_time": 889.5680625,
            "transcription": " our most recent prediction, which means that by combining softmax activations and categorical cross entropy losses, we can get outputs that make a lot of sense for classification problems, and we can get gradients that will rapidly push those outputs towards the correct predictions."
        }
    ],
    "AlphaGo & AlphaGo Zero (DL 24)": [
        {
            "start_time": 0,
            "end_time": 30,
            "transcription": " AlphaGo and AlphaGo Zero are the two Go-playing agents developed by Google subsidiary DeepMind. This system combined deep convolutional neural networks with self-play reinforcement learning to achieve a tremendous leap forward over previous Go algorithms, and in 2016 became the first GoAI to defeat a human world champion. In this video, I'll explain how DeepMind created these systems and cover some of the highlights"
        },
        {
            "start_time": 30,
            "end_time": 60,
            "transcription": " from the research papers they published about both AlphaGo and AlphaGo Zero. Go is a two-player game that occupies a similar position in East Asian culture to that of chess in American or European culture. The rules are incredibly simple to describe. Players alternate placing black and white stones on any empty intersection of the board, and if a stone or group of stones is completely surrounded by the opponent's"
        },
        {
            "start_time": 60,
            "end_time": 90,
            "transcription": " pieces, it is captured and removed from the board. The game ends when both players pass, and a player's score is the number of stones they have on the board, plus the number of empty intersections their pieces surround. There's also a rule that if stones have been captured, you can't play a move that would cause a previous board position to be repeated, but that's it. And even though the rules of the game can be described in about 30 seconds,"
        },
        {
            "start_time": 90,
            "end_time": 120,
            "transcription": " it has enormous strategic complexity. So much so that as recently as 2015, many experts thought that Go playing AIs that could compete with top human players were still decades away. And so it's worth asking, why did experts in both artificial intelligence and Go think that just before AlphaGo debuted in 2016? In order to plan a good move in a game like Go,"
        },
        {
            "start_time": 120,
            "end_time": 150,
            "transcription": " chess, you have to think several steps ahead. If I play this move, then how will my opponent respond, and how will I respond to their response, and so on? The chess engine Deep Blue, which in the mid-1990s became the first to defeat a human world champion, did this by considering every possible sequence of moves and counter moves some number of steps ahead, and then evaluating for each of the resulting board states"
        },
        {
            "start_time": 150,
            "end_time": 180,
            "transcription": " how good is it to end up in that position? In chess, this can be done largely on the basis of which pieces have been captured and other easy-to-compute heuristics. But unfortunately in Go, evaluating the quality of a board position is a much harder problem. And in addition, Go has a much higher branching factor than chess, meaning that at any given state there are"
        },
        {
            "start_time": 180,
            "end_time": 210,
            "transcription": " a lot more legal moves, and so to consider them all exhaustively, more than a couple of steps ahead quickly becomes infeasible. So if we want to develop an algorithm that reasons several steps ahead in go, we need some way of restricting the hundreds of possible moves to only the most promising few choices that we want to pay attention to. And we also need some way of determining for the state"
        },
        {
            "start_time": 210,
            "end_time": 240,
            "transcription": " that result from those move sequences, how good is that board position? The key innovation of AlphaGo was to solve both of these problems using deep learning. AlphaGo trained deep neural networks to solve two separate but closely related problems. Both the value network and the policy network take as input a representation of a state of the board, but the value network estimate"
        },
        {
            "start_time": 240,
            "end_time": 270,
            "transcription": " the probability of each player winning if the game continues from that state, while the job of the policy network is to estimate the quality of each of that state's legal moves. The output of both of these models can then be incorporated into the planning algorithm, where the policy network helps restrict our attention by showing which moves are the most promising for the search to fail."
        },
        {
            "start_time": 270,
            "end_time": 300,
            "transcription": " focus on, while the value network can tell us when we reach some future board state, how good would it be to end up in that position? So to understand how the deep learning components of these systems work, let's dive into the architecture that was used and the kinds of data that they were trained on. The network architecture is one of the areas where AlphaGo and AlphaGo Zero differ substantially. In the first paper,"
        },
        {
            "start_time": 300,
            "end_time": 330,
            "transcription": " the authors describe a plain convolutional architecture, and they talk about training separate policy and value networks. In fact, they train three neural networks because there are two different versions of the policy network, one that is optimized for accuracy and another optimized for speed. But in the newer paper, they found more success using a residual architecture and training a single core network"
        },
        {
            "start_time": 330,
            "end_time": 360,
            "transcription": " network with separate heads for the policy and value outputs. The idea of this core and head architecture is that the residual blocks can learn to detect important information about the board state that is needed for both estimating the value and picking good moves. And then each of the heads can be a much smaller network that takes all of that information about the board state that was computed by the"
        },
        {
            "start_time": 360,
            "end_time": 390,
            "transcription": " residual blocks and outputs either a wind probability or a waiting over possible moves. Within each of the blocks is a relatively standard residual architecture consisting of convolutional neurons with rectifier linear activations along with batch normalization and a residual connection. The other big difference between AlphaGo and AlphaGo Zero is the data that they trained on."
        },
        {
            "start_time": 390,
            "end_time": 420,
            "transcription": " AlphaGo started its training with a large data set of games played online between high level amateurs, whereas AlphaGo Zero used zero human knowledge in its training data. Training the value network, or the value head of the combined network, is relatively simple. The input is a board state, represented by ones for black pieces, minus ones for white"
        },
        {
            "start_time": 420,
            "end_time": 450,
            "transcription": " pieces and zeros for empty intersections, while the targets are plus one or minus one for who ended up winning the game. And this sort of data can easily come from the human games where we sample some board state and then look at who ended up winning the game, or they can come from games that the AI played against itself in the same way. The training data for the policy network"
        },
        {
            "start_time": 450,
            "end_time": 480,
            "transcription": " or the policy head of the combined network is more complicated, if we are training with data from human games, then the sort of observations we have are, from a given board state, what move did the human play? And so the policy networks for the original AlphaGo were trained by categorical cross entropy to just predict the human move. And when the original AlphaGo went beyond the human"
        },
        {
            "start_time": 480,
            "end_time": 510,
            "transcription": " data by adding self-play training data, it also used just the move that was played as the training target. But the loss function also incorporated information about which agent ended up winning the game so that it could up-weight moves that led to a win and down-weight moves that led to a loss. But the original AlphaGo paper actually reports that they didn't achieve a lot of improvement in the policy"
        },
        {
            "start_time": 510,
            "end_time": 540,
            "transcription": " network from this sort of self-play training. And in fact, as of the publication date of that paper, the best version of AlphaGo used a policy network that was entirely based on learning to imitate human moves, but with a value network that had been updated using data from self-play. But for AlphaGo Zero, the authors realized that there's actually much more information available"
        },
        {
            "start_time": 540,
            "end_time": 570,
            "transcription": " from the self-play data that can be used for training the policy network. In particular, when the planning algorithm is thinking several steps ahead and considering lots of different options for what move to play, it gathers information not just about the move it ended up playing, but in fact, about all of the possible moves. So AlphaGo Zero trains the policy network on targets that are not just based on"
        },
        {
            "start_time": 570,
            "end_time": 600,
            "transcription": " what move got played, but instead on the estimates of move quality that were produced by the search that the planning algorithm performed. Which means that, since the planning algorithm is looking several steps ahead, over time the policy network will be trained to output estimates of move quality that correspond to thinking many steps into the future."
        },
        {
            "start_time": 600,
            "end_time": 630,
            "transcription": " So the overall loss function that the AlphaGo Zero combined network gets trained on is a sum of a cross entropy term for the policy head, a squared error term for the value head, and an L2 regularization term to make sure the weights don't explode. Of note, both the AlphaGo and AlphaGo Zero papers build on lots of prior work in deep"
        },
        {
            "start_time": 630,
            "end_time": 660,
            "transcription": " reinforcement learning, where overfitting to particular aspects of a task is common. And so AlphaGo makes sure to not train on every single board state from a single game, and instead samples random board states from a large collection of games. And this ensures that the network doesn't overfit to the path of play from one particular game. And as the agent improves"
        },
        {
            "start_time": 660,
            "end_time": 690,
            "transcription": " over time with better and better versions of the neural networks, it still hangs on to data from previous versions of the agent to make sure that it isn't overfitting to just playing against itself. So these models need to be incorporated into a planning algorithm that can take advantage of the policy network to restrict which moves are being considered and the value network to estimate the"
        },
        {
            "start_time": 690,
            "end_time": 720,
            "transcription": " probability from different board states. And the planning algorithm used by both AlphaGo and AlphaGo Zero is called Monte Carlo Tree Search. MCTS had previously been used by other GoAIs, but without policy and value networks was only able to compete with mid-level amateurs. With or without the deep learning components, MCTS works by performing some number of rollouts."
        },
        {
            "start_time": 720,
            "end_time": 750,
            "transcription": " Each MCTS rollout considers some sequence of moves beginning from the current board state. And over time, MCTS builds up a data structure to store the information it has gathered about the value of different move sequences. This figure from the AlphaGo Zero paper only shows two moves, but obviously in a real Go game, there could be hundreds of legal moves from a given state. But for each"
        },
        {
            "start_time": 750,
            "end_time": 780,
            "transcription": " of those moves, MCTS stores an estimate of the move's quality, and an estimate of the uncertainty about that move. The quality score is initialized based on the outputs of the policy network, which tells us for each of the legal moves, how good does the model think it will be. Then over time, as the algorithm performs rollouts, where it simulates playing that move,"
        },
        {
            "start_time": 780,
            "end_time": 810,
            "transcription": " It will update the quality based on the value estimates for future states in those rollouts. The uncertainty term is based on how many times a particular move has been tried in previous rollouts. So the uncertainty term will start out equal for all of the moves, and each time the search considers a particular move, the uncertainty for that move will"
        },
        {
            "start_time": 810,
            "end_time": 840,
            "transcription": " decrease. And so when considering which move to try in a given rollout, MCTS will pick the move that has the highest sum of the quality and uncertainty terms. This means that initially when all of the uncertainties are equal, the highest quality move, according to the policy network's estimate, will be selected. But after we have tried that move, we will update its quality estimate"
        },
        {
            "start_time": 840,
            "end_time": 870,
            "transcription": " based on what we found, and we will lower its uncertainty every time we try it. Which means that after we have tried a move a few times and decreased its uncertainty, other moves may end up with a higher quality plus uncertainty and get explored on future rollouts. The search continues in this way until it reaches a state that has never been seen before. While we're looking at states that have"
        },
        {
            "start_time": 870,
            "end_time": 900,
            "transcription": " already been explored, we can pick the move with the highest quality plus uncertainty. But when we get to a new state, we have to evaluate it using our neural networks. So we can give that new state that we just found to our double-headed policy and value network, from which we'll get a value estimate, meaning the probability that we win if we reach that state, and also a quality for each of the legal"
        },
        {
            "start_time": 900,
            "end_time": 930,
            "transcription": " moves from that state. We'll use the output of the policy network to initialize the quality scores for all of that state's legal moves. And then we will use the output of the value network to update the quality score for each of the moves that we played in the rollout that got us to this state. Which means that over time these quality scores will become an average over the values that we"
        },
        {
            "start_time": 930,
            "end_time": 960,
            "transcription": " found in all of the rollouts where we played that move. After a large number of these rollouts have been performed, we should have good quality estimates based on thinking several moves ahead for all of the legal moves at the current state. In addition, we should have very low uncertainty for the better moves that have been explored more times. And so to actually"
        },
        {
            "start_time": 960,
            "end_time": 990,
            "transcription": " play a move in the game, the agent can pick among the legal moves at the current state, either the one with the highest quality, or the one with the lowest uncertainty, because in general those should be the same, and then after the move is played, the current state of the board will change, and the agent whose turn it is now can start running Monte Carlo Research to make a plan for what move it will choose from that state."
        },
        {
            "start_time": 990,
            "end_time": 1020,
            "transcription": " So both AlphaGo and AlphaGo Zero perform their self-play reinforcement learning by having two Monte Carlo-treat search agents play against one another, and gathering data from those gains to expand the training set for both the policy and value networks. And in fact, the system can be in parallel running self-play games between agents with the current versions of the network, and training"
        },
        {
            "start_time": 1020,
            "end_time": 1050,
            "transcription": " new versions of the networks using data from the previous rounds of self-play games. During self-play, we need to store the sort of data that the networks would be trained on. So for AlphaGo Zero, that means we need to save the state of the board, which player ended up winning the game, and the quality estimates for all of the legal moves that Monte Carlo"
        },
        {
            "start_time": 1050,
            "end_time": 1080,
            "transcription": " found when it was evaluating that board state. Then the neural network training will use samples from the self-play games, where it will pick out a random board state to use as input, and train the value head to estimate who will win from that state, and the policy head to output quality scores that are more like the several-step look-ahead produced by Monte Carlo"
        },
        {
            "start_time": 1080,
            "end_time": 1110,
            "transcription": " many iterations of playing games with the current networks and training new networks with the data from those games, we will gradually improve the quality of estimates from both the value and policy heads of the network, meaning that the agent will get better and better at playing Go, and the AlphaGo Zero paper showed that this can start completely from scratch, where the agent initially makes completely random moves, but over time"
        },
        {
            "start_time": 1110,
            "end_time": 1140,
            "transcription": " time, the value head will start to be able to estimate who's going to win from a given position. And then once the value head has some useful information about the quality of different states, the Monte Carlo research will be able to incorporate that information and make better predictions for the quality of different moves. And since the policy head is being trained to imitate those MCTS"
        },
        {
            "start_time": 1140,
            "end_time": 1170,
            "transcription": " evaluations, it will eventually also learn to usefully restrict the space of possible options and make Monte Carlo research better and better over time. And over tens of millions of self-play games and millions of stochastic gradient descent mini-batch updates, AlphaGo Zero eventually outperformed the original AlphaGo, meaning that the version that trained itself entirely from scratch"
        },
        {
            "start_time": 1170,
            "end_time": 1200,
            "transcription": " eventually outperformed the version with a huge head start from imitating humans. AlphaGo and AlphaGo Zero have achieved some very impressive results. In 2016, the original version reported in the AlphaGo paper defeated one of the best players in the world in a five-game match. And then after a bunch of additional training, that same AlphaGo system won a sequence of 60 consecutive online matches against professionals."
        },
        {
            "start_time": 1200,
            "end_time": 1230,
            "transcription": " Go players. And then in additional show matches, defeated both the number one ranked player in the world and a team of top professionals collaborating against it. But in the AlphaGo Zero paper, DeepMind showed results of the new system playing against both of those older iterations of AlphaGo. And AlphaGo Zero beat them both by wide margins. Since the publication of these pages"
        },
        {
            "start_time": 1230,
            "end_time": 1260,
            "transcription": " and especially AlphaGo Zero have had a big impact on the Go community. Lots of other projects, including open source and commercial software, have replicated AlphaGo Zero's results. And while the training still takes an awful lot of self-play data, the inference phase, actually using the agent to play a game of Go, can now run on just about any kind of"
        },
        {
            "start_time": 1260,
            "end_time": 1290,
            "transcription": " computer. Which means that in online Go, cheating with bots has become a very big concern. But at the same time, this wide availability has made a big difference in how Go players study the game, where instead of using the AI to cheat, they can use the AI after the fact to analyze how well they played a game and figure out what sorts of moves they should be considering in the future to do better."
        },
        {
            "start_time": 1290,
            "end_time": 1320,
            "transcription": " Outside of the Go community, AlphaGo has also had a big impact through follow-up papers on related systems. In 2017, DeepMind also published about Alpha Zero, a system that uses the AlphaGo Zero system, but swaps in the rules of a different game instead of the rules of Go. And they showed that AlphaZero can achieve very impressive performance in games like chess and show-go."
        },
        {
            "start_time": 1320,
            "end_time": 1350,
            "transcription": " And very recently, DeepMind published a new application of AlphaGo called AlphaTensor, where instead of playing a game, they're using Monte Carlo Tree Search and Deep Learning to plan the execution of matrix multiplication algorithms, and they showed that this system can devise new algorithms that improve both the big O running time and the practical efficiency on real hardware of matrix multiplication algorithms."
        },
        {
            "start_time": 1350,
            "end_time": 1361.21175,
            "transcription": " multiplication, which has the potential to someday improve all of the calculations that we do in training all the neural networks we use in deep learning."
        }
    ],
    "The Adam Optimizer (DL 21)": [
        {
            "start_time": 0,
            "end_time": 30,
            "transcription": " The goal of this lecture is to show how we can shore up some of the weaknesses of stochastic gradient descent and build up to the atom optimizer, which is how we've actually been training our neural networks. When we do stochastic gradient descent, we are for every weight in the neural network subtracting from that weight for every batch that we train on the partial derivative of the loss for that batch with respect"
        },
        {
            "start_time": 30,
            "end_time": 60,
            "transcription": " to weight w multiplied by our learning rate. When we do this for every weight in the network, we are taking one step of size eta in the direction of minus the gradient of the loss on that batch. But gradient descent has a few common failure modes where it can get stuck or learn slowly. Here I have some one and two dimensional examples that illustrate some ideas that also come"
        },
        {
            "start_time": 60,
            "end_time": 90,
            "transcription": " up in the much higher dimensional gradient descent case of training a neural network. In this one dimensional example with just a single weight, we minimize our loss at this point. Gradient descent in this one dimensional case just takes us downhill from any given point. So the gradient would point in this direction. And if we take a step in"
        },
        {
            "start_time": 90,
            "end_time": 120,
            "transcription": " the direction of a gradient from an initial weight here, we'll get a next weight that is closer to the minimum. But that's not true for every possible point. We have a couple of examples illustrated here where gradient descent might not be helpful. First of all, if we're at a local minimum of the loss function with a weight value here, the gradient will be zero. And so we would never"
        },
        {
            "start_time": 120,
            "end_time": 150,
            "transcription": " move away from this point. We also have a gradient of zero if we find ourselves on a plateau of the loss function. Stocasticity from training on different random batches of our data can sometimes help with getting out of local minima or getting across plateaus, but it doesn't completely eliminate either problem. When we work in higher dimensions, here we have two weight dimensions, and so we're representing the loss of"
        },
        {
            "start_time": 150,
            "end_time": 180,
            "transcription": " as a contour plot, we also have the potential for saddle points. At a point like this in our weight space, we have in one direction a local minimum, and in another direction a local maximum of the loss function. And so at a saddle point, we would also have a gradient of zero. Saddle points are a smaller problem than local minima, because stochasticity is very likely to help us out of"
        },
        {
            "start_time": 180,
            "end_time": 210,
            "transcription": " a saddle point because a small perturbation in the direction where we're at a local maximum would take us to a point where the gradient descent steps would move us away. But in the vicinity of a saddle point, we have small gradients, which means that learning updates will move very slowly in the vicinity of saddle points. And in high dimensions, saddle points can become much more common. So it's worth thinking about"
        },
        {
            "start_time": 210,
            "end_time": 240,
            "transcription": " how can we ensure that learning doesn't slow down too much? Another problem that's common in higher dimensional optimization problems is for the partial derivative to be on a very different scale in different dimensions. And that can often lead to cases like this region where taking a step in the direction of the gradient can overshoot in one or more dimensions."
        },
        {
            "start_time": 240,
            "end_time": 270,
            "transcription": " And if we continue gradient descent from here, now we take a step in this direction. And so in cases where the partial derivatives on different weights are dramatically different, we can end up with this kind of zig-zagging behavior. The atom optimizer that we have typically been using to train our neural networks incorporates two key"
        },
        {
            "start_time": 270,
            "end_time": 300,
            "transcription": " ideas that help to combat some of these problems. The first of these ideas is momentum. The idea of momentum is that if we have taken several steps in the same direction, then we should err on the side of continuing to move in that direction. This can be really helpful with cases like a plateau because if we came onto that plateau from one direction,"
        },
        {
            "start_time": 300,
            "end_time": 330,
            "transcription": " Then continuing in the same direction that we came seems like a likely way to get off of that plateau. Whereas if we come down to a local minimum, then if we continue in the other direction and overshoot it, then our gradients will push us back in the other direction. Momentum can also help us with saddle points because if we find ourselves"
        },
        {
            "start_time": 330,
            "end_time": 360,
            "transcription": " coming into the vicinity of a saddle point, we would continue in the direction we were already going and get away from that region of very small partial derivatives. And for very small local minima, sometimes momentum can help carry us past them. The way momentum is handled is by replacing our partial derivative update term with an update term based on a"
        },
        {
            "start_time": 360,
            "end_time": 390,
            "transcription": " moving average of partial derivatives. If over the time steps t of our optimization, we keep track of a running average of these partial derivatives, then we can update that running average by taking beta times our old average plus one minus beta times our"
        },
        {
            "start_time": 390,
            "end_time": 420,
            "transcription": " partial derivative. And so for each subsequent time step, we will incorporate the partial derivative at that time step, weeding it alongside the old information about previous partial derivatives. And so then we can replace our partial derivative term in the update with our partial derivative moving average term V."
        },
        {
            "start_time": 420,
            "end_time": 450,
            "transcription": " The second key idea is that we can also keep track of a moving average of the square of the partial derivative. So these two variables, V and S, differ only in that V is a moving average of the partial derivative and S is a moving average of the squared partial derivative. This weighted average of the square is a way of estimating"
        },
        {
            "start_time": 450,
            "end_time": 480,
            "transcription": " the second moment of the partial derivative. And the second moment is related to the variance. And the reason it's helpful to keep track of the variance is that we can square root the variance to get the standard deviation and then normalize by the standard deviation of each of the different dimensions. This will have the effect of giving us similar size steps in different dimensions that can"
        },
        {
            "start_time": 480,
            "end_time": 510,
            "transcription": " generally help us reduce the problem of zigzagging and overshoot it. So our update now becomes W minus equals our learning rate times V divided by the square root of S. Because S could be zero and we want to make sure that we don't have a division by zero, we'll typically add plus epsilon in the denominator"
        },
        {
            "start_time": 510,
            "end_time": 540,
            "transcription": " where epsilon is some very small constant like 10 to the minus 8th. So this is essentially the update that is performed by atom. There's a small debiasing that I've left out that you can find in the reading. The reading also provides an alternate explanation of the S term based on an approximation of the second derivatives in the Hessian matrix, which you may or may not find more convincing than the one that I've given."
        },
        {
            "start_time": 540,
            "end_time": 552.9019375,
            "transcription": " But what's more important is that in practice the atom optimizer has proven to be much more effective than vanilla stochastic gradient descent for training neural networks."
        }
    ],
    "Automatic Differentiation (DL 26)": [
        {
            "start_time": 0,
            "end_time": 30,
            "transcription": " In this lecture, I want to introduce reverse mode automatic differentiation. The Julia library's zygote does automatic differentiation. If we were doing our large scale machine learning projects in Julia, you would be using the flux deep learning library, which is implemented on top of zygote. So what does zygote do? Well, among other things, it can automatically compute the gradient of a function."
        },
        {
            "start_time": 30,
            "end_time": 60,
            "transcription": " So the Zygote library provides us a gradient function that takes in another function and arguments at which to evaluate the gradient. And it tells us that at the input point, 1, 2, minus 1, the gradient of this function is"
        },
        {
            "start_time": 60,
            "end_time": 90,
            "transcription": " 22 4 minus 12. And we could verify that by taking the symbolic derivative of this function and plugging in the point 1 2 minus 1. So that's handy, but so far not very different from what you implemented way back in project 0. What's awesome about automatic differentiation is that it allows us to compute gradients of much more interesting functions. So let's consider a function that you've probably implemented at some point in your"
        },
        {
            "start_time": 90,
            "end_time": 120,
            "transcription": " past computer science career."
        },
        {
            "start_time": 120,
            "end_time": 150,
            "transcription": " of 0,1,12 and that will give us that the 12th element of the Fibonacci sequence is 144. But this version of the function also lets us specify other starting points besides 0 and 1. And now with the gradient function from zygote, we can compute the gradient of this Fibonacci function. And it tells us that at the"
        },
        {
            "start_time": 150,
            "end_time": 180,
            "transcription": " the inputs 0, 1, and 12, the partial derivative of the function with respect to a is 89. The partial derivative with respect to b is 144, and there is no partial derivative with respect to n. This makes sense because the variable n is not a continuous variable in this function. It's used only as the loop constant. And so if we made a small"
        },
        {
            "start_time": 180,
            "end_time": 210,
            "transcription": " perturbation to n, we wouldn't have any change in the output of the function. But it's saying that if we made a small perturbation to a, that would have an 89 times larger effect on the output of fib. And we can test that numerically. So let's set an epsilon to some small value. And then we can compute fib on"
        },
        {
            "start_time": 210,
            "end_time": 240,
            "transcription": " 0, 1, 12, and compare that to fib computed on epsilon comma 1 comma 12, and then divide that by epsilon, and we get approximately 89. Similarly, for the partial derivative with respect to b, we could take a finite difference at"
        },
        {
            "start_time": 240,
            "end_time": 270,
            "transcription": " 1 to 1 plus epsilon and get approximately 144. So how on earth are we computing the gradient of this complicated function with a for loop? It turns out there are several ways that this can be done and they fall into a few broad categories. The category we're going to focus on is reverse mode automatic differentiation. And the way reverse mode automatic differentiation works"
        },
        {
            "start_time": 270,
            "end_time": 300,
            "transcription": " is to essentially build up a computation graph automatically as the function is being executed, and then to back propagate partial derivatives through that computation graph. So let's switch over to the iPad and draw this out. If we want to compute the partial derivative of a function like fib with respect to its inputs A and B, the key idea is to replace those variables,"
        },
        {
            "start_time": 300,
            "end_time": 330,
            "transcription": " and B, which are currently just holding numerical values with objects that can hold both the numerical value and some additional information that helps us in computing derivatives. There are two types of information that we could store in these variables. The first for forward mode automatic differentiation would store both the value of the variable and the partial derivative of that variable"
        },
        {
            "start_time": 330,
            "end_time": 360,
            "transcription": " value with respect to each of the inputs to the function. The second version for reverse mode automatic differentiation would store along with the value of the variable, the other variables that preceded it in the computation, and the function that was used to compute its value from those predecessor variables."
        },
        {
            "start_time": 360,
            "end_time": 390,
            "transcription": " For this example, it would be pretty feasible to do forward mode automatic differentiation by hand and I will post a reading by Dr. Niedinger on how that works. But for deep learning reverse mode automatic differentiation is much more useful. The basic reason for this is that forward mode scales in the number of inputs because we have to propagate a partial derivative with respect to each"
        },
        {
            "start_time": 390,
            "end_time": 420,
            "transcription": " input and in deep learning we have an enormous number of inputs, namely all of the weight parameters in the network. And so forward mode would get out of hand quickly, whereas reverse mode scales in the number of outputs, which is typically one just the network's loss. So the purpose of these reverse mode autodiff objects is to automatically construct a"
        },
        {
            "start_time": 420,
            "end_time": 450,
            "transcription": " computation graph while evaluating the function that allows us to later back propagate the derivatives. We start with two of these objects corresponding to the two inputs to the function with respect to which we want to compute partial derivatives. And they have a value equal to the values that we passed into the function. And they have"
        },
        {
            "start_time": 450,
            "end_time": 480,
            "transcription": " no prior values and no function that was used to compute them, so those other fields will be null. And now as we proceed through the function, at every step we want to create a new variable storing the intermediate result of that computation. On the first iteration of this for loop when i equals 2, we'll get intermediate values that we can call C2B2."
        },
        {
            "start_time": 480,
            "end_time": 510,
            "transcription": " and a2, and so C gets the value 1, and its value depends on inputs a and b, and we get that by the function plus that adds a to b. Whereas b2 and a2 get their values from assignment statements from a single previous variable. On the next iteration of the for loop, we'll get intermediate values C3, b3, and"
        },
        {
            "start_time": 510,
            "end_time": 540,
            "transcription": " A3, C3 will get the value 2 from inputs A and B and we add them together. B3 gets its input from C3 and A3 gets its input from the variable B2. And we can proceed forward like this until we get to the end of computing the functions output. By keeping track of all of these intermediate"
        },
        {
            "start_time": 540,
            "end_time": 570,
            "transcription": " variables along the way, we have been implicitly building up a computational graph, where these parents in the object tell us which other nodes in the graph feed into the node for this variable. And so we can draw in all of those dependencies and also the function nodes that produce the value of a variable."
        },
        {
            "start_time": 570,
            "end_time": 600,
            "transcription": " And once we have added all of these parent edges to the graph, we have what is essentially a complete computational graph for this function. Along the way, we stored the value of each of the intermediate variables. And so, when we get to the end, we know the output of the function. And so we can use the computational graph"
        },
        {
            "start_time": 600,
            "end_time": 630,
            "transcription": " and the intermediate variables to perform backpropagation. Because for each node we've stored the function that computed it and the value that was computed we can apply the chain rule to that node and because we stored the parents we can propagate the derivatives back through the network. This sequence of intermediate objects with a value pair"
        },
        {
            "start_time": 630,
            "end_time": 660,
            "transcription": " and a function can be stored in a list or other data structure and this collection of objects storing the reverse mode automatic differentiation objects is referred to as a gradient tape. All of this can still be done if we have more complicated intermediate functions as long as we know how to compute the derivative of those functions and the value of these variables"
        },
        {
            "start_time": 660,
            "end_time": 683.978625,
            "transcription": " need not be a scalar, but can be a vector matrix or tensor. And so as long as all of the smallest components of which our function is composed are differentiable, we can break that down with automatic differentiation and compute the partial derivatives of that function."
        }
    ],
    "Convolutional Layers (DL 13)": [
        {
            "start_time": 0,
            "end_time": 30,
            "transcription": " All the neural networks we've considered up to now have been densely connected, meaning we can think of each layer as having a vector of activations, and each neuron in one layer is connected to every neuron in the next. And dense networks are a great place to start because they are, in a sense, the most general neural networks. But it turns out that a lot of the time we can choose some other neural network architecture"
        },
        {
            "start_time": 30,
            "end_time": 60,
            "transcription": " that is more effective for a particular application. And so starting with this video, we'll move away from dense architectures. And the first alternative architecture we'll consider is based on convolutional layers. Convolutional networks are particularly well adapted for image processing tasks. So far when our inputs have been images, we have taken the pixels of the image and put them into"
        },
        {
            "start_time": 60,
            "end_time": 90,
            "transcription": " one great big vector to serve as the input layer for our dense networks. But when we connect every neuron in our first hidden layer to every pixel of the image, that loses a lot of spatial information about which pixels are near one another. That seems like it would be helpful in processing that image. In principle, our dense network might well learn which pixels are near each other along the way."
        },
        {
            "start_time": 90,
            "end_time": 120,
            "transcription": " But if instead we use an architecture that actually captures spatial proximity of the pixels, that could give our network a big head start in learning to do image processing. And so the idea of a convolutional network is that we will connect our neurons to only the pixels from a small region of the image. If we have a neuron that only gets inputs from a small"
        },
        {
            "start_time": 120,
            "end_time": 150,
            "transcription": " subregion of the image, then whatever that neuron learns to compute as we perform gradient descent and update its weights, will be necessarily a function of that portion of the image. And then we can slide this input window and for other regions of the image have other sets of neurons. Then, just like in our dense layers, we had many neurons."
        },
        {
            "start_time": 150,
            "end_time": 180,
            "transcription": " that we're all receiving the same inputs, we can for any given window of connections have many different neurons that are processing that same sub-region. This is important because each individual neuron can only learn a relatively simple function, and there might be many different types of processing we would want to do on a given spatial region. On top of"
        },
        {
            "start_time": 180,
            "end_time": 210,
            "transcription": " spatial connectivity, the other key idea that gives convolutional layers a big head start is that we often want to apply the same function to many different regions of the image. For example, if one of these neurons learned to be an edge detection function, it would be really great to use that function all over the input image."
        },
        {
            "start_time": 210,
            "end_time": 240,
            "transcription": " We can't directly control what function a given neuron will learn, but it turns out that we can apply the same function all over the image. And we do this using wait tying. Our goal here is to ensure that for each region in the image, the first neuron is computing the same function, and the second neuron is computing the same function on each region, but the first and second neuron"
        },
        {
            "start_time": 240,
            "end_time": 270,
            "transcription": " neurons will be computing different functions from each other. To ensure that these two neurons compute the same function on two different regions of the image, we start by initializing the weights to be the same from each input in the previous layer. That means that the top left input to the red neuron will have the same initial weight"
        },
        {
            "start_time": 270,
            "end_time": 300,
            "transcription": " as the top left input to the blue neuron. And likewise, both neurons will have the same initial weight from each other input pixel. And we also initialize them to have the same bias. We'll still generate the weights and biases randomly, but we'll do that once for each distinct neuron."
        },
        {
            "start_time": 300,
            "end_time": 330,
            "transcription": " and will copy the weights for the first neuron to the first neuron of every region in the image. Then when we are computing gradients and updating the weights, we will think of these weights that I've drawn in the same color as being a single parameter. And the contribution of this green weight to the overall loss of the network will be the sum of its contribution by"
        },
        {
            "start_time": 330,
            "end_time": 360,
            "transcription": " all of the different paths, meaning we will sum up its contribution to each of the neurons that it's connected to. And that means we'll perform the same weight update for the green weight across every single neuron in the entire layer. So if two neurons started with identical weights and we perform identical updates, we know that they'll always be computing identical functions on"
        },
        {
            "start_time": 360,
            "end_time": 390,
            "transcription": " their different sub-regions of the image. So the combination of local connectivity and weight tying lets the network learn to apply the same function to many different regions of the image, and we can give it many neurons to let it compute many different local functions, and all of this gives a big head start on learning to process images relative to the completely unstructured"
        },
        {
            "start_time": 390,
            "end_time": 420,
            "transcription": " learning that we would do with a dense network. But now, if we use convolutional layers, we have a bunch of new hyperparameters that we have to think about. The first is we need to choose how big of a subregion each neuron will get its input from. And this is known as the kernel size. These kernels are most often square, and it's more common to choose an odd size, but neither of those is a strict requirement. And"
        },
        {
            "start_time": 420,
            "end_time": 450,
            "transcription": " Even if we choose odd-sized square kernels, we have a lot of options for how big that square should be. But also, when we are sliding our window of inputs, we don't necessarily have to slide it by the size of that window. Think back to our supposition that one neuron might learn to be an edge detector. If there were an edge right on the boundary of these kernels, then we might not be able to detect"
        },
        {
            "start_time": 450,
            "end_time": 480,
            "transcription": " it. And so we might want to slide the window by less than the width of the kernel. And so we have a hyperparameter for the amount that we slide the window in each dimension. Here I've drawn in red an initial 4x4 kernel, and the blue kernel is offset by a stride of 1. Shifting by 1 gives the maximum possible over"
        },
        {
            "start_time": 480,
            "end_time": 510,
            "transcription": " overlap and ensures that we won't miss anything. But it also results in a very large number of neurons in our convolutional layer. And so we might sometimes prefer to shift by more and only have a small amount of overlap, like here, where relative to the red kernel, the green one is shifted by a stride of three. Again, we could in principle choose different strides for the different"
        },
        {
            "start_time": 510,
            "end_time": 540,
            "transcription": " dimensions of the image, but we usually don't. And we almost always want to choose the strides to be less than or equal to the kernel size so that we don't miss any inputs. We also said we want many neurons to be applied to each window of the image because each neuron computes a simple function and we might need to compose many simple functions to do good image processing. And so that number of functions that we apply to the same region"
        },
        {
            "start_time": 540,
            "end_time": 570,
            "transcription": " has several different names. Sometimes you'll see it called the output channels of the layer, or sometimes it's called the number of filters. And in this case, I've drawn six neurons all getting their inputs from this 4x4 region. But just like with dense networks, where we often use many more neurons in practice than it's practical to draw on the whiteboard, we'll often have many"
        },
        {
            "start_time": 570,
            "end_time": 600,
            "transcription": " more channels or filters than what I have drawn here. There are also many other hyperparameters associated with convolutional layers, and I'm not enumerating all of the possibilities here. But another important one is what to do when we get to the edges of the image. It could often be the case that the combination of kernel size and strides that we've chosen doesn't perfectly match up with the dimensions of the input image."
        },
        {
            "start_time": 600,
            "end_time": 630,
            "transcription": " And so we might end up with some kernels that hang off the edge of the image, and in that case we need to decide what to do with the inputs that are out of bounds. And so we have a couple of hyper parameters associated with this padding. First is the amount, which has to do with how far off the edge of the image are we willing to go. Sometimes the amount of padding is forced,"
        },
        {
            "start_time": 630,
            "end_time": 660,
            "transcription": " by the relationship between the input size and the other parameters. But it turns out there are cases where it makes sense to deliberately let the kernels go further off the edge than they need to, which mostly have to do with preserving shape from one layer to the next in contexts that we'll talk about later. But regardless of how much padding we have, we also have to decide what values to put there. And the most common choices are to either fill in zero"
        },
        {
            "start_time": 660,
            "end_time": 690,
            "transcription": " or to duplicate the pixels near the boundary. And these are referred to as zero padding or same padding. Another hyperparameter that's often associated with convolution is an amount of pooling. And the purpose of pooling is to reduce the size of a layer so that we don't have an explosion in the number of parameters when we use multiple convolutional layers. So before getting into pooling, we should think about how many parameters"
        },
        {
            "start_time": 690,
            "end_time": 720,
            "transcription": " parameters we have in a convolutional layer, which means we need to talk about tensors. From a computer science perspective, a tensor is simply a multi-dimensional array. We can think of a vector as a one-dimensional array or a matrix as a two-dimensional array. And so a tensor is just a generalization to an array of arbitrary dimensions. And if we think about the inputs when we do image processing,"
        },
        {
            "start_time": 720,
            "end_time": 750,
            "transcription": " and are not flattening them out into a vector, well, images have a height and a width, but it turns out they also have a third dimension because for every pixel in the image there are three color channels for red, green, and blue. And so when we're working with 2D images and thinking about two-dimensional convolution, it turns out that our data is often"
        },
        {
            "start_time": 750,
            "end_time": 780,
            "transcription": " represented with a three-dimensional tensor. So supposing that our input image is 200 pixels high and 300 pixels wide, then the array storing each image is 200 by 300 by 3. But as we know, neural networks don't operate on just one data point at a time. Instead, they work with a"
        },
        {
            "start_time": 780,
            "end_time": 810,
            "transcription": " of data. And so we might have yet another dimension that stores many different images. And so if we have a hundred images in a batch, our tensor storing such a batch might have shape 200 by 300 by 300 by 100. But just like how with a dense network, we don't typically draw the batch dimension, I will not try to draw the fourth dimension representing the batch."
        },
        {
            "start_time": 810,
            "end_time": 840,
            "transcription": " matches here. But now let's think about what type of array stores the activations for a hidden convolutional layer. Well, since we are sliding our windows across a two dimensional image, we can think of the first neuron of each window as being arranged in a two dimensional grid. But then we also have several"
        },
        {
            "start_time": 840,
            "end_time": 870,
            "transcription": " neurons that apply to each window. And so those stack up in a third dimension. And thus, we can think of the hidden layer as having height and width dimensions corresponding to height and width from the image reduced by some factor based on how quickly we slid the windows. And then the depth of the layer is based on how many functions we had per width."
        },
        {
            "start_time": 870,
            "end_time": 900,
            "transcription": " And so if our strides moved the window by five pixels each time, and we reduced the height and the width by a factor of five, then we'd end up with 40 by 50 as the height and width of our hidden layer."
        },
        {
            "start_time": 900,
            "end_time": 930,
            "transcription": " But if we had 50 functions applying to each window, then we would have a depth of 50. And again, we have a fourth dimension for the batch size of 100 that I can't draw on the whiteboard. So this gives us a shape for the activation tensor. But how many neurons and how many parameters does that mean? So if we have a"
        },
        {
            "start_time": 930,
            "end_time": 960,
            "transcription": " of 50 neurons per window and 40 by 60 windows, that gives us our number of neurons. So this layer would contain 120,000 neurons, but luckily a lot of those neurons share weights and biases. So in calculating the number of distinct parameters, we can completely ignore the height and width dimensions."
        },
        {
            "start_time": 960,
            "end_time": 990,
            "transcription": " because we know the neurons are just duplicated over all of the different windows. So with 50 different functions, we end up with 50 biases, and 50 times the number of weights per function, total weights. And since we have 9 by 9 kernels, we will have"
        },
        {
            "start_time": 990,
            "end_time": 1020,
            "transcription": " by 9 pixels coming into each neuron. And each pixel actually consists of three values for the three color channels. So we have 9 times 9 times 3 weights per neuron for a total of about 12,000 weights. So far, this is actually far fewer parameters than we would have with a dense network. Supposing that we had 50 nodes in a"
        },
        {
            "start_time": 1020,
            "end_time": 1050,
            "transcription": " dense layer, which of note would be far less capable than this convolutional layer with 50 functions per window, then we would have 200 by 300 weights for each of those neurons times 50 neurons, giving us a total of 3 million parameters. And this is a big part of why it can be so much easier to train a convolutional network than a densely connected"
        },
        {
            "start_time": 1050,
            "end_time": 1080,
            "transcription": " Unfortunately, the trouble comes when we try to add additional layers after this first convolutional layer. We might want to add additional convolutional layers that would have a sliding window over the 40 by 60 dimension of our hidden layer, which we could think of as aggregating together the functions computed on these small windows into new functions that are now"
        },
        {
            "start_time": 1080,
            "end_time": 1110,
            "transcription": " applying to large regions of the image, or at some point we could flatten out our tensor and have a densely connected layer that lets us make a prediction with a one-hot output vector. But either way, we'll end up with weights coming from each of the neurons in the layer, and so our very large number of neurons is going to explode the number of weights at the next layer."
        },
        {
            "start_time": 1110,
            "end_time": 1140,
            "transcription": " And this is where pooling can come to our rescue. Pooling is based on the idea that if our functions are learning to identify some local feature in the image, we might not need to know exactly which locations the feature happens in. It might be sufficient to figure out which broad areas of the image has the feature happened in in order to do our subsequent calculations."
        },
        {
            "start_time": 1140,
            "end_time": 1170,
            "transcription": " And so we could take some, for example, 3 by 3 or 5 by 5 window from our hidden layer and somehow pool together the results of the neurons in that region. This pooling is generally not a learned function, but rather some really easy to compute way of aggregating the results, either by averaging them or by taking a max."
        },
        {
            "start_time": 1170,
            "end_time": 1200,
            "transcription": " A possible justification for pooling many neurons by a max is if we're thinking of those neurons as feature detectors where they'll go off if the feature that they're looking for has happened, then taking a max over several nearby windows would tell us, did the feature detector go off in any of those windows?"
        },
        {
            "start_time": 1200,
            "end_time": 1230,
            "transcription": " imperfect analogy because we don't know exactly what function the neurons are learning. But max pooling is very effective at reducing the dimensionality of the problem. And in practice often results in very effective deep learning. So let's suppose that after our convolutional layer, we added a five by five max pooling. Now our number of neurons"
        },
        {
            "start_time": 1230,
            "end_time": 1260,
            "transcription": " is reduced by a factor of 5 on both the height and width dimensions. So after pooling, we have only 4800 neurons, which is much less than our original 200 by 300 by 3 inputs in the image. And so adding another hidden layer after this one will no longer be a disaster."
        },
        {
            "start_time": 1260,
            "end_time": 1290,
            "transcription": " exercise to think about would be if we added another convolutional layer after our max pooling, and you can take your pick of kernel size and strides, how many neurons and how many parameters would we end up with for that convolutional layer that is still doing two-dimensional convolution over the height and width of this tensor, but where each function that is applied"
        },
        {
            "start_time": 1290,
            "end_time": 1320,
            "transcription": " to a given window, will receive input from all 50 neurons across the depth dimension. I recommend trying to work this out with paper and pencil, and then check your work by creating such a network in your favorite library, and having it print out the summary information for that model, which will tell you exactly how many neurons and how many parameters each layer has. The last thing to mention is that we've been"
        },
        {
            "start_time": 1320,
            "end_time": 1350,
            "transcription": " considering two-dimensional convolution when we are thinking about sliding windows over a two-dimensional image, but it's entirely possible to do convolution on inputs with other dimensions. One-dimensional convolution would mean that we are sliding windows over an input"
        },
        {
            "start_time": 1350,
            "end_time": 1380,
            "transcription": " and so the nodes in the layer could be arranged much like we do for a dense network, but instead of connecting to everything, we would have neurons connected to only nearby inputs in the previous layer. One-dimensional convolution is great for time series data, and an example where that can come up is processing audio signals. Three-dimensional convolution comes up frequently in video"
        },
        {
            "start_time": 1380,
            "end_time": 1410,
            "transcription": " processing, where we can think of the frames of a video as a third dimension, and we might want to not only slide our windows across the height and width of the image, but also through the time dimension represented by the video frames. These other sorts of variants may have their own hyperparameters, and there are other hyperparameters that you may see when using deep learning libraries."
        },
        {
            "start_time": 1410,
            "end_time": 1424.27725,
            "transcription": " But I hope that this has given you enough of a conceptual introduction to 2D convolution and the parameters that you might care about to start applying it in your own projects."
        }
    ],
    "Avoiding Neural Network Overfitting (DL 12)": [
        {
            "start_time": 0,
            "end_time": 30,
            "transcription": " As we start working with larger and larger neural networks to do genuine deep learning, the potential for overfitting increases dramatically. And so we need to spend some time thinking about what causes overfitting and how we can detect and avoid it. In general, overfitting is the problem of a machine learning model becoming too specific to its particular training set and not generalizing well to new data."
        },
        {
            "start_time": 30,
            "end_time": 60,
            "transcription": " The primary cause of overfitting is when a model has too much freedom in its parameters relative to the amount of data it trains on. And so models with a high degree of freedom or with small training sets are the most prone to overfitting. If we think about an example like polynomial regression, when we increase the degree of the polynomial, we have more free parameters to tune, and so the model"
        },
        {
            "start_time": 60,
            "end_time": 90,
            "transcription": " model can fit the training data more precisely, but that would be a bad thing if it comes at the cost of less ability to generalize to examples outside the training set. In the context of neural networks, our parameters are the weights and biases. And so as our neural networks get bigger and have more and more weights, they will have more and more freedom to choose their parameters. So whenever we"
        },
        {
            "start_time": 90,
            "end_time": 120,
            "transcription": " we're training a large neural network we want to be on the lookout for potential overfitting. And the number one way to identify overfitting is by keeping an eye on the validation set. This is the primary reason why we tend to split our data set three ways into training, validation, and testing sets so that while holding out a test set for later, we can, during training, look at the validation set to assess whether"
        },
        {
            "start_time": 120,
            "end_time": 150,
            "transcription": " the network can generalize or whether it's overfit to the training data. The sorts of symptoms we're looking for are when the loss or the accuracy gets out of line between the training set and the validation set. Most of the time, over the course of our epochs of training, our loss on the training set will be decreasing. If we ever see the training set loss"
        },
        {
            "start_time": 150,
            "end_time": 180,
            "transcription": " to increase, that's definitely a sign of a problem, but that doesn't happen very often if we've chosen a reasonable model and hyperparameters. But if we're also observing the loss on the validation set, we hope to see the validation set loss decreasing roughly in line with the training set loss, though it may sometimes lag behind slightly, but if we reach a point where the loss on the validation set"
        },
        {
            "start_time": 180,
            "end_time": 210,
            "transcription": " set starts to increase while the training set loss is still decreasing, that's a very strong sign that the network is overfitting to the training set. Similarly, if we're working on a classification problem, we can look at the accuracy of the model on both the training and validation sets. And once again, if we start to see the results on the validation set,"
        },
        {
            "start_time": 210,
            "end_time": 240,
            "transcription": " set diverge, that's a sign that we're overfitting. So whenever you're training a neural network, you should be keeping an eye on the loss and or the accuracy on both the training set and the validation set so that you can identify if the network is overfitting. But if we think that our network might be overfitting, what sorts of things can we do about it? Well, our first two options are to address the causes of overfitting."
        },
        {
            "start_time": 240,
            "end_time": 270,
            "transcription": " overfitting is more likely if we don't have enough data. And so the first thing we might think to do is go find more data to train on. And this is exactly why if you read about some of the big recent successes in deep learning, you'll see that those models were trained on enormous sets of data, things like all of the text that could be crawled from the web, or all of the video from thousands of self-driving cars."
        },
        {
            "start_time": 270,
            "end_time": 300,
            "transcription": " or all of the histories from millions of games of Go or Chess. But unfortunately, when we're training our own neural networks, we may not have access to data or compute at that kind of scale. And so there may be some things we can do to take advantage of additional data, and we'll talk in the future about techniques like data augmentation or transfer learning that can help with this. But we will also sometimes need to think about combating the"
        },
        {
            "start_time": 300,
            "end_time": 330,
            "transcription": " cause of too much parameter freedom. In particular, if we are training a very large neural network on a not very large data set, we shouldn't expect good results. And if we can't scale up the data set, we should think about scaling down the model. In addition to combating overfitting, reducing the size of our network can also pay huge dividends in terms of efficiency. And so if we are compute limited, it's important not to pick too big of a network. And so the general"
        },
        {
            "start_time": 330,
            "end_time": 360,
            "transcription": " rule of thumb is that you should choose the neural network architecture that's just big enough to solve your particular problem. But supposing we've picked what we think is a reasonable size of model and a reasonable amount of data, and we're still worried about overfitting, what else can we do? Well, the first idea should be an obvious one based on how we could detect overfitting, which is that if we notice a separation between"
        },
        {
            "start_time": 360,
            "end_time": 390,
            "transcription": " the training and validation sets, at that point we should just stop training. And this is a thing that we can write code to do automatically. We can detect if the loss or the accuracy on the validation set has stopped improving and cut off training even if we haven't reached the maximum number of epochs."
        },
        {
            "start_time": 390,
            "end_time": 420,
            "transcription": " But there are also more advanced techniques we can use, and to motivate dropout and regularization, it helps to think about what's going on in a neural network when it overfits. Well, we know that overfitting means that a model is becoming overly specific to its particular sample of training data, and in a neural network that means that our weights are becoming overly specific, and that individual points are having two"
        },
        {
            "start_time": 420,
            "end_time": 450,
            "transcription": " big of an effect on the network's activations. And so one potential solution would be to directly combat this by getting rid of particular parts of a data point or particular nodes in the network to prevent them from having an outsized effect. And this leads us to the idea of dropout where we will randomly zero out some of the activations in the network during the training."
        },
        {
            "start_time": 450,
            "end_time": 480,
            "transcription": " The idea is that if we have a large number of activations and we're worried that they are getting too specific, then when we're training on some particular data point, we can just drop out a few of the neurons and that forces the functions learned by subsequent layers of the network to not rely on those neurons always being active."
        },
        {
            "start_time": 480,
            "end_time": 510,
            "transcription": " We accomplish this by setting some dropout probability. And when we do a forward pass to compute activations on a batch, for any layers where we are performing dropout, each entry in the batch of activations has a probability p of being set to zero. That zero activation means that the node will continue to"
        },
        {
            "start_time": 510,
            "end_time": 540,
            "transcription": " contribute nothing to the weighted sums of inputs at the next layer. And so it's as though, for that particular data point, the neuron was dropped from the network. Then when we perform back propagation, for any cases where the node had its activation zeroed, it will also have no contribution to the deltas. And so our gradient descent update will not be affected by neurons that were dropped out."
        },
        {
            "start_time": 540,
            "end_time": 570,
            "transcription": " In general, when we're making predictions after training is complete, we want to use all of the neurons available to make the best possible predictions. And so if we're not doing dropout when we're testing, then the weighted sums of inputs will generally be larger. And so we need to make an adjustment to account for this. And so for any layer where we were doing"
        },
        {
            "start_time": 570,
            "end_time": 600,
            "transcription": " dropout, we should adjust the weights down by a 1 minus p factor to compensate for the fact that there will now be more neurons contributing to the weighted sum of inputs at the next layer. The final method I want to talk about for combating overfitting is weight regularization. And this comes back to the notion that overfitting means the weights are getting overly specific. And the idea is that we can discourage weights that depend on"
        },
        {
            "start_time": 600,
            "end_time": 630,
            "transcription": " depend too much on particular inputs by discouraging the weights from becoming too large. And we do that by changing the loss function that we use to train the network. Specifically, we will add a new term to the loss function that serves as a penalty discouraging large weights."
        },
        {
            "start_time": 630,
            "end_time": 660,
            "transcription": " One example of this would be a quadratic penalty term, which is the sum of the squares of all of the weights. This quadratic penalty, where we sum up the square of every weight, is also known as L2 regularization."
        },
        {
            "start_time": 660,
            "end_time": 690,
            "transcription": " So if we want to incorporate L2 regularization into our training, then we need to add it to our existing loss function. And so we get a new loss function that is a weighted sum of our original loss and our L2 regularization. This gives us a hyperparameter lambda."
        },
        {
            "start_time": 690,
            "end_time": 720,
            "transcription": " which controls how much emphasis do we put on the regularization portion of the loss versus the cross entropy or other standard training loss. And we can augment our computational graph to include the L2 regularization. And so now the loss of this network"
        },
        {
            "start_time": 720,
            "end_time": 750,
            "transcription": " combines the categorical cross entropy and the L2 regularization terms. And when we compute derivatives, the weights will contribute to the loss both through their usual contribution to the cross entropy and through the derivative of this L2 regularization. And so that means that the training can increase the weights if it decreases the training loss enough, but it can't just"
        },
        {
            "start_time": 750,
            "end_time": 780,
            "transcription": " increase the weights arbitrarily because that will increase the L2 penalty. So overall, regularization discourages large weights and therefore discourages weights that are overly specific to particular activations and can thus help to combat overfitting. Overall, the lessons here are to keep an eye out for overfitting whenever you're training a neural network"
        },
        {
            "start_time": 780,
            "end_time": 796.4793125,
            "transcription": " And to think about how big is your model and how much data do you have, and then also to consider techniques like early stopping, dropout, and regularization if you think that your training will be prone to overfitting anyway."
        }
    ],
    "Training large networks with little data\uff1a transfer learning and data augmentation (DL 14)": [
        {
            "start_time": 0,
            "end_time": 30,
            "transcription": " An extremely common occurrence in deep learning is that we find ourselves wanting to solve some problem that we know deep learning is good at, but we also know that training a deep model requires an enormous amount of data, and for our particular problem, we have nowhere near enough. This comes up with all kinds of different problems and all kinds of neural network architectures, but for now let's think about the case of an image processing task with a convolutional network"
        },
        {
            "start_time": 30,
            "end_time": 60,
            "transcription": " where our image data set is relatively small. But keep in mind that the ideas will translate to many other domains. We know that deep convolutional networks are good at image processing, but if we were to train a deep convolutional network on a small set of images, we would expect extreme overfitting where the network basically just memorizes the input data. In such cases, we need to either find some way to make better use"
        },
        {
            "start_time": 60,
            "end_time": 90,
            "transcription": " use of our data or find other data we can use. And if we can collect a larger data set for our problem, we totally should. But if we can't, we can turn to data augmentation and transfer learning. Transfer learning is one of the most important ideas in modern deep learning, and yet it's remarkably simple to explain. The core concept is that we will train a deep neural network on"
        },
        {
            "start_time": 90,
            "end_time": 120,
            "transcription": " some related but more general problem than the one we're trying to solve, and then we'll just reuse that pre-trained network with a bit more training on our dataset to solve our problem. In an image processing setting, we could train a network on large image datasets that have been scraped from the web or compiled for various machine learning competitions, and a network trained on those datasets would have a"
        },
        {
            "start_time": 120,
            "end_time": 150,
            "transcription": " last layer that's used for performing classification on those types of images. So now if we're doing some other image processing task with a different sort of output layer, we could throw away the outputs of this network, but take the rest of the network and add our own output layer. So we can choose an output layer shape that matches the"
        },
        {
            "start_time": 150,
            "end_time": 180,
            "transcription": " the problem we're trying to solve, such as the number of labels we want our classifier to output, and adding this new layer will mean adding new weights that connect it back to the last layer of the pre-trained network. And so those weights will require training, and we can train them using our small data set. So at this point, it's reasonable to ask why we would expect this to work."
        },
        {
            "start_time": 180,
            "end_time": 210,
            "transcription": " of the hope is that if the problem we did our pre-training on is similar enough that it would share functionality with our problem, then much of that functionality will transfer over when we preserve most of the network. One way to think about what's going on is that if we pre-trained on a generic image processing task, and now we want to solve a smaller, specific image processing problem,"
        },
        {
            "start_time": 210,
            "end_time": 240,
            "transcription": " these two different image processing problems, if we were to solve them by hand, would probably use some of the same libraries for basic functionality. And so we might hope that the large neural network for the generic problem would, along the way, learn some generic image processing functions that we can then take advantage of when we train a little bit on our specific problem."
        },
        {
            "start_time": 240,
            "end_time": 270,
            "transcription": " Another perspective for thinking about what's going on here is that if we have one dense layer at the end of the network that's performing classification, we know that each neuron in that layer is computing a relatively simple function of whatever it received from the previous layer. And so the pre-trained network must have done some pretty substantial transformations on the input data."
        },
        {
            "start_time": 270,
            "end_time": 300,
            "transcription": " to arrive at something that we could then do a simple processing on at the end to produce the classification or other result of the pre-training. And so we could think of all of these previous layers as a type of input transformation, a much more advanced version of giving a linear model quadratic inputs, where what all of these layers have"
        },
        {
            "start_time": 300,
            "end_time": 330,
            "transcription": " learned to do is transform input images into something that can be easily classified. And so we can think of transfer learning as applying this learned transformation to our small data set, and then learning how to go from the transformed inputs to the outputs for our problem. When doing transfer learning, we generally have some options for how to use the pre-transfer"
        },
        {
            "start_time": 330,
            "end_time": 360,
            "transcription": " model, we definitely have to throw away its output layer because the outputs need to match up with our problem. But if we wanted, we could also throw away other layers of that model if we think that the useful pre-processing has been done earlier in the network. And if we wanted to, we could add on multiple layers to do more sufficient"
        },
        {
            "start_time": 360,
            "end_time": 390,
            "transcription": " sophisticated processing of the training that we're doing for our problem. Another option that's available to us, which can help ensure that any useful processing these early layers are doing gets preserved, is when we are retraining, we could explicitly freeze the weights on those earlier layers. This is especially useful if the"
        },
        {
            "start_time": 390,
            "end_time": 420,
            "transcription": " the pre-trained model had an enormous data set and our problem has a very small data set, in which case it's unlikely that we'll get substantial improvement to those early layers from training on our little bit of data. So now, whenever we're solving a problem on a small data set, but there exist models trained on related data sets, we should consider applying transfer learning."
        },
        {
            "start_time": 420,
            "end_time": 450,
            "transcription": " All of the deep learning libraries we've encountered have what are known as model zoos for facilitating transfer learning. A model zoo is a collection of pre-trained models for different types of problems that you can try out as a starting point for transfer learning when you are solving smaller problems. And this is a huge part of what makes deep learning an accessible tool"
        },
        {
            "start_time": 450,
            "end_time": 480,
            "transcription": " for solving a wide range of different problems. But sometimes even with the assistance of transfer learning, our dataset isn't big enough to do a good job training a network, even if we're only working on the last few layers. And in that case, we need to think about how can we get as much information out of our dataset as possible, which leads us to the idea of data augmentation."
        },
        {
            "start_time": 480,
            "end_time": 510,
            "transcription": " idea of data augmentation is to apply transformations to the data set that will look different to the neural network, but will mean the same thing to the humans or to whatever system is using the learned model. In the case of image processing, there are a number of different transformations we can apply to an image that will not change"
        },
        {
            "start_time": 510,
            "end_time": 540,
            "transcription": " a human's perception of what that image represents. For example, if we're classifying an image of an insect, we could rotate the image and a human looking at the rotated image would still be able to identify what sort of insect is pictured. Likewise, we could zoom in on the image and crucially, these sorts of transformations mean that the input data"
        },
        {
            "start_time": 540,
            "end_time": 570,
            "transcription": " being seen by the neural network is substantially different. When we rotate the image, that means that virtually all of the pixel values in the input tensor have been changed. And this will make it much harder for the network to memorize the specific input examples or to rely on fine details of what pixels appear where. But we do have to think about whether the"
        },
        {
            "start_time": 570,
            "end_time": 600,
            "transcription": " transformations will change the meaning of the data. For example, if we zoom in too far, then it would no longer be possible to identify what insect is in the image. And we have to think about whether the transformations actually look different from the neural network's perspective. For example, if we think about a translation of the image, I claim that this transformation doesn't make sense"
        },
        {
            "start_time": 600,
            "end_time": 630,
            "transcription": " much difference if we're training a convolutional network. And that's because when we do convolution, the same neuron gets applied to many different subregions of the image, translated across the image by our strides amount, and that tends to make convolutional networks relatively invariant to translations of the input image. And so this is probably a low-prime"
        },
        {
            "start_time": 630,
            "end_time": 660,
            "transcription": " that won't make a big difference in the network's ability to generalize. But there are many other ways that we can modify the data, such as adding a little bit of random noise, that is not enough to distort how we would perceive the picture. And likewise, we could blur the image slightly so that it's still recognizable. And so as long as we can see the image"
        },
        {
            "start_time": 660,
            "end_time": 690,
            "transcription": " any of these transformations are easy to compute, then whenever we're loading in a batch of data, we could choose some random transformations out of this set to apply to each of the input images before we pass that batch of data through the network. And that way, if we train our network for a large number of epochs on our small data set,"
        },
        {
            "start_time": 690,
            "end_time": 720,
            "transcription": " It won't just be seeing the exact same inputs over and over again and memorizing them. Importantly, none of these data augmentation techniques are anywhere near as good as finding more data to train on, and that includes having related data to use for pre-training. But data augmentation in combination with transfer learning can help us to solve a much wider range of problems using deep neural networks."
        },
        {
            "start_time": 720,
            "end_time": 720.271375,
            "transcription": " you"
        }
    ],
    "LSTMs (DL 18)": [
        {
            "start_time": 0,
            "end_time": 30,
            "transcription": " The goal of this lecture is to give you a sense of how recurrent neural networks are actually used for language modeling in practice. Last time we talked about using a recurrent neural network on the problem of predicting the next word in a sentence. We picked that example in part because it gives us an easy way to demonstrate how a recurrent neural network can operate, but also in part because that's a common pre-training task used on recurrent neural networks."
        },
        {
            "start_time": 30,
            "end_time": 60,
            "transcription": " want to use an RNN for a more interesting task like question answering or machine translation. Then in a transfer learning type approach, we might first train it on a predict the next word type of problem and then fine tune it for the particular task that we're interested in. So the first topic we should consider is how can we get something more interesting than just a next word prediction as output from an RNN. And the key idea is"
        },
        {
            "start_time": 60,
            "end_time": 90,
            "transcription": " that these hidden activations that we're passing back into the network or when we unroll the network that we're passing forward in time are actually the state that we really care about. We can think of this predict the next word task as a throwaway pre-training like we did for word embeddings. And then when we actually apply a recurrent neural network to a problem, what we're really interested in is this output here. That is, if we want to"
        },
        {
            "start_time": 90,
            "end_time": 120,
            "transcription": " take some text as input, say a sentence that we're trying to translate to some other language, then we can give each word in the sentence to the recurrent neural network in order. And then once we've given all of the tokens in that sentence to the network, the hidden state that is produced at the last time step is a representation of the entire text"
        },
        {
            "start_time": 120,
            "end_time": 150,
            "transcription": " that we've given. And so then we can feed this hidden state that comes from the last time step and represents the entire text from the neural networks perspective into some other neural network layers that will solve the task that we're interested in solving on the text that we were given. So if we were trying to classify the text, we might give this representation of the text to a few"
        },
        {
            "start_time": 150,
            "end_time": 180,
            "transcription": " feed forward neural network layers that can perform classification. But if what we want is a text as output, then we could do something like inverting this text input that we've done to produce the output. So we're now thinking of this entire process of feeding a whole sentence or a whole text of some sort to a recurrent neural network as a way of"
        },
        {
            "start_time": 180,
            "end_time": 210,
            "transcription": " of encoding that text into a neural network hidden layer activation state that we can then do subsequent processing on. And so we can now build another similar neural network that will operate as the decoder."
        },
        {
            "start_time": 210,
            "end_time": 240,
            "transcription": " encode a sentence into the state of the neural network by feeding each token in that sentence into the recurrent network. And so in this encoding process, we are ignoring the outputs at all of the intermediate time steps and simply taking the activations at the end as an encoded representation of the entire text. Then,"
        },
        {
            "start_time": 240,
            "end_time": 270,
            "transcription": " We can have a second recurrent neural network that we train to take as input this encoding of a text and produce one token of output at a time. And as the state gets passed back into this decoder network, it will update along the way, allowing us to produce the next word of the output sentence."
        },
        {
            "start_time": 270,
            "end_time": 300,
            "transcription": " And so we can train this pair of recurrent neural networks to take in a sentence of text as input and then produce another sentence of text, either a translation into some other language or an answer to the question or a next line of a story as the output. The trouble is that we said last time that we were using 10H activations in these recurrent neural networks."
        },
        {
            "start_time": 300,
            "end_time": 330,
            "transcription": " And we know that 10H functions are subject to vanish ingredients. And so if we want to translate anything much longer than this very simple sentence, we are going to have problems with training such a big network with 10H activation functions. Simply because the 10H activations will make it hard for errors to propagate far back through"
        },
        {
            "start_time": 330,
            "end_time": 360,
            "transcription": " of these recurrent neural networks if we have a large number of time steps. The core idea to how this problem is solved has a lot in common with the idea of residual networks that we saw for improving deep convolutional networks, which is that we want to create a path that the gradients can take backwards through the network that is subject to far less squishing"
        },
        {
            "start_time": 360,
            "end_time": 390,
            "transcription": " by 10H activation functions and therefore lets gradients propagate all the way through a long encoding and decoding sequence. We can achieve this by modifying the recurrent neural network to have multiple paths that activations can take that are specifically designed for the purpose of encoding the information in the text and allowing gradients to perform"
        },
        {
            "start_time": 390,
            "end_time": 420,
            "transcription": " propagate back through the network."
        },
        {
            "start_time": 420,
            "end_time": 450,
            "transcription": " The LSTM layer illustrated here achieves this goal of allowing us to encode and decode long sequences of text by allowing gradients to propagate efficiently backwards through the recurrent network. We have an input and an output to the recurrent layer that we can use like we talked about last time to train on a predict the next word task or other pre-trained"
        },
        {
            "start_time": 450,
            "end_time": 480,
            "transcription": " of the network and the input here gets concatenated together with one of the hidden states that were passed along from the previous layer. So we can think of this bottom input as corresponding to the hidden state that we're passing along in the plane recurrent network before. But we also have this additional hidden state"
        },
        {
            "start_time": 480,
            "end_time": 510,
            "transcription": " see that we are passing along from the recurrent network to itself on each time step. So the main path through this network that corresponds to a vanilla recurrent network is that the hidden state and the input are concatenated together and then they go into this tan H layer which is a fully connected neural network layer with learnable weights. And then"
        },
        {
            "start_time": 510,
            "end_time": 540,
            "transcription": " that is eventually passed through here and becomes the output of this particular time step and the hidden state that is passed along to the next time step. So this path through the layer corresponds to the simple recurrent neural network."
        },
        {
            "start_time": 540,
            "end_time": 570,
            "transcription": " This hidden state on this screen path gets concatenated with the current input and then passes through one fully connected layer with 10H activations. But we also have several fully connected neural network layers with sigmoid activation functions. And for each of these sigmoid layers, their output goes into an element-wise multiplication. We know that sigmoid activation functions output a value between 0 and 1."
        },
        {
            "start_time": 570,
            "end_time": 600,
            "transcription": " Each of the sigmoid layers is being element-wise multiplied with some other tensor. We know that the result will be whatever other tensor we had, all of its elements will be multiplied by some value between 0 and 1. And so we can think of multiplying by 0 as forgetting that element of the previous activations. We can think of multiplying by 1"
        },
        {
            "start_time": 600,
            "end_time": 630,
            "transcription": " as remembering that element and multiplying by something between zero and one as partially remembering it. And so we can think of each of these sigmoid layers as learning what part of a previous activation do we want to pass through to subsequent computations and what do we want to forget. So for example, this first sigmoid layer is telling us of the hidden"
        },
        {
            "start_time": 630,
            "end_time": 660,
            "transcription": " state C that we got from the previous time step, which part do we want to forget? And we can think about this second layer as learning of the outputs from the tanh based on the input from the previous hidden layer and the inputs concatenated together. What part of the tanh activations do we want to pass along and which parts do we want to ignore?"
        },
        {
            "start_time": 660,
            "end_time": 690,
            "transcription": " And here we are adding together the h and c portions of the network and then passing them through a 10 h activation function. But of note, this 10 h is not a learned layer. It is simply an activation function applied to this addition to squish it back to the range minus one to plus one. And then the output of that 10 h will have a"
        },
        {
            "start_time": 690,
            "end_time": 720,
            "transcription": " another sigmoid element-wise multiplied with it to determine what do we want to pass along and what do we want to forget. So each of these sigmoid layers applies to the previous hidden state and the current input. Those are concatenated together and fed into the sigmoids. And each of these is a fully connected neural network layer with learnable weights. And so we will be learning how to base"
        },
        {
            "start_time": 720,
            "end_time": 750,
            "transcription": " based on the previous state and the current input, decide what information we want to pass through each of these element-wise multiplication operations. So we can think of these sigmoid activations as gates that decide what information we want to allow to pass through versus what information do we want to cut off, and the functioning of those gates is a thing that will be learned from the training data."
        },
        {
            "start_time": 750,
            "end_time": 780,
            "transcription": " important deviations here from the plane recurrent neural network are that we have some control of what information gets passed along and what information gets blocked based on these learned gates. And so these gates control what portion of the hidden state combined with the new input gets passed along to the next time step in the recurrent network."
        },
        {
            "start_time": 780,
            "end_time": 810,
            "transcription": " But then we also have this additional C hidden state that is also getting passed along to the next step and this C output does not directly pass through any activation function. And so if we follow the path backwards through the time steps of the recurrent network, this C path allows us to propagate gradients backwards through time with"
        },
        {
            "start_time": 810,
            "end_time": 840,
            "transcription": " out the decay of a 10H activation function. But at every time step, this C path does get combined with the H path. And so the learned encoding that corresponds to what we had in the previous version of a recurrent neural network is getting incorporated into these Cs at every time step. And so we can think of these first two signals"
        },
        {
            "start_time": 840,
            "end_time": 870,
            "transcription": " We could in principle based on some particularly important input decide that we're going to have lots of zeros here and forget most of the information from the C path."
        },
        {
            "start_time": 870,
            "end_time": 900,
            "transcription": " and presumably at the same time we would want to pass along more information from the path where the new input has come in. And so these gates give us the opportunity as we are taking in new information later in a text to forget old information that is less important to the encoding of the overall text. Of note, in practice we don't actually know"
        },
        {
            "start_time": 900,
            "end_time": 930,
            "transcription": " what functions each of these internal layers will be learning. They are made up of parameters that will be trained by stochastic gradient descent. And so in much the same way that with convolutional neural networks, we don't actually know what each of the feature detectors is computing. We don't actually get to decide what function each of these gates and the tanh layer is computing. But we have set up the architect"
        },
        {
            "start_time": 930,
            "end_time": 960,
            "transcription": " in such a way that we think it has the potential to learn useful types of functions. In addition, in analogy to residual networks, we have two paths through each component of the network, where in residual networks we had the path that went through the block and the shortcut path that went around the block that then got combined. Here we have the H path that goes through the 10H activations and the C path"
        },
        {
            "start_time": 960,
            "end_time": 990,
            "transcription": " path that doesn't have activations applied directly. And so these two paths give us both the ability to do interesting computations within a particular LSTM time step and the ability to propagate gradients rapidly backwards through many applications of an LSTM network. So in summary, there are some theoretical justifications behind the"
        },
        {
            "start_time": 990,
            "end_time": 1005.633,
            "transcription": " architecture choices of an LSTM, but the reason that they are so widely used is that in practice they have found to be effective compared to other types of recurrent neural networks."
        }
    ],
    "What is Deep Learning\uff1f (DL 01)": [
        {
            "start_time": 0,
            "end_time": 30,
            "transcription": " Hi, welcome to Deep Learning. I'm Bryce, and I'm excited to help you learn about one of the hottest topics in computer science. So, what is deep learning, and why should you be interested? Well, deep learning is all around us every day. The algorithms that recognize your face when you unlock a device, or understand your speech when you talk to a digital assistant, or recommend you content on your favorite platform, are all based on deep learning. And that's all great, but"
        },
        {
            "start_time": 30,
            "end_time": 60,
            "transcription": " What does deep learning mean? Well, I would define deep learning as the use of neural networks and differentiable programming to perform machine learning. But that just raises more questions. What are neural networks and what is machine learning? Well, let's start with ML. If you asked me to categorize machine learning, I would tell you that it lies at the intersection of artificial intelligence and data science."
        },
        {
            "start_time": 60,
            "end_time": 90,
            "transcription": " Data science is all about organizing, analyzing, and making good use of data. AI is about solving problems computationally that seem to require intelligence when solved by humans. So machine learning is about making intelligent inferences automatically from data. And this requires a bit of a perspective shift from how we normally do things in computer science, where we'll usually design algorithms to"
        },
        {
            "start_time": 90,
            "end_time": 120,
            "transcription": " solve a problem directly. In machine learning, instead, we let the data examples define the inputs and outputs of the problem, and we implement algorithms whose job is to infer the solution to the problem from our data set. Broadly speaking, we can break most machine learning problems down into two categories, regression and classification. In a regression problem, we're trying to infer a function that maps"
        },
        {
            "start_time": 120,
            "end_time": 150,
            "transcription": " continuous inputs to continuous outputs. The simplest example is linear regression, where our goal is to pick the line that best describes the relationship between inputs and outputs for all of the points in our data set. In a classification problem, our goal is to assign a discrete label to each input point. Here, the simplest example is inferring a decision boundary that separates"
        },
        {
            "start_time": 150,
            "end_time": 180,
            "transcription": " where we apply the label 0, from where we apply the label 1. Both regression and classification can get much more complicated than these two-dimensional examples, and lots of interesting problems combine aspects of both classification and regression. For example, a classic deep learning task is object recognition, where we are trying to learn a function that takes as input, an image represented as a grid of pixels,"
        },
        {
            "start_time": 180,
            "end_time": 210,
            "transcription": " and outputs for each object in the image, a bounding box, described in terms of continuous coordinates, as well as a label for what that object is in one of several discrete categories. In this class, we'll start with some simple examples to learn the ropes, and then quickly move on to the much more exciting types of problems that we can solve with deep learning."
        },
        {
            "start_time": 210,
            "end_time": 240,
            "transcription": " And I said before that in deep learning we solve these types of problems using neural networks and differentiable programming, but what's that all about? Well, a neural network is a computational model that's based very loosely on the behavior of neurons in the brain. Specifically, neurons activate and send electrical signals that are then sensed by other neurons, which may"
        },
        {
            "start_time": 240,
            "end_time": 270,
            "transcription": " in turn cause those neurons to activate. Some of the connections between neurons are stronger than others, and some pairs of neurons tend to activate together while others have an inhibitory relationship. And this inspired a mathematical model where nodes in a graph represent neurons and directed edges represent connections between them. And each edge has a numerical weight"
        },
        {
            "start_time": 270,
            "end_time": 300,
            "transcription": " indicating the strength of the connection, where a positive weight indicates that the second edge is excited by the first, while a negative weight indicates that it's inhibited. And then in this model, each neuron can sum up the weighted inputs from each of its neighbors to determine whether it will activate. In this example, our neuron has three incoming connections that all"
        },
        {
            "start_time": 300,
            "end_time": 330,
            "transcription": " have different weights. The two here indicates that this neighbor has a stronger influence on our neuron, while this negative one indicates that when this neighbor is active, that makes our neuron less likely to activate. To determine whether our neuron activates, we start by multiplying each of its neighbor's activation by the corresponding weight and summing them up."
        },
        {
            "start_time": 330,
            "end_time": 360,
            "transcription": " We then compare that sum to our neurons activation threshold. And since 1 is greater than 0.5, our neuron activates, outputting a 1. But this is the point where we in deep learning will break away from neuroscience. There's lots of interesting research out there about how neurons really activate and how the connections between them change."
        },
        {
            "start_time": 360,
            "end_time": 390,
            "transcription": " But we won't be using any of that research. Instead, we will treat the neural network as a practical tool for performing machine learning. And to do that, we need some way of training a neural network based on examples from a data set. And as we'll see over the next few classes, the key idea behind that training is gradient descent. We know from calculus that a function's"
        },
        {
            "start_time": 390,
            "end_time": 420,
            "transcription": " gradient always points in the direction of steepest increase and that we can use that directional information to help us minimize the function. We'll dive much more deeply into this soon, but for now the key idea is that because we need gradients, we need to differentiate our neural networks activations. If we think of the way this neuron activates as a function of"
        },
        {
            "start_time": 420,
            "end_time": 450,
            "transcription": " its input sum. Since the threshold is at 0.5, the function outputs 0 everywhere below 0.5, at which point it jumps up and starts outputting 1. And so our neurons activation is described by a step function. Unfortunately, the derivative of this step function is incredibly uninformative. The derivative of this function"
        },
        {
            "start_time": 450,
            "end_time": 480,
            "transcription": " is zero everywhere that the function is flat and undefined at the one point where it's discontinuous. So if we want to use gradients to train the neural network, we'll need an activation function with nicer derivatives. And the first example we'll explore of a better activation function is a smooth approximation to this step function known as a sigmoid."
        },
        {
            "start_time": 480,
            "end_time": 510,
            "transcription": " In the next few lectures, we'll develop the tools to differentiate activation functions and pass those derivatives around through the network. This will set us up to use stochastic gradient descent to train a neural network on regression and classification problems. But it turns out that a lot of the ideas we'll see for training a neural network also apply"
        },
        {
            "start_time": 510,
            "end_time": 540,
            "transcription": " more broadly. If we think of our neurons as computing a very simple program that performs the weighted sum and applies an activation function, then we could imagine more interesting programs that fill the same role. And this leads us to the idea of differentiable programming, where if we can write functions that we can think about mathematical"
        },
        {
            "start_time": 540,
            "end_time": 570,
            "transcription": " as operating on some numerical inputs and being tuned based on some numerical parameters. And if we can calculate the derivatives of that function, then those functions can also be incorporated into our deep learning models. So in this course, we will start with the simplest possible neural networks"
        },
        {
            "start_time": 570,
            "end_time": 600,
            "transcription": " dig into the basics of machine learning and the math behind stochastic gradient descent. And then we'll gradually add complexity building up to deep neural networks and general differentiable program. Along the way, we'll get lots of practice with powerful deep learning libraries. And we'll extensively discuss the limitations and downsides, both of particular models and of deep learning in general."
        },
        {
            "start_time": 600,
            "end_time": 612.461125,
            "transcription": " By the end of the semester, you should be prepared to design, apply, evaluate, and criticize deep learning models for a wide variety of exciting real-world problems."
        }
    ],
    "Generative Adversarial Networks (DL 23)": [
        {
            "start_time": 0,
            "end_time": 30,
            "transcription": " In the last lecture we covered variational autoencoders, which are one way of doing generative modeling. In the autoencoder approach, our goal is to learn latent variables that are then used for sampling from the generative distribution. But an alternate way of thinking about generating samples from a distribution is to think in terms of computational random number generators."
        },
        {
            "start_time": 30,
            "end_time": 60,
            "transcription": " Whenever you use a random library in any programming language to generate samples from any random distribution, it always starts from a sequence of random or pseudo-random bits. The random number generator then performs some computation to transform that sequence of ones and zeros into a sample from some other distribution."
        },
        {
            "start_time": 60,
            "end_time": 90,
            "transcription": " So this means that if we want to sample from a uniform distribution or from a beta distribution or from any arbitrary random distribution, there is some function in the random library that is transforming a sequence of random bits into some other output. And the way these are constructed, many other distributions are built on top of uniform distributions. So in some cases, the random"
        },
        {
            "start_time": 90,
            "end_time": 120,
            "transcription": " number generator will start with its zeros and ones, transform that into a sample from a uniform distribution, and then subsequently transform that uniform distribution sample into a sample from something else. And this suggests an alternate approach to generative modeling, where we treat our learning problem as coming up with this transformation. And this leads us to the idea of a generator neural network."
        },
        {
            "start_time": 120,
            "end_time": 150,
            "transcription": " A generator neural network takes as input random noise and then transforms that random noise into a random sample from some data distribution. So for example, if our data set was images of puppies, then the goal of this neural network is for any given"
        },
        {
            "start_time": 150,
            "end_time": 180,
            "transcription": " input noise to produce a random picture of a puppy. So our goal is to train this neural network to perform the transformation that takes random noise and turns it into a sample from our data distribution. But how can we train the neural network to do this? Well, the key idea is to use an additional neural network."
        },
        {
            "start_time": 180,
            "end_time": 210,
            "transcription": " The second neural network is known as a discriminator. And its job is to take inputs that are randomly chosen from either a real training data set or from the output of the generator network and determine whether its input was real or fake. And so these are known as"
        },
        {
            "start_time": 210,
            "end_time": 240,
            "transcription": " as adversarial networks. The generator network is trying to produce samples that are close enough to the distribution of real data that they can fool the discriminator network, whereas the discriminator network is trying to distinguish the outputs of the generator from the real data. So how can we train these adversarial networks? Well, the discriminator network"
        },
        {
            "start_time": 240,
            "end_time": 270,
            "transcription": " is fundamentally solving a classification problem. It's being given inputs from two different classes, real data and fake data, and trying to output a label. So training the discriminator looks just like normal training of a neural network for classification. But how should we train the generator neural network to produce outputs that have the potential to fool the discriminator? The key idea is that"
        },
        {
            "start_time": 270,
            "end_time": 300,
            "transcription": " that we can propagate gradients all the way back through the discriminator network and into the generator network. So we can think of this output of the generator network that becomes the input to the discriminator network as just another hidden layer and pass the delta's back through this combined network, allowing us to figure out how we need to update"
        },
        {
            "start_time": 300,
            "end_time": 330,
            "transcription": " the weights in the generator network in a way that improves its objective function. So this raises the question of what loss function do we want to use for the generator network? And a first idea is that we could simply use minus the loss from the discriminator network. The discriminator is performing a classification task where its loss will be minimized if it perfectly predicts"
        },
        {
            "start_time": 330,
            "end_time": 360,
            "transcription": " whether its inputs were real or fake. But the generator's objective could be exactly the opposite, where its loss is minimized if it perfectly fools the discriminator. But this isn't our only option for a loss function for the generator. If we wanted to use some other loss function in training our generator network, we could still propagate the deltas through the discriminator"
        },
        {
            "start_time": 360,
            "end_time": 390,
            "transcription": " into the generator network, but then only update the generator network based on that loss. So we could use some other loss function and still figure out the gradient of that loss function with respect to just the weights in the generator network and then update just the generator network based on that other loss function. So one option is to use the same loss function"
        },
        {
            "start_time": 390,
            "end_time": 420,
            "transcription": " with opposite signs for the two networks, but we also have the option of using different loss functions for these two networks, and we would just have to propagate both loss functions through the discriminator so that the generator's loss could then reach the weights in the earlier network. And this becomes important if we want either the generator or the discriminator to solve a more interesting problem. In particular, we might want either or both of the networks to not generate"
        },
        {
            "start_time": 420,
            "end_time": 450,
            "transcription": " just give any data samples, but to give data samples for specific categories. So for example, we could add some additional information to the generator's input, where it is given both noise and a label that it should condition its sample on. So we might tell it that we want an image of a dog, and so then the generator"
        },
        {
            "start_time": 450,
            "end_time": 480,
            "transcription": " will only be succeeding if it fools the discriminator into outputting that the image is both real and an image of a dog. And so then the discriminator would be doing a more complicated classification task where it is not just predicting real or fake, but rather is predicting that it's either fake or that it came from one of many possible cases."
        },
        {
            "start_time": 480,
            "end_time": 510,
            "transcription": " categories. So now we're training the discriminator on a more complex multi-class classification task and we are training the generator to fool the discriminator in specific ways to make it think that it has produced samples from particular classes within the data distribution. And so in these more interesting cases we can't just use the same"
        },
        {
            "start_time": 510,
            "end_time": 540,
            "transcription": " loss for the two different networks, we need to figure out what is the function that the generator is trying to optimize and what is the function that the discriminator is trying to optimize and choose appropriate loss functions for optimizing each of those two different problems. So let's think about what will happen if we train this pair of adversarial networks."
        },
        {
            "start_time": 540,
            "end_time": 570,
            "transcription": " Initially, the generator network will receive random noise as input, and before its weights have been trained, it will produce essentially random noise as output. And so, the discriminating network will initially learn to distinguish real data from random noise. That should be a fairly easy classification task, and most of the discriminator's training effort will be focused"
        },
        {
            "start_time": 570,
            "end_time": 600,
            "transcription": " on distinguishing the different categories within the real data. Of note, we will typically run training first on the discriminator so that it can learn the distribution of the real data. And only once the discriminator has started to have success in solving that problem will we begin to also train the generator. And it is common to alternate back and forth between training the discriminator and training the discriminator"
        },
        {
            "start_time": 600,
            "end_time": 630,
            "transcription": " the generator so that the discriminator can learn to distinguish the current outputs that the generator is producing. And then once the discriminator has learned to distinguish those, then we will retrain the generator so that it can adapt to what the discriminator has so far learned. So if the discriminator initially learns to separate the real data from a random noise, then the generator will now"
        },
        {
            "start_time": 630,
            "end_time": 660,
            "transcription": " need to produce samples that don't look like random noise and look more like however the discriminator is distinguishing the real data. So then when we start to train the generator, we hope that it will begin to produce outputs that look more like the real data, like images of puppies, if that is the data distribution that we're working from. And as the generator network gets better and better at producing these images,"
        },
        {
            "start_time": 660,
            "end_time": 690,
            "transcription": " the discriminator network's task will get harder and harder for it to separate out the real from the fake images. And if this works, then we will have a neural network that can take random noise and transform it into things that are difficult to distinguish from samples of the real data. But the question then is, will this training actually work? And it's worth thinking about some of the"
        },
        {
            "start_time": 690,
            "end_time": 720,
            "transcription": " the possible failure modes. The main failure mode that we're worried about is the possibility that the generator network will somehow manage to just produce samples that are straight from the true data distribution. A worst case would be if the generator learns to output one specific image that matches exactly with one of the images in the data set. But if that"
        },
        {
            "start_time": 720,
            "end_time": 750,
            "transcription": " the case, if the generator network were learning to always produce the same output, no matter what random noise it was given, then the discriminator could achieve quite high performance by simply outputting fake on that particular image and outputting real for all of the others. And it would get one of its inputs from the real data set wrong, but by getting the rest of them and all of them"
        },
        {
            "start_time": 750,
            "end_time": 780,
            "transcription": " the fake ones correct, it would achieve very low loss. And so if we've set up our loss function for the generator correctly to be trying to fool the discriminator, this will not be minimizing the generator's objective when we go and retrain the generator. So it's reasonable to expect that the generator will succeed in spreading out the outputs that it produces and not just producing the"
        },
        {
            "start_time": 780,
            "end_time": 810,
            "transcription": " same sample every time. But what if the generator network learns to just reproduce some subset or even the entirety of the actual data set? Then it would definitely succeed in minimizing its objective function. So this is a case that we definitely need to worry about and there is substantial potential for overfitting of the generator network to the data set."
        },
        {
            "start_time": 810,
            "end_time": 840,
            "transcription": " This is a problem because if all we wanted to do was produce the real data, we wouldn't have needed to train a generator network to do this. The only reason that it's valuable to train a generator network is if it can produce things that are similar to but not identical to the actual data set. We want to use this generator to produce additional samples of things that go beyond the real data set."
        },
        {
            "start_time": 840,
            "end_time": 870,
            "transcription": " generative adversarial networks to avoid overfitting, we want to be very sure that we are not giving the generator too much opportunity to memorize the actual data. This is why the real data set is not an input to the generator network. The only opportunity that the generator has to learn from the real data is when the real data influences the weights in the discriminator network and then the weights"
        },
        {
            "start_time": 870,
            "end_time": 900,
            "transcription": " in the discriminator network influence the loss that is passed back to the generator. So, it's reasonable to hope that the generator will be unable to directly memorize the data, but even still, we need to be very careful that we don't present the same data too many times. But if all of this succeeds and we can train a generator network, where we can tell it to give us a sample from a particular"
        },
        {
            "start_time": 900,
            "end_time": 930,
            "transcription": " class, or we could instead of giving class labels, we could give it other features that we want it to condition on, we can use the generator network to produce data samples that are useful for other purposes. For example, this is an important way that we could potentially augment the data that we use for training neural networks on other problems."
        },
        {
            "start_time": 930,
            "end_time": 960,
            "transcription": " And in addition, this is a source of many of the fun examples that you will see of neural networks being used as inspiration for art, where we can, after training a generator network to produce images of dogs or cats, we could give it an input of.5 dog.5 cat and see what kind of sample does the generator produce when we try to mix"
        },
        {
            "start_time": 960,
            "end_time": 985.1994375,
            "transcription": " multiple different categories of image. So I will find some examples to share of using generative adversarial networks for art. And I will find some examples for us to discuss later in the week of using generative adversarial networks to produce data that improves our ability to train neural networks to solve other important problems."
        }
    ],
    "Deep Learning Prerequisites (DL 02)": [
        {
            "start_time": 0,
            "end_time": 30,
            "transcription": " What background do you need to have success in a course on deep learning? Deep learning depends on a number of concepts from computer science and math. So there are prerequisites in both fields. In short, if you've taken courses in data structures, linear algebra and multivariable calculus, you're good to go. But it's worth going into a bit more detail about how each of these will come up and what's important to success in a deep learning class. To begin with, this is"
        },
        {
            "start_time": 30,
            "end_time": 60,
            "transcription": " upper level undergraduate computer science, and so I'm assuming a decent amount of programming background. And so the number one reason why data structures is a prerequisite is to ensure that you've done at least that much programming in the past. But we'll also sometimes be interested in algorithmic efficiency and related concepts you would have encountered in data structures. In this class, my videos will mostly use pseudocode or I will express computation"
        },
        {
            "start_time": 60,
            "end_time": 90,
            "transcription": " But in our assignments, you will be programming in both Python and Julia. The reason this course uses two different programming languages is because the best, most popular deep learning libraries are in Python. Specifically, I want you to get practice in TensorFlow and PyTorch along the way in this class. But Julia is the best language I know of for translating between mathematics and programming."
        },
        {
            "start_time": 90,
            "end_time": 120,
            "transcription": " computation. And so when we express ideas about deep learning mathematically and then want to actually write code that implements them, doing that in Julia will make it easiest for us to understand what's going on under the hood of our neural networks. In terms of mathematical background, we will use concepts from linear algebra and from multivariable calculus. However, in both cases,"
        },
        {
            "start_time": 120,
            "end_time": 150,
            "transcription": " the concepts we'll be using are a small fraction of what's generally taught in those courses. And so if, for example, you've only taken one of these classes, it should be possible for you to catch up on the concepts that you need from the other relatively quickly and not fall behind at the beginning of the semester. From linear algebra, the main thing that we need is to be comfortable with matrix notation."
        },
        {
            "start_time": 150,
            "end_time": 180,
            "transcription": " In deep learning, we'll do lots of operations on vectors, matrices, and even higher dimensional arrays, what we'll call tensors. And I will try to break those operations down and show you what's happening to individual elements, but we're also going to need to express those things compactly using matrix and vector notation. So you'll need to be comfortable with doing matrix vector products"
        },
        {
            "start_time": 180,
            "end_time": 210,
            "transcription": " and operations like dot products and norms. An example of the sorts of operations we might see very soon would be multiplying a 3x5 matrix with a 5-element vector, applying some function to the result, taking a difference with some other 3-element vector, and then taking a norm. If you're comfortable with all of those operations, then that's most of what we need from linear"
        },
        {
            "start_time": 210,
            "end_time": 240,
            "transcription": " in your algebra for this class. From multivariable calculus, the key concept we'll be using all semester is gradients. We'll need to evaluate the gradient of many different functions, and gradients are made up of partial derivatives, so we'll need to be comfortable taking partial derivatives. And of course, that relies on the rules that we learned in basic calculus, like the product rule on the quotient rule."
        },
        {
            "start_time": 240,
            "end_time": 270,
            "transcription": " operation will be doing soon would be constructing the gradient vector that contains the partial derivative of this function with respect to each element of the vector w. If these operations make sense to you, that's most of what we need from multivariable calculus to do deep learning. For either multivariable calculus or linear algebra, if you haven't had a full course on that topic, or if you're feeling a bit"
        },
        {
            "start_time": 270,
            "end_time": 300,
            "transcription": " rusty and want to brush up, I will post a playlist with videos by Grant Sanderson that you should watch to get up to speed. Both of those playlists contain videos for an entire course in the subject, and so I will highlight the specific videos that are most important for the concepts that we'll actually be using. So if these concepts are unfamiliar, or if it's been a little while since you've used them, I recommend going through those videos to make sure"
        },
        {
            "start_time": 300,
            "end_time": 304.6574375,
            "transcription": " you're ready to go for our activities in the first week of class."
        }
    ],
    "Computation Graphs (DL 25)": [
        {
            "start_time": 0,
            "end_time": 30,
            "transcription": " This lecture is about computational graphs, which are a way of visualizing the data flow and sequence of computations in any program, but are most often used for understanding forward and backward propagation in neural networks. Here we have a small slice of a neural network, where we're showing one node in each of the last three layers, and some of the edges coming into and out of those nodes. This visualization captures all of the"
        },
        {
            "start_time": 30,
            "end_time": 60,
            "transcription": " computations going on in a neural network if we understand all of the implicit operations like multiplying weights by activations, summing the incoming weights to a node, computing the activation function for a node, and computing the loss at the end of the network. But sometimes it's useful to make those different computational steps more explicit."
        },
        {
            "start_time": 60,
            "end_time": 90,
            "transcription": " Here I've drawn a slice from a computational graph where the goal is to make all of the implicit operations in the neural network representation explicit. So for example, in the computational graph we have nodes for the activation A1 and the weight W1 and those variables in the program"
        },
        {
            "start_time": 90,
            "end_time": 120,
            "transcription": " are brought together into a node that computes their product. And then the product A1 times W1 is one of many inputs to the sum of all of the incoming values here. So this node computes the sum of weighted inputs for node two. And so the output of this sum node is what we have been calling x"
        },
        {
            "start_time": 120,
            "end_time": 150,
            "transcription": " to the input to this node. And then that goes into a node that computes a sigmoid activation function where the output of the sigmoid is a2. And then we proceed further, right? This output goes to several other nodes in the computational graph because several nodes at the next layer all receive a2 as inputs. But in particular,"
        },
        {
            "start_time": 150,
            "end_time": 180,
            "transcription": " among those is that it gets multiplied with W2 and then summed with all of the inputs to node three, which gives us the input X3. And then we compute a softmax where that softmax depends not just on X3, but on several other weighted sums of inputs, all the others for this layer. And so that gives us the activation of this node. And then that activation"
        },
        {
            "start_time": 180,
            "end_time": 210,
            "transcription": " is one of many inputs to a categorical cross entropy loss that we're computing in order to start our back propagation. So the idea of this computational graph is that everything that could be a variable in the program that implements this computation is a node, and we have edges in this graph indicating all of the dependencies between variables in the program. If our variable for this"
        },
        {
            "start_time": 210,
            "end_time": 240,
            "transcription": " activation is an input to several other computations in the program, then it has edges to those other computations. And sometimes we're explicitly representing things that wouldn't be given a name in our code, but would be an intermediate value that is computed as part of a larger computation. And we could in fact break down some of these computations even further. We know that a sigmoid activation function is 1 over 1 plus"
        },
        {
            "start_time": 240,
            "end_time": 270,
            "transcription": " e to the minus x. We could break that down into e to the minus x, 1 over 1 plus that intermediate result, or other levels of granularity. But the key choice in the level of granularity that we're interested in is that we want to know how to take the derivative of any node in our computational graph. Because our goal here is to represent both the forward and back propagation steps of our"
        },
        {
            "start_time": 270,
            "end_time": 300,
            "transcription": " neural network. And so if we know how to take the derivative of any node in our graph, then it's clear how the partial derivatives propagate backward through the network. We could in principle use this computational graph to compute the outputs of a neural network. We would begin with a topological sort of the graph where all variables appear after everything else they depend on. And so then we could go through the graph in order and when we get to any given"
        },
        {
            "start_time": 300,
            "end_time": 330,
            "transcription": " variable in the graph, we know all of its inputs and so we can compute the value of that variable. But the real value of this representation is in showing where the partial derivatives of the parameters that we're modifying with gradient descent come from. When we do gradient descent on a neural network, the thing that we need to compute is the partial derivative of the loss with respect to each of the weights. So for example, the partial derivative of the"
        },
        {
            "start_time": 330,
            "end_time": 360,
            "transcription": " derivative of L with respect to weight 2. And by choosing the nodes of this graph such that we know how to take the derivative of any given node, we can use this graph to work our way backwards and figure out the partial derivative for any of the parameters that we want as part of our gradient descent step. For example, suppose that we have the output"
        },
        {
            "start_time": 360,
            "end_time": 390,
            "transcription": " delta for this softmax node. That would be the partial derivative of the loss with respect to x3. So this would be delta for output node 3. If our goal is to carry the partial derivatives backwards through the network, then at each step we are applying the chain rule, where we are multiplying the derivative of this"
        },
        {
            "start_time": 390,
            "end_time": 420,
            "transcription": " operation by our derivative so far. So if we have carried a derivative this far, then we need to multiply that by the derivative of this operation with respect to the particular input we're interested in. So here if we take the partial derivative of a sum with respect to one of the inputs of that sum, all of the other terms in the sum are constant and this one is linear and so all of the others go to zero and"
        },
        {
            "start_time": 420,
            "end_time": 450,
            "transcription": " this particular term goes to 1. So the derivative of this sum with respect to one of its inputs is 1. And so through this sum, we just get 1 times the derivative so far, which just carries back that same delta. For a multiplication operation, we know that if we have two values, a times w, then the derivative with respect to a"
        },
        {
            "start_time": 450,
            "end_time": 480,
            "transcription": " is w, and the derivative with respect to w is a. And so when we apply the chain rule, we will take the derivative so far and multiply it with the derivative of this node. So this is the partial derivative of the loss with respect to this operation star. So here we would get a partial derivative, which is this partial derivative times a2. So here we would get a2."
        },
        {
            "start_time": 480,
            "end_time": 510,
            "transcription": " times partial L over partial star and that gives us the partial derivative of L with respect to W2. And if we carry our derivatives back this way, we get W2 times partial L partial output of this node and that lets us carry the derivative back further through the network. When we get to a node that has multiple outputs,"
        },
        {
            "start_time": 510,
            "end_time": 540,
            "transcription": " the derivative of the eventual loss with respect to this node is a sum over all of the things that it feeds into. So here we get a sum over all of these derivatives that we're carrying back for each of the nodes that it feeds into. So here we get a sum over all of these multiplications in the next layer of"
        },
        {
            "start_time": 540,
            "end_time": 570,
            "transcription": " the derivative that we're carrying back here. Once we have that sum, that gives us the partial derivative of the loss with respect to this node's output A2. And so we can carry that back through this node by multiplying it with the derivative of the sigmoid function, which we know is activation times one minus activation. So that gives us A times one minus A times the sum of the incoming derivatives."
        },
        {
            "start_time": 570,
            "end_time": 600,
            "transcription": " And unsurprisingly, this is exactly the same partial derivative that we've derived before. It's just a different way of thinking about how do these derivatives propagate back through the network. So for any node to carry the derivative of the loss back through that node, we're applying the chain rule, which means we're multiplying the derivative of the loss with respect to that node's output times the derivative at that node. So we could use"
        },
        {
            "start_time": 600,
            "end_time": 630,
            "transcription": " this computational graph to compute all of the partial derivatives in the network by working backwards through our topological sort and propagating the partial derivative through each of the nodes by multiplying when we go through a node by the derivative of that node. And whenever we have a node with multiple outputs, we'll need to sum the derivatives coming from each of those outputs. Since this graph that I've drawn is just a small slice out of a much larger"
        },
        {
            "start_time": 630,
            "end_time": 660,
            "transcription": " represent all of the computations going on in an entire neural network. It may not be clear how a computational graph works in general. So let's take a look at a much smaller example so that we're clear on what it means to calculate the result of a function or the derivatives of a function using a computational graph. So here's a computational graph where we are computing some function"
        },
        {
            "start_time": 660,
            "end_time": 690,
            "transcription": " function f of x, y, and z. What is the function that we're computing here? Well, we're adding together an exponential and a sum. So we will have y plus z plus an exponential. This is y plus z plus e to the, well, this is the sum of y plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus z plus"
        },
        {
            "start_time": 690,
            "end_time": 720,
            "transcription": " plus z and x times y. So e to the x, y plus y plus z. So what we've done by drawing this computational graph is broken this function down into lots of intermediate values that help us in doing this computation. And so we could give names to all of those intermediate values. If we wanted to compute the output of this function"
        },
        {
            "start_time": 720,
            "end_time": 750,
            "transcription": " function for some particular combination of x, y, and z. So let's say we had minus one, one, two. Then for this point in x, y, z space, we can compute the value of f by propagating values forward through the network. So x times y gives us minus one. y plus z gives us three."
        },
        {
            "start_time": 750,
            "end_time": 780,
            "transcription": " The sum of A and B gives us two. The exponent here gives us E squared, and then this sum gives us E squared plus three. And so, the value of function F at minus one one two is E squared plus three."
        },
        {
            "start_time": 780,
            "end_time": 810,
            "transcription": " We can also use this computational graph to give us the partial derivative of f with respect to any of our inputs or with respect to any of the intermediate variables within the network. To pass derivatives back through the network, we need to know how to take the derivative of any nodes function with respect to each of its inputs. So here f is equal to b plus d."
        },
        {
            "start_time": 810,
            "end_time": 840,
            "transcription": " And so the partial derivative of f with respect to b is, while d goes to zero and b goes to one. So partial f over partial b equals one and the same for d. The more interesting cases are nodes like the exponential or the product. So the derivative of node d with respect to its input"
        },
        {
            "start_time": 840,
            "end_time": 870,
            "transcription": " So, this is e to the c for whatever value of c we had when we propagated the values forward through the network. For node A, the derivative of x times y with respect to x times y is equal to e to the c."
        },
        {
            "start_time": 870,
            "end_time": 900,
            "transcription": " x is y and the derivative with respect to y is x. And for each of these, some nodes will again get a partial derivative of 1 for each of the inputs. So, if we know how to compute the derivative of any of the nodes in our computational graph, then we can use those to run a backwards pass through the network and calculate"
        },
        {
            "start_time": 900,
            "end_time": 930,
            "transcription": " to calculate all of the partial derivatives. So the partial derivative of F with respect to D, we know to be one. So we'll get a one here, similarly here. Now as we work our way backwards, the partial derivative of F with respect to C is this partial derivative times the derivative of D with respect to C."
        },
        {
            "start_time": 930,
            "end_time": 960,
            "transcription": " d with respect to c is e to the c, and on our forward pass we computed that c was 2, so we get 1 times e to the 2, and so this partial derivative is e squared. That's the partial derivative of f with respect to c. Then we continue the partial derivative of a sum is 1, so we get 1 times e squared, so just e squared here, e squared here."
        },
        {
            "start_time": 960,
            "end_time": 990,
            "transcription": " to node B and we want to get the partial derivatives for each of these incoming edges, we need to first sum up all of B's eventual contributions. So we have a contribution of 1 here plus a contribution of E squared here and so we get 1 plus E squared. So we get a total of 1 plus E squared as this node's contribution to the derivative of F."
        },
        {
            "start_time": 990,
            "end_time": 1020,
            "transcription": " And so now, when we carry that back through the sum, well, the sum just gives us one as its derivative, so we get one times this on each of these edges. So we get one plus e squared, one plus e squared. For note A, we have e squared here, and we know that the partial derivative of A with respect to x is y and with respect to y is x. So we will, using this as a"
        },
        {
            "start_time": 1020,
            "end_time": 1050,
            "transcription": " chain rule multiply the downstream derivative by the derivative of this with respect to each of its inputs. So here we get y times e squared and here we get x times e squared. And so we know the values of x and y from our forward pass. So here y was 1 and so this becomes just e squared and x was minus 1 so"
        },
        {
            "start_time": 1050,
            "end_time": 1080,
            "transcription": " This becomes minus e squared. And so then finally for x and z, we now know the derivative of f with respect to each of them for y we need to sum over each of these edges and we get one plus e squared minus e squared. So this just gives us a derivative of one. Note that what we've done here is not to compute the full symbol"
        },
        {
            "start_time": 1080,
            "end_time": 1110,
            "transcription": " derivative of f with respect to each of its variables, we've instead evaluated the derivative at the specific point minus one one two. Feel free to check symbolically that the partial derivative of f with respect to x evaluated at the point minus one one two would give you e squared. But since our goal in back propagation is just to figure out which direction is the gradient pointing"
        },
        {
            "start_time": 1110,
            "end_time": 1140,
            "transcription": " for our current inputs, we don't actually need the full symbolic derivative, we just need the partial derivatives for the weights that we want to change evaluated at the current data points. But where computational graphs really become valuable is if we think not just in terms of scalar variables, but in terms of variables, nodes in the computational graph that correspond to vector"
        },
        {
            "start_time": 1140,
            "end_time": 1170,
            "transcription": " matrices or even tensors. Let's redraw the computational graph for a densely connected neural network, but instead of having a node for every variable, every activation, and every weight, let's instead have variables that correspond to vectors of activations or matrices of weights. To that end, let's name the vector of activation"
        },
        {
            "start_time": 1170,
            "end_time": 1200,
            "transcription": " at this entire layer of the network A1 with a vector hat, A2 with a vector hat, A3 with a vector hat for our three layers that we're illustrating here. And similarly, we could call the inputs to each of these layers X1, X2, and X3 with a vector hat, where these give the vector of all the"
        },
        {
            "start_time": 1200,
            "end_time": 1230,
            "transcription": " of the weighted inputs to this layer. And similarly, as we've done before, we could represent all of the weights between two layers with a weight matrix, which we'll call w1 or w2. So now if we draw our computational graph, we can represent the computation of a layer with just a couple of nodes."
        },
        {
            "start_time": 1230,
            "end_time": 1260,
            "transcription": " So now we have nodes in our graph for our vector of activations and our matrix of weights. And we can represent the weighted sum of inputs to this layer. So this is computing the vector x2 as the matrix vector product w1a1. And then this vector of inputs to the layer goes into our element y's"
        },
        {
            "start_time": 1260,
            "end_time": 1290,
            "transcription": " activation function, where we're applying the sigmoid element-wise to our input vector, and this gives us the vector of activations A2. Then similarly for the next layer we can get its vector of inputs by a matrix vector product. So this gives us our vector of inputs to the third layer, and this vector of input"
        },
        {
            "start_time": 1290,
            "end_time": 1320,
            "transcription": " inputs to the layer goes into a softmax activation function. And now, rather than having to think of a separate softmax activation function for each of the different outputs where all of those softmaxes have to take all of the x inputs, we now give this input vector to a single node that computes the vector of softmax activations."
        },
        {
            "start_time": 1320,
            "end_time": 1350,
            "transcription": " we can think of this softmax activation as being an input to the loss function where we are computing the categorical cross entropy and the other input to this categorical cross entropy function is our target vector of what we wanted the output of the network to be. And so now we can think of"
        },
        {
            "start_time": 1350,
            "end_time": 1380,
            "transcription": " of our vectorized neural network computations as a computational graph that operates on matrix and vector variables. And the advantage of this computational graph representation is that we've broken out the steps of the computation to show that for each layer, we are first computing the weight of some of inputs, and then we are applying the activation function. And we know how to take the derivative of the activation function"
        },
        {
            "start_time": 1380,
            "end_time": 1410,
            "transcription": " and we also know how to take the vector derivative of this matrix vector multiplication. If we want to use this computational graph to compute the gradient of the loss with respect to the network's weights, then to pass the derivatives backwards through the network, we need to apply the chain rule at every step, and we need to know how to take the derivative of each of these intermediate variables"
        },
        {
            "start_time": 1410,
            "end_time": 1440,
            "transcription": " with respect to their inputs. If we think about the same example from before where we have the deltas for the output layer nodes and we want to propagate those backwards to give us the deltas earlier in the network, the output layer deltas correspond to the partial derivative of the loss with respect to x3. So as we are propagating backwards through the network, we have"
        },
        {
            "start_time": 1440,
            "end_time": 1470,
            "transcription": " minus the deltas for this third layer. So the partial derivative of the loss with respect to the softmax inputs corresponds to this output layer delta. And so now if we want to propagate that backwards through the network, then we need to know the derivative of this matrix vector multiplication."
        },
        {
            "start_time": 1470,
            "end_time": 1500,
            "transcription": " But from multivariable calculus, we know that if we have a matrix vector multiplication, if we're multiplying matrix M times vector V, then the derivative of this with respect to M, so the d, d, M of this thing,"
        },
        {
            "start_time": 1500,
            "end_time": 1530,
            "transcription": " is the transpose and d dv of this thing is m. So if we apply the chain rule, then we will get the incoming derivative times the derivative of this node. And so for the weights, the derivative is the activation vector. And so we'll get"
        },
        {
            "start_time": 1530,
            "end_time": 1560,
            "transcription": " the delta's for the next layer times the transpose of our activations. And for this one, we will get a derivative, which is the weight matrix. So we'll get w2 times our next layer derivative, which is the delta 3 vector."
        },
        {
            "start_time": 1560,
            "end_time": 1590,
            "transcription": " and I'm probably off by a transpose on one or both of these, but if we get the shapes to line up, then this corresponds exactly to the one that we computed by hand when we worked an element at a time up here. So we could have done all of the derivations that we did previously where we thought about the partial derivative of one weight at a time"
        },
        {
            "start_time": 1590,
            "end_time": 1620,
            "transcription": " or one input at a time. If we had worked in terms of matrix and vector calculus, we could have just started from this computational graph representation and talked about passing the derivatives with respect to these matrices and vectors back through the computational graph. But this alternate derivation allows us to zoom out and think about the computation of an entire layer at a time. And this will make it much"
        },
        {
            "start_time": 1620,
            "end_time": 1629.100375,
            "transcription": " easier to derive the operations that we want for more complicated layer structures like convolutions."
        }
    ],
    "Vanishing (or Exploding) Gradients (DL 11)": [
        {
            "start_time": 0,
            "end_time": 30,
            "transcription": " Now that we've explored how neural networks operate and how to make them fast, it's time to start exploring how to scale them up to solve big problems, and that means adding more layers to our networks. But as soon as our neural nets start getting deeper, we can run into some problems during training caused by vanishing or exploding gradients. Let's start by thinking about a deep neural network with sigmoid activations for the hidden layers."
        },
        {
            "start_time": 30,
            "end_time": 60,
            "transcription": " ways we could visualize what such a network computes. In previous videos, I've drawn all of the nodes and all of the edges in the network, but when we start thinking about deep networks with potentially hundreds of nodes in a layer and dozens of layers, that's not at all practical. So here I've switched to a block diagram where each column represents a layer of the network, and we've indicated the activation functions within each block."
        },
        {
            "start_time": 60,
            "end_time": 90,
            "transcription": " And for now we'll still assume dense connections, meaning that every node in one layer is connected to every node in the next. Another way we could visualize this neural network is in the computational graph shown at the top. This graph shows the sequence of operations that gets applied to each batch of data. We start with a matrix of inputs, and we multiply the weight matrix"
        },
        {
            "start_time": 90,
            "end_time": 120,
            "transcription": " from layer 1 to layer 2 by those inputs, which I'll indicate by this asterisk operation. And then we add the vector of biases to every column, which I'll indicate by this plus. And that produces the inputs x2 that go into the activation function at layer 2. Then we apply the layer 2 activation function, which produces our matrix of"
        },
        {
            "start_time": 120,
            "end_time": 150,
            "transcription": " activations at layer two. From layer two to three, we have the same sequence of operations performing a matrix multiplication, then adding the vector of biases and applying the activation function. And this proceeds through each of the hidden layers in the network. When we get to the output layer, the activation function changes to a softmax. And then we can think of the loss as being computed from the activations of the output layer,"
        },
        {
            "start_time": 150,
            "end_time": 180,
            "transcription": " and the targets for this batch. This computational graph makes much more explicit the sequence of operations that we're performing, and it can also make it easier to think about what derivatives we need to take on the backward pass. And so we'll see these computation graphs a lot throughout the rest of the course. But yet another way to think about our computations during training would be to write out mathematical expressions for the loss and for the delta"
        },
        {
            "start_time": 180,
            "end_time": 210,
            "transcription": " that we will use to perform our gradient descent updates. The loss results from this entire chain of computations through the graph, and so our functions start from our first matrix multiplication with the input batch. To get our inputs for the first hidden layer, we multiply the weight matrix by the input batch matrix and add the bias."
        },
        {
            "start_time": 210,
            "end_time": 240,
            "transcription": " That weighted sum then becomes the input to the first hidden layers sigmoid activation function. Those activations then get multiplied by the weights from layer 2 to 3, and we add the layer 3 bias. And this is now an expression for computing the inputs to layer 3, to which we again apply a sigmoid activation function."
        },
        {
            "start_time": 240,
            "end_time": 270,
            "transcription": " And this proceeds through the hidden layers. Each time, the output of one hidden layer gets multiplied by the weight matrix and we add the bias and that becomes the input to the next. Once we have an expression for the last hidden layers activations, we still multiply by weights and add the bias, but now we apply the"
        },
        {
            "start_time": 270,
            "end_time": 300,
            "transcription": " the output layer's softmax activation. Then the output activations along with the targets become the inputs to the categorical cross entropy calculation that gives us the loss. Writing out this gigantic expression for the loss makes clear that when we're trying to compute derivatives, we're going to have to apply the chain rule a whole bunch of times. And to really drive the point home,"
        },
        {
            "start_time": 300,
            "end_time": 330,
            "transcription": " Let's write out an expression for the deltas on the first hidden layer. Our delta calculations start from the output layer, and since we're using softmax with categorical cross entropy, our output layer deltas are just the difference between activations and targets. To propagate deltas backwards, at each hidden layer, we need to do a matrix multiplication with the transpose of the weight matrix, and then an element"
        },
        {
            "start_time": 330,
            "end_time": 360,
            "transcription": " So to get the deltas at layer 5, we start by multiplying with the transpose of the weights from 5 to 6. And then we element-wise multiply with the derivative of the activations for layer 5. Then to go from layer 5 to layer 4, we have the same sequence of the variables."
        },
        {
            "start_time": 360,
            "end_time": 390,
            "transcription": " operations, and this proceeds backwards through all of the hidden layers. And so for every layer we propagate derivatives through, we multiply by both the weights and the derivative of the activations. So the question is,"
        },
        {
            "start_time": 390,
            "end_time": 420,
            "transcription": " Why does this cause a problem when training deep neural networks? Well, because we're using sigmoid activation functions for our hidden layers in this network, each time we propagate backwards, we're multiplying by the derivative of a sigmoid. And unfortunately, the derivative of a sigmoid function tends to have very small values. Its maximum output is 0.25, and for much of its range,"
        },
        {
            "start_time": 420,
            "end_time": 450,
            "transcription": " the output is much smaller still. And this means that if we multiply by sigmoid activations repeatedly every time we go backwards through the network, our deltas will tend to get smaller and smaller at each successive layer. And this is known as the vanishing gradient problem. The reason vanishing gradients are a problem is that when we start training a neural network, it's computing a completely random"
        },
        {
            "start_time": 450,
            "end_time": 480,
            "transcription": " because we have chosen random initial values for the weight matrices. And because a deep neural network applies many random weight matrices to the inputs, by the time data gets to the end of the network, it has been pretty thoroughly scrambled. And so the information that we get from the loss function computed on the targets has to"
        },
        {
            "start_time": 480,
            "end_time": 510,
            "transcription": " propagate back through the network and it's particularly important to update the weights for these early layers that are being applied most directly to the input. But if the gradients have vanished by the time we get to the early layers, then when we take gradient descent steps, we will be only making extremely tiny changes to these weights. And so it will be very hard to move these first few weight matrices"
        },
        {
            "start_time": 510,
            "end_time": 540,
            "transcription": " away from just being random and scrambling the data. Prior to the 2010s, sigmoids were the most popular activation function for neural networks. And so the vanishing gradient problem was a big obstacle to training deep neural networks. And a big part of the reason that deep learning took off around a decade ago was that a few different approaches were devised to overcome the vanishing gradient"
        },
        {
            "start_time": 540,
            "end_time": 570,
            "transcription": " problem. One of the ways we can counteract vanishing gradients is by changing our initialization of the weight matrices. Since every layer that we back propagate through multiplies by a sigmoid derivative and also the transpose of the weight matrix, if we generate larger initial random weights that will tend to increase the magnitude of the"
        },
        {
            "start_time": 570,
            "end_time": 600,
            "transcription": " deltas, and if we've chosen those weights properly, that could counteract the decreasing of the deltas from the derivative. But the solution that really made deep neural networks take off is switching away from sigmoid activations towards rectifier linear units. Because the ReLU activation function has a derivative of 1 whenever it has non-zero output, multiplying by"
        },
        {
            "start_time": 600,
            "end_time": 630,
            "transcription": " derivatives instead of sigmoid derivatives tends to squish the gradients much less. And this is the number one reason why ReLU activations became more popular than sigmoids. But when we replace our sigmoid activations with rectifier linear units, we now have a potential problem in the other direction, specifically because the ReLU derivative tends"
        },
        {
            "start_time": 630,
            "end_time": 660,
            "transcription": " not to reduce the deltas as we go backwards, it's possible that if the weight matrices have particularly large values, then as we go backwards, the deltas could get larger and larger. And this is known as exploding gradients. Much like vanishing gradients resulted in taking gradient descent steps that were far too small, exploding gradients mean that we will take gradient descent steps that are way too big."
        },
        {
            "start_time": 660,
            "end_time": 690,
            "transcription": " If gradient descent takes gigantic steps, that may not result in moving down the loss landscape at all, and so there may be no way to effectively train the network. So if we're using ReLU neurons, we generally want to choose smaller initial weights than we would with sigmoids. The takeaway messages here are that ReLU neurons work great for hidden layers because they make the network easier to train, not to mention because"
        },
        {
            "start_time": 690,
            "end_time": 720,
            "transcription": " computing both the activation and the derivative is extremely efficient, and that the best way to generate random initial weights can be different for different types of activation function. But lucky for us, the deep learning community has put a lot of work into determining the best way to initialize weights for different types of layers. And so if we're using a modern deep learning library we can take for granted that the weight initial"
        },
        {
            "start_time": 720,
            "end_time": 726.0299375,
            "transcription": " will be done appropriately for the type of activations that we specify."
        }
    ],
    "How to train your neuron (DL 04)": [
        {
            "start_time": 0,
            "end_time": 30,
            "transcription": " In our last video, we saw how a single neuron computes by taking a weighted sum of inputs, adding a bias, and applying an activation function. And that by using this step function for our activation, we can get a binary classifier, or by using this linear function for our activation, we can get a regressor."
        },
        {
            "start_time": 30,
            "end_time": 60,
            "transcription": " with the idea of measuring a model's loss on its data set using the sum of squared errors and training the model using the gradient of that loss function. I've written out a loss function here with notation that emphasizes that the loss depends on the parameters of the model, namely the weights and the bias. This loss function sums up over all capital N data points, the squared difference"
        },
        {
            "start_time": 60,
            "end_time": 90,
            "transcription": " between the data output and the model's output, and then divides by the number of data points. So this is a mean squared error loss function, which we will more frequently use in computations. But for doing the mathematical derivation, it's nicer to work with the sum of squared errors, so we don't have to carry this one over n around the whole time. To see how the loss function depends on the parameters,"
        },
        {
            "start_time": 90,
            "end_time": 120,
            "transcription": " and to set up how we can modify the parameters to decrease the loss, we can start by calculating the loss on a small regression data set. This data set consists of four points and each of the inputs are one-dimensional and our current one neuron model has a weight of two and a bias of negative two."
        },
        {
            "start_time": 120,
            "end_time": 150,
            "transcription": " So the predictions this neuron would make for any given input x are represented by this line. Since this neuron is taking the input x and calculating 2x minus 2, we can fill in the values for what it will predict at each of these inputs appearing in the data set. Then to calculate the loss, we need to sum up"
        },
        {
            "start_time": 150,
            "end_time": 180,
            "transcription": " the squared differences between the correct and predicted outputs. So our loss sums up for each data point, the difference between the target and what our model gave us. Our errors are 1, 1, negative 2, and negative 2. So when we square and sum them, we get a loss of 10. Now our goal is to minimize the loss"
        },
        {
            "start_time": 180,
            "end_time": 210,
            "transcription": " And we can do that by updating the parameters in a direction that will decrease the loss. And we know from calculus that the direction that decreases the loss is minus the gradient. So now we need to find the gradient of the loss function, which consists of two partial derivatives, the first with respect to weight one and the second with respect to the bias. But let's step back from the specific case of"
        },
        {
            "start_time": 210,
            "end_time": 240,
            "transcription": " this neuron on this data set and think first about the general case of our loss function on any neuron and any data set. And we'll see how much we can derive in the general case so that it can be applied for both regression and classification and on different data sets with different dimensions. In the general case, the gradient of the loss will always be a vector of partial derivatives"
        },
        {
            "start_time": 240,
            "end_time": 270,
            "transcription": " and we'll have one partial derivative for each parameter of the model. So let's think about the partial derivative of the loss with respect to an arbitrary weight, wi. When we take the derivative of the sum, that's the sum of the derivatives. To get the derivative of the term inside the sum, we apply the chain rule, which says the derivative of the outside evaluated at the inside times the derivative of the inside."
        },
        {
            "start_time": 270,
            "end_time": 300,
            "transcription": " The outside here is squaring, so we get 2 times the inside times the derivative of the inside, and with respect to the variable whose derivative we are taking, w i, y j is a constant, and so we get minus the derivative of the activation function."
        },
        {
            "start_time": 300,
            "end_time": 330,
            "transcription": " And we'll bring the minus sign out front, and we should also factor out the two. Now we're taking the derivative of the activation function, which I can't give you as a general case, since we have multiple possible activation functions. But when we apply the chain rule here, we will eventually need to take a derivative of the weighted sum of inputs. And since that calculation happens for every neuron, we'll always need that partial derivative."
        },
        {
            "start_time": 330,
            "end_time": 360,
            "transcription": " For data point j, I'm using xj without the vector hat to mean this weighted sum of inputs for that data point. And the derivative of that weighted sum of inputs with respect to the i'th weight ends up with only one term because there is only one term in this sum that contains wi. The derivative of wi xi is just xi."
        },
        {
            "start_time": 360,
            "end_time": 390,
            "transcription": " So we get for this partial derivative the ith component of this jth data point. Similarly, if we were taking the partial derivative with respect to the bias, the whole summation would go away and we'd end up with a derivative of 1. Next we'd like to apply this to our regression example, but first it's worth noting that the main term in this sum is something we've already seen."
        },
        {
            "start_time": 390,
            "end_time": 420,
            "transcription": " difference between a data points correct output and the models output on that data point is exactly what we were computing here before squaring it in the loss calculation. So let's name that the error for the jth data point, which means that calculating our partial derivative just means summing up the errors times the derivative of the activation function."
        },
        {
            "start_time": 420,
            "end_time": 450,
            "transcription": " over all of the data points and multiplying by negative 2. When we bring that back to our example, we are trying to calculate the gradient of this loss function, which is a vector that has two components. The first component is the partial derivative with respect to w1, and the second component is the partial derivative with respect to b."
        },
        {
            "start_time": 450,
            "end_time": 480,
            "transcription": " So our derivative with respect to weight 1 sums up the errors times the derivative of the activation. But the derivative of the activation function at data point j means passing the chain rule through our activation function. And since that activation function has a derivative of 1, we end up with a derivative that's just the x component of the data point that was multiplied"
        },
        {
            "start_time": 480,
            "end_time": 510,
            "transcription": " multiplied by weight 1. Which means our sum looks an awful lot like this one, except instead of squaring each difference, we're multiplying that difference by the corresponding x value. And the partial derivative with respect to the bias is similar, only instead of getting actual inputs,"
        },
        {
            "start_time": 510,
            "end_time": 540,
            "transcription": " here, the bias was multiplied by 1, and so we just add up the errors. Now we have both partial derivatives, so putting them together into a vector gives us the gradient. At this point, it's worth taking a moment to think about the geometry of this loss function and its gradient. Remember the loss is a function"
        },
        {
            "start_time": 540,
            "end_time": 570,
            "transcription": " of the model parameters, that is the weight and the bias. And so if we were going to draw the loss function, it would have two inputs, W1 and B. And any point on this diagram represents some possible function the neuron could compute. If I picked a point over here, that would specify a weight of negative three and a bias of two. And so that would be"
        },
        {
            "start_time": 570,
            "end_time": 600,
            "transcription": " would specify a different parameterization of our linear neuron. Our particular example corresponds to W1 equals 2 and B equals minus 2. And for these particular parameters, we determined that the loss was 10. But for other parameters, we would get different values of the loss. And so we can think about a plot where the loss value is coming out from the whiteboard."
        },
        {
            "start_time": 600,
            "end_time": 630,
            "transcription": " And we are trying to find the point within this space that minimizes that loss function. And when we calculate the gradient at a particular point in the space, that tells us from this point in which direction is the loss most steeply increasing. And so if we take a step in the direction opposite the gradient, then we should be able to decrease the loss and have a model that"
        },
        {
            "start_time": 630,
            "end_time": 660,
            "transcription": " that is better at representing the data set. We take that step by subtracting a small multiple of the gradient from the current values of our parameters. That multiple is known as the step size, and I'll represent it for now with the letter eta. And if we chose eta to be 0.01, then we would subtract 0.22 from the weight and subtract 0.22"
        },
        {
            "start_time": 660,
            "end_time": 690,
            "transcription": " 0.04 from the bias, meaning that we are moving in this direction to try and decrease the loss of our model. And now, with new values for both parameters, we can evaluate the loss the model would have on the data set with the new parameters, and then take another gradient step to improve it further."
        },
        {
            "start_time": 690,
            "end_time": 720,
            "transcription": " In this particular case, the loss surface is shaped somewhat like this. And if I were to draw contour lines on the graph, it would look something like... My drawing is far from perfect, but we know from the gradient that the landscape is very steep in this direction and much less steep in this direction, which makes sense because adjusting the weight"
        },
        {
            "start_time": 720,
            "end_time": 750,
            "transcription": " will affect the error much more quickly than nudging the function up and down by changing the bias. And we can also think about what would the function we get here look like? Well, we've slightly decreased the bias, so we've shifted the intercept down, but we've much more significantly decreased the weight, which is going to reduce the slope. And so we're gonna get something that is slightly shifted down and tilted"
        },
        {
            "start_time": 750,
            "end_time": 780,
            "transcription": " to the right, which is in fact a better representation of our data set. Now we'd like to apply the same approach to classification, but we immediately run into a problem, which is that when we take the derivative of the activation function, that's almost always going to be zero, and so we're gonna have no information at all in the gradient of the loss function."
        },
        {
            "start_time": 780,
            "end_time": 810,
            "transcription": " this by replacing the step function activation with a smooth approximation known as a sigmoid. And the formula for the sigmoid activation function is 1 over 1 plus e to the minus x. When we think about how this function behaves, when x is large, this exponent is tiny, so this term goes to 0, and the function approaches 1."
        },
        {
            "start_time": 810,
            "end_time": 840,
            "transcription": " other hand, when x is very negative, this becomes a very large exponent, meaning that the denominator is going off to infinity and the function is approaching zero. So we get roughly zero over here and roughly one over here, but a nicely smooth transition between them. If we plot a classification boundary with this sigmoid function, we can still draw a dividing line"
        },
        {
            "start_time": 840,
            "end_time": 870,
            "transcription": " where the weighted sum of inputs goes from being negative to being positive. But now, when the input is close to that dividing line, the function will be outputting approximately a half, and it will only be outputting something close to one or close to zero when we are far from that dividing line. But that may very well be okay, because we probably shouldn't be quite as confident"
        },
        {
            "start_time": 870,
            "end_time": 900,
            "transcription": " our predictions for points that lie right on the boundary. If we used this neuron where we have w1 and w2 of 1 and a bias of negative 4, then we would get predictions of 0, 0, 1, 1, 1, 1, meaning our predictions are correct for every point but this one. On the other hand, if we use the sigmoid activation function, when we"
        },
        {
            "start_time": 900,
            "end_time": 930,
            "transcription": " We calculate our weighted sum of inputs. We'll get minus 1 for the first point. And when we pass that through the sigmoid, we'll get 1 over 1 plus e, which comes out to 0.27. The second point has the exact same weighted sum of inputs. So here, our weighted sum of inputs is 1, so we get 1 plus e to the minus 1 in the denominator, which means that the activation"
        },
        {
            "start_time": 930,
            "end_time": 960,
            "transcription": " function comes out to 0.73. And our weighted sum of inputs is the same for the following two points. For the last point, our weighted sum of inputs is 2, and so we're further over and should get a larger activation. And when we calculate it, we do. It comes out to 0.88. So, just like we did for linear regression, we can calculate the loss of our current model by taking"
        },
        {
            "start_time": 960,
            "end_time": 990,
            "transcription": " the sum of squared differences between the correct answer and our prediction, but really our primary goal is to find the gradient of that loss, and that requires us to differentiate the sigmoid function. So let's take a moment to do that. Taking the derivative of this function with respect to x requires us to first invoke the quotient rule, so we'll get minus the derivative of the bottom over the bottom squared."
        },
        {
            "start_time": 990,
            "end_time": 1020,
            "transcription": " and the derivative of the bottom, well the one goes away and e to the minus x has a derivative of minus e to the minus x and our denominator is one plus e to the minus x quantity squared. I claim that this value is in fact exactly what we would get if we took sigma evaluated at x and multiplied it by"
        },
        {
            "start_time": 1020,
            "end_time": 1050,
            "transcription": " 1 minus sigma evaluated at x. I will leave this for you to verify yourself, but the algebra isn't that hard. And now that we know the derivative of the activation function, we can use that to find partial derivatives and therefore the gradient of the loss. Like we did for the regressor, I want to think about the partial derivative for one of the weights, and then the calculation"
        },
        {
            "start_time": 1050,
            "end_time": 1080,
            "transcription": " for the other weight and the bias will be similar. Again, we get minus two times a sum over all of the data points. And inside the sum, we have the error for the jth data point. But now that's multiplied by the derivative of the activation function. Here, the activation function is being applied to the weighted sum of increase"
        },
        {
            "start_time": 1080,
            "end_time": 1110,
            "transcription": " So we are using the chain rule. The chain rule gives us the derivative of the activation function evaluated at the weighted sum of inputs times the derivative of the weighted sum of inputs. And then, just like before, the derivative of the weighted sum with respect to our particular weight is just the component of the current data point"
        },
        {
            "start_time": 1110,
            "end_time": 1140,
            "transcription": " corresponding to the weight that we are currently taking a derivative with. The partial derivative for weight 1 would look almost identical except that we'd have the first dimension of each data point. And the partial derivative for the bias would end up with a 1 in place of the xj components. And so we'd just have"
        },
        {
            "start_time": 1140,
            "end_time": 1170,
            "transcription": " this portion. When we put those three partial derivatives together, we end up with a gradient vector. And again, that gradient vector will point in the direction that increases the loss. And so we can adjust the parameters of the model by taking a small step in the minus gradient direction. And then with our newly updated weights, we can calculate the gradient again, take another step, and continue to improve our model."
        },
        {
            "start_time": 1170,
            "end_time": 1200,
            "transcription": " It's important to remember that everything we've done here generalizes to higher dimensions. If our input had more coordinates, we would simply have more weights, but the formula for calculating the partial derivatives would be the same. We would just have to apply it more times to get the full gradient vector, and likewise for classification. And overall, what we've just derived is a version of"
        },
        {
            "start_time": 1200,
            "end_time": 1209.074625,
            "transcription": " of linear regression and a version of logistic regression using nothing but a single neuron and gradient descent."
        }
    ],
    "Other Metrics and the ROC Curve (DL 20)": [
        {
            "start_time": 0,
            "end_time": 30,
            "transcription": " This is a short lecture on some simple alternatives to categorical accuracy for measuring success in binary classification. In a binary classification task, if we are solving it with a neural network, we will typically have two output nodes. And so all of our target vectors will either be one zero or zero one. When our network produces a two vector of outputs and we decode that to a category label, there are four possible outcomes."
        },
        {
            "start_time": 30,
            "end_time": 60,
            "transcription": " We could have a target of 1 0, meaning that the data point belongs to the first category, and if our decoded output agrees, we call that a true positive. Whereas, if the label should have been the first category, and we labeled it the second category, then that's a false negative, because we thought that it was the negative category, but we were wrong, and it should have been the positive category."
        },
        {
            "start_time": 60,
            "end_time": 90,
            "transcription": " of 0,1 we can again get it either right, which is a true negative or wrong, which would be a false positive. I won't use the terms type 1 and type 2 error, but you may have heard them before and they correspond to false positives and false negatives. Statistics has a lot of labels for the different possible rates that could occur between false negatives, false positives, true negatives, and true positives."
        },
        {
            "start_time": 90,
            "end_time": 120,
            "transcription": " In this figure from Wikipedia, the rectangle represents all possible inputs and the points represent the inputs that are actually in our data set. The vertical line represents a correct decision boundary separating the positive examples on the left from the negative examples on the right. And the ellipse represents a decision boundary of a model where inside the ellipse are the things that the model labels as positive and outside the ellipse"
        },
        {
            "start_time": 120,
            "end_time": 150,
            "transcription": " are the things that the model labels as negative. And so we could measure, for example, the fraction of data points that are model labeled as positive that should have been labeled positive under the true labeling. This would give us the precision of our model. We could also measure the fraction of things that should have been labeled as the first category that we actually labeled as the first category. So this would be the"
        },
        {
            "start_time": 150,
            "end_time": 180,
            "transcription": " number of things in the left half of the ellipse out of the number of data points with the positive label. And this is known as the sensitivity or recall of the model. We could also measure the fraction of things that should have been labeled as the second or negative category that the model actually labels as the second or negative category. That's the"
        },
        {
            "start_time": 180,
            "end_time": 210,
            "transcription": " true negatives divided by all of the things that should have been labeled negative, and this is known as the specificity. Comparing these to the accuracy metric that we've used so far, accuracy is the fraction of things that we gave the correct label to. So it is the things that our model gave the correct positive label, plus the things that our model gave the correct negative label out of the total number."
        },
        {
            "start_time": 210,
            "end_time": 240,
            "transcription": " of data points. So why are we concerning ourselves with metrics other than accuracy? Well, there are a few cases where accuracy isn't really what we care about. One case is when false positives and false negatives have very different importance for the model. This might be the case in medical diagnosis. And accuracy is also heavily affected by the fraction of positive and negative labels in the data"
        },
        {
            "start_time": 240,
            "end_time": 270,
            "transcription": " If we have a data set where 95% of our data are positive labels, then we could just guess that everything belongs to the first category and get 95% accuracy. And so some of these other metrics or ways of waiting between them can help us focus on how well our model does, specifically on the positive or the negative categories. It's common for machine learning models to exhibit a trade-off between"
        },
        {
            "start_time": 270,
            "end_time": 300,
            "transcription": " precision and recall. If we imagine that we are choosing among these different ellipses where the size of the ellipse is a parameter of our model that we are allowed to play with, we could improve our precision by shrinking the ellipse where we predict label 1. With a small enough ellipse we can get only positive examples in our positive label predictions, but that sacrifices our ability to capture most of the positive examples in the data set."
        },
        {
            "start_time": 300,
            "end_time": 330,
            "transcription": " On the other hand, we could get great recall by increasing the size where with a big enough ellipse we could capture all of the data points that should have the positive label, but that would be at the cost of labeling lots of things that should have been negative also as positive. There's also often a trade-off between sensitivity and specificity of the model. Sensitivity is the same thing as recall."
        },
        {
            "start_time": 330,
            "end_time": 360,
            "transcription": " And so in this example, we could increase the size of the ellipse to increase the sensitivity. And while specificity is different from precision, it turns out that in this case, we can also improve our specificity by shrinking the size of our ellipse. Ensuring that out of the things that should be labeled negative, we do in fact label them all as negative by making the region where we label positive sufficiently small. These ellipses may seem like a contrived example,"
        },
        {
            "start_time": 360,
            "end_time": 390,
            "transcription": " but these trade-offs between precision and recall, or between sensitivity and specificity, apply to a wide range of machine learning models. And so it's common to think about for a particular model, how does that model trade off between sensitivity and specificity? A common way to visualize this is with a receiver operating characteristic curve. The ROC curve plots false positive rate against true"
        },
        {
            "start_time": 390,
            "end_time": 420,
            "transcription": " positive rate. So if our model were to predict that everything belonged in the negative category, then it would have a 0% false positive rate, but also a 0% true positive rate. On the other hand, if our model predicted that everything were in the positive class, then it would capture 100% of the positive examples, but also give false positives on 100% of the negative examples. So these"
        },
        {
            "start_time": 420,
            "end_time": 450,
            "transcription": " points at either end of the 45 degree line are extremes of the possibilities for how we could make predictions. And so if we were to guess independently at random between labeling things positive and labeling things negative, then we would move along this 45 degree line where the probability with which we said positive would determine where along this line we end up. On the other hand, a perfect classifier"
        },
        {
            "start_time": 450,
            "end_time": 480,
            "transcription": " would get true positives 100% of the time and false positives 0% of the time and so would end in this corner. So that means that we prefer machine learning models that are closer to this top left corner of the ROC plot. But if we think about training the same model with different hyper parameters, we might find ourselves at different points in this trade-off between false positive and true positive rate."
        },
        {
            "start_time": 480,
            "end_time": 510,
            "transcription": " And so if we were to connect up the points that we can get with a particular type of model, then we get an ROC curve for that class of model. And so for different classes of models, we will have different curves, and in general, ones that come closer to the perfect classifier curve are better. But even within a given class of models, we may, depending on the relative importance of"
        },
        {
            "start_time": 510,
            "end_time": 540,
            "transcription": " false positives and false negatives for the problem that we are solving, we might choose different points along that model class's ROC curve as preferable for the problem that we want to apply the model to. So in working on your project this week, you should think about the trade-offs among your different models between false positives and false negatives rather than just measuring the overall accuracy."
        },
        {
            "start_time": 540,
            "end_time": 541.942125,
            "transcription": " of the model."
        }
    ],
    "The Data Analysis Pipeline (DL 05)": [
        {
            "start_time": 0,
            "end_time": 30,
            "transcription": " In a deep learning class where we'll be studying neural networks in great depth, it would be easy to forget that a neural network or any machine learning model is always part of a larger system. Before data can arrive at a neural network, it has to be collected and processed into a form that a neural network can understand. And the outputs that a neural network produces will often require post-processing or other analysis."
        },
        {
            "start_time": 30,
            "end_time": 60,
            "transcription": " And therefore, it will be extremely helpful throughout the semester to keep in mind the metaphor of the data analysis pipeline. This analogy reminds us that whenever we're doing machine learning, our goal is to turn observations of the world into predictions about the world, and that the neural network is just one step in that process. As we move from observations to predictions, the pipeline reminds us that we need to consider"
        },
        {
            "start_time": 60,
            "end_time": 90,
            "transcription": " what stages our data goes through, and how each stage feeds into the next. For different problems we'll be solving, we will need different stages in the pipeline, and sometimes when we're working with standardized or simulated data sets, we'll be able to skip certain stages entirely. But if we ever want to apply deep learning to real-world problems, we'll need to keep the realities of practical data analysis in mind. To make all of this more concrete,"
        },
        {
            "start_time": 90,
            "end_time": 120,
            "transcription": " Let's talk in more detail about some of the things that appear in common data analysis pipelines. Because doing machine learning requires having data to train a model on, the first stage in any data analysis pipeline is generally collecting the data. There will certainly be cases where we'll work with pre-existing data sets, but if you want to go out and solve some new problem with deep learning, you'll need to figure out what data"
        },
        {
            "start_time": 120,
            "end_time": 150,
            "transcription": " can you use to train your deep learning model? And when you go and collect that data, you'll need to make sure that you have a reasonable amount. Mainly, this means making sure that you have enough data. And that's because the recent successes of deep learning have largely been based on the ability to train on enormous data sets. But on the other hand, when we don't have enormous amounts of computing power available, there is definitely such a thing as"
        },
        {
            "start_time": 150,
            "end_time": 180,
            "transcription": " much data, and especially in cases where we are doing exploration and trying to figure out what's even possible, it can be very helpful to work with a limited amount of data. But often the biggest challenge in data collection is making sure that the data set we're going to train on is representative of the problem we actually want to solve. This includes some obvious things, like making sure all of the classes we want"
        },
        {
            "start_time": 180,
            "end_time": 210,
            "transcription": " our classifier to recognize are well represented among the data. But this can also include subtler issues like making sure important outliers are not missed by the model. For example, a network doing computer vision for a self-driving car had better be able to recognize an Amish buggy and know that it's not moving as fast as a car. But it would be very easy if we're not being careful to construct a data set that doesn't contain that type of example."
        },
        {
            "start_time": 210,
            "end_time": 240,
            "transcription": " Even more problematic is identifying systematic biases in a dataset. This can range from having too many pictures that were taken on sunny days, so your image classifier has trouble when it's cloudy, to predictions about outcomes in health or education that end up ascribing to an individual factors that are actually about broader social structures."
        },
        {
            "start_time": 240,
            "end_time": 270,
            "transcription": " that a dataset can have some underlying bias, so we very much need to keep potential biases in mind when we are collecting datasets. But it's also worth noting that identifying and correcting bias is a hard problem that is the subject of lots of ongoing deep learning research. Once we have collected some data, there will often be a fair amount of cleaning up of the data that will be required before we can try to do any sort of"
        },
        {
            "start_time": 270,
            "end_time": 300,
            "transcription": " sort of machine learning or other processing on the data set. For example, there will often be dimensions of the data that are present for some data points and absent for others. If say our data set is based on health records, we might have blood pressure readings for most of the examples, but not all of them. And we'll therefore need to make a decision of whether that feature is included in the data set."
        },
        {
            "start_time": 300,
            "end_time": 330,
            "transcription": " And if so, how do we represent the cases where the data is missing? It may also be the case that some of the potential dimensions of our data are totally irrelevant to the problem that we're trying to solve. And if we include something like a patient ID number in our model, that's only going to create extra randomness and distract from the model's ability to learn the real underlying correlations."
        },
        {
            "start_time": 330,
            "end_time": 360,
            "transcription": " In some cases, we may have a fundamental mismatch of the dimension of different examples in our dataset. Think of an image processing task where we might want to include in our dataset images of different resolution or aspect ratio. So we'll need to think about how can we take data that has different dimensions for different examples and use them as part of our training set for a single model."
        },
        {
            "start_time": 360,
            "end_time": 390,
            "transcription": " If we are collecting data ourselves, how do we get appropriate labels for the data so that we can perform supervised learning? Suppose we were trying to transcribe speech in American Sign Language. If all we have is video of the sign language, then even if we speak sign language ourselves, it might be an awful lot of work to come up with the appropriate label for all of the data in all of the videos."
        },
        {
            "start_time": 390,
            "end_time": 420,
            "transcription": " we want to train on. Or suppose that our videos were of a sign language interpreter, we might have a transcript of the speech that they were interpreting, but there might not be a one-to-one correspondence between what was said in sign language and what was said out loud in English. So we need to not only make sure that we have labels for our data, but that the labels really do indicate the things that we want our model to learn about the data."
        },
        {
            "start_time": 420,
            "end_time": 450,
            "transcription": " Once we have collected and cleaned a data set to the point where we think it adequately represents the problem we're trying to solve, we then need to turn the data into a format that will be as effective as possible for training our neural network or other machine learning model. In particular, this means encoding our data numerically. A neural network expects a vector or matrix of numerical data as a"
        },
        {
            "start_time": 450,
            "end_time": 480,
            "transcription": " input. And we'll see throughout the semester that the problem of devising an appropriate numerical encoding for the data can vary wildly in difficulty from one problem to another. It turns out that processing image data is relatively easy because the underlying pixel-based representation that computers already use is great for machine learning. But when we want to process text data, the ASCII format"
        },
        {
            "start_time": 480,
            "end_time": 510,
            "transcription": " that computers use to store text files is extremely unhelpful for a neural network. And so we'll want to think about alternate ways that the same data set can be represented. And the simplest idea I can give of transforming the representation of a data set comes from the linear models we've already seen, where to do good classification, we need data that can be"
        },
        {
            "start_time": 510,
            "end_time": 540,
            "transcription": " we cleanly separated by a linear decision boundary. And if we have here data in terms of dimensions, x1 and x2, there's no easy way to draw a line that separates most of the blue points from most of the red ones. But if instead of representing our data in Cartesian coordinates, we transformed it into polar coordinates, where each data point is represented"
        },
        {
            "start_time": 540,
            "end_time": 570,
            "transcription": " by a radius from the origin and an angle. Now, there is a very clear linear decision boundary that easily separates our two classes. But it's also quite possible that for a different data set, neither of these representations would let us easily draw a linear decision boundary. But maybe if we combined both of these representations, and even though our data started off as two-dimensional,"
        },
        {
            "start_time": 570,
            "end_time": 600,
            "transcription": " we could turn each of those two-dimensional data points into a four-dimensional data point that included both the x1 and x2 coordinates as well as the r and theta coordinates, then in that higher dimensional representation, we could come up with a simple model to separate our classes. And this idea of changing the representation or even the dimension of the data can take on"
        },
        {
            "start_time": 600,
            "end_time": 630,
            "transcription": " even more importance as the type of problem we're representing gets more complex. Another issue we'll encounter with numerical representations is that neural networks are very good at outputting values in the range zero to one. And because of the ways we tend to initialize weights and other parameters, it can also be helpful for the inputs to be in that range. And so if we have a data set that involves pixel"
        },
        {
            "start_time": 630,
            "end_time": 660,
            "transcription": " values from 0 to 255, or numbers of people or amounts of money with very large numbers. It can be helpful to normalize the data where we squash the range of data values down so that the inputs to the neural network are much closer together. And then as a post-processing step, we have to denormalize and undo that transformation. There are many other possible types of pre-processing"
        },
        {
            "start_time": 660,
            "end_time": 690,
            "transcription": " But the last and most important that I want to talk about is splitting our data into a training set and a test set. The key idea here is that to make sure our model makes good predictions, we need to test it on data that it hasn't seen before. So we need to hold back some portion of our data set that the model doesn't get to train on, and then we can use that data when we are in a test set."
        },
        {
            "start_time": 690,
            "end_time": 720,
            "transcription": " evaluating our model later. And this idea of splitting up the data set and the ways that we can use the test set to tune our model will be the core subject of our next video. But after data has gone through the neural network, there may also be post-processing steps that we want to apply. First, we will generally need to take the output of the neural network and turn that back into the type of prediction we're actually interested in."
        },
        {
            "start_time": 720,
            "end_time": 750,
            "transcription": " Since neural networks are good at outputting vectors of zeros and ones, if we had categorical labels in our training set, we might have transformed those into a zero-one encoding, where each dimension of this target vector corresponds to a possible label, and the target would be zero for all of the dimensions except for"
        },
        {
            "start_time": 750,
            "end_time": 780,
            "transcription": " for the one corresponding to the correct label for that data point. So if we train our network to output this sort of one-hot vector, we would need to then decode that back into the label that we actually want as our prediction. We might also want, as a post-processing step, to convey information about how confident we are in various predictions the neural network is making. And we need to think"
        },
        {
            "start_time": 780,
            "end_time": 810,
            "transcription": " about what will be done with the predictions that our model produces. For example, are they going to be interpreted by a human, like say, a system that is trying to help a doctor make a diagnosis, in which case it might be a very good idea to produce plots or other visualizations that help a human to understand the data set or the predictions as a whole? Or is the output of our model going to be fed"
        },
        {
            "start_time": 810,
            "end_time": 840,
            "transcription": " into some algorithm or other computation, like recognizing a vehicle that will then be part of the decision process for a self-driving car, in which case we need to think about the data representation that will be used by the next steps of the computation within which our model is embedded. And finally, as the deep learning practitioners who trained the model, we need to be thinking about what sort of validation can"
        },
        {
            "start_time": 840,
            "end_time": 857.663875,
            "transcription": " we do to check that it's working, and what sort of tuning can we do to help improve it? And for these sorts of analysis, we can take advantage of the data that we held back at the pre-processing stage. And these sorts of validation and tuning will be the subject of our next video."
        }
    ],
    "Auto-Encoders (DL 22)": [
        {
            "start_time": 0,
            "end_time": 30,
            "transcription": " An idea we encounter in many different deep learning contexts is that we can often train a neural network with one dataset that's easy to get or easy to use and then take some hidden layer from that network and use it as a way of encoding data that can subsequently be applied to other problems or datasets."
        },
        {
            "start_time": 30,
            "end_time": 60,
            "transcription": " for learning with residual networks, we can think of the pre-training as learning some sort of useful image processing that can then be applied to a different data set of images. And when we throw away the output layers of the pre-trained model and add different output layers for a new task, it's kind of like we have a new network"
        },
        {
            "start_time": 60,
            "end_time": 90,
            "transcription": " that is processing the encoded data that the pre-trained model produces. Likewise, with word embeddings, producing this sort of data encoding is explicitly the purpose of the learning. And this comes up in all sorts of other contexts as well. The deep learning model that takes this idea to its extreme is the auto encoder. An auto encoder is a network"
        },
        {
            "start_time": 90,
            "end_time": 120,
            "transcription": " that is trained on a completely unsupervised dataset because the data that we give as input to the network is also the data that the network is trained to output. This means we don't need any labels and we're solving a regression problem that would be utterly trivial to solve with linear regression. But mapping a dataset to itself isn't actually the point here."
        },
        {
            "start_time": 120,
            "end_time": 150,
            "transcription": " The goal is that if we make the hidden layers progressively smaller, we will force the network to learn some representation that shows up in its hidden layers that is more compact than the input that we gave. And if the network is able to reproduce the original input consistently from that smaller"
        },
        {
            "start_time": 150,
            "end_time": 180,
            "transcription": " representation, this network is learning to perform data compression. Supposing, for example, that we had a 200 by 200 pixel image as the input and the output, then we have 20,000 dimensional data points. But if we could bring that down to a smallest hidden layer of, say, 1,000 neurons, and if the network is able to, from those 1,000 hidden neurons and"
        },
        {
            "start_time": 180,
            "end_time": 210,
            "transcription": " the middle, expand back out and reproduce a very good approximation of the input image, then we could think of the first half of this network as performing 20 to 1 compression, and the back half of the network as performing decompression. But this idea of using a neural network to perform data compression isn't actually all that useful. There are way better, non-learning-based"
        },
        {
            "start_time": 210,
            "end_time": 240,
            "transcription": " algorithms for compressing various sorts of data. So where we can actually make use of this sort of auto encoder is if either the first half of the network is useful in producing a representation that we give as input for some other deep learning problem, or if the second half of the network is useful for producing examples of data from the input set. The idea of"
        },
        {
            "start_time": 240,
            "end_time": 270,
            "transcription": " of using the first half of an auto encoder for transfer learning was popular in the early days of deep learning, but now we have way better approaches to transfer learning. But the idea of using the second half of the network as a decoder that produces examples from a data set is the starting point for many other deep learning algorithms. The simplest of these is if we have achieved the"
        },
        {
            "start_time": 270,
            "end_time": 300,
            "transcription": " maximal possible compression by making this hidden layer as small as we possibly could, then it ought to be the case that just about any reasonable vector that we could give as input here will produce something that looks like data from the data set if we run it through the decoder half of the network. So if we wanted to generate random samples of data that"
        },
        {
            "start_time": 300,
            "end_time": 330,
            "transcription": " looks like the distribution of our input data, we could generate a random vector and then feed it through the decoder in order to produce a random example of the type of data that the network was trained on. Unfortunately, that only works if we really have achieved the maximum possible compression here. And it's really hard in general to be sure that you've done"
        },
        {
            "start_time": 330,
            "end_time": 360,
            "transcription": " What's much more likely is that you'll either find this hidden layer is much too small and no matter how much you train it, it can't get very good at reproducing the inputs. Or this is too big and there are some possible input vectors that we could give to the decoder that don't actually resemble anything from the dataset. To build some intuition for this, think about the oversimplified case when"
        },
        {
            "start_time": 360,
            "end_time": 390,
            "transcription": " this smallest hidden layer is just two neurons, where we can then plot the values of those two neurons. And if the black points are all of the values that we got for these two neurons, when we passed the data through after training, if we were to randomly generate a Z1Z2 vector, we could end up somewhere"
        },
        {
            "start_time": 390,
            "end_time": 420,
            "transcription": " over here, that's not anywhere near the data from the data set. So when we pass this through the decoder, the output is probably just going to look like random garbage. So if we want the decoder that we train to be useful for randomly sampling from a data set, we can change the architecture slightly to encourage the auto encoder to learn representations that look like the"
        },
        {
            "start_time": 420,
            "end_time": 450,
            "transcription": " sorts of things we would get if we sampled random vectors, and that leads us to the variational autoencoder. The idea of a variational autoencoder is very similar to the regular autoencoder, except that we replace this middle hidden vector that is the encoding of the data with two vectors that serve as a mean and a variable."
        },
        {
            "start_time": 450,
            "end_time": 480,
            "transcription": " Then we likewise train the auto encoder by giving it an input and the same input is the target. So we are encouraging the network to produce a compressed representation in the encoder and then decompress that representation in the decoder. But the representation gets combined in a particular way. We have a random vector R that gets generated by sampling each element from the"
        },
        {
            "start_time": 480,
            "end_time": 510,
            "transcription": " from a normal distribution with mean zero and standard deviation one, then that gets combined with the two vectors that are the hidden encoding of the data. The random vector gets multiplied with our variance vector, and then that gets added to the mean vector, and that produces the vector that goes as input into the decoder portion of the network. When we train this network, the decoder portion"
        },
        {
            "start_time": 510,
            "end_time": 540,
            "transcription": " has loss that comes from how close it got to reproducing the input and that decoder loss also passes back and influences the weights in the encoder. But the loss for the encoder network also has a second component that gets added to the decoder loss and that is a divergence between the mean and variance that it is learned."
        },
        {
            "start_time": 540,
            "end_time": 570,
            "transcription": " and a normal 0,1 distribution. So this loss is trying to ensure that the encoder doesn't stray very far from outputting a mean of 0 and a variance of 1 on each dimension, which means that we will get representations that are clustered around the center of the space of possible representations. And so later, if we randomly"
        },
        {
            "start_time": 570,
            "end_time": 600,
            "transcription": " sample, we are unlikely to get things that are far away from the learned distribution. So once we have trained the auto encoder, if we want to generate samples that look like the distribution of our data, we can generate random z vectors, sampling them from a normal distribution on each dimension, and give those to the decoder. And because of the"
        },
        {
            "start_time": 600,
            "end_time": 630,
            "transcription": " fact that we used this divergence loss and also a regularization that I haven't shown here. We can be reasonably confident that any vector we sample in this way is reasonably close to the sorts of representations the encoder was producing. And so when we pass this through the decoder it will produce an output that is very likely to resemble"
        },
        {
            "start_time": 630,
            "end_time": 660,
            "transcription": " the sorts of data in the training set. So what all this does is takes the idea of using a neural networks hidden layer as a data encoding and turns that into an encoding that we can sample from, which means it's possible to use a neural network to learn a probability distribution over a data set. And this gives us the starting point for generative adversarial"
        },
        {
            "start_time": 660,
            "end_time": 665.7741875,
            "transcription": " networks that we can use to generate all kinds of interesting data."
        }
    ],
    "What can a single neuron compute\uff1f (DL 03)": [
        {
            "start_time": 0,
            "end_time": 30,
            "transcription": " Before we try to wrap our heads around neural networks, which can have many many nodes and an enormous number of connections between them, it will help us to zoom in on an individual neuron and explore what a single neuron can compute, what types of model that allows us to express, and how those models can be trained. A node in a neural network receives some number of inputs"
        },
        {
            "start_time": 30,
            "end_time": 60,
            "transcription": " and performs a simple computation to produce a numerical output. That computation happens in two stages. First, we multiply each of the inputs by the corresponding weight and sum them up. Then that sum of weighted inputs is passed through an activation function to produce the output. To express this mathematically, our output comes from applying some activation function"
        },
        {
            "start_time": 60,
            "end_time": 90,
            "transcription": " to the result of summing up the weighted inputs. So our output is the result of applying the activation function to the sum of each weight times the input plus a bias term. The purpose of the bias is to allow this sum to be non-zero"
        },
        {
            "start_time": 90,
            "end_time": 120,
            "transcription": " even if all of the inputs were zero. And we can think of this bias as behaving like another weight, and so I will often draw it as another arrow coming into the node. So every time we have a neuron, it will perform a weighted sum over its inputs, but different neurons can have different activation functions. And in the context of a single neuron model, there are a couple of"
        },
        {
            "start_time": 120,
            "end_time": 150,
            "transcription": " activation functions I'd like to highlight. Specifically, we can use a linear activation function to make our one neuron model perform regression, or we can use a step function to get our single neuron model to perform a classification. In this first example, we have a neuron with a single input, and so the weighted sum of inputs comes from"
        },
        {
            "start_time": 150,
            "end_time": 180,
            "transcription": " simply multiplying input by weight and adding the bias. And then the linear function I have chosen to go from this weighted sum of inputs to our output is simply y equals x. Why have I chosen this particularly simple linear function? Well, we already have the ability to express any linear function of x1 using our parameters w1 and b."
        },
        {
            "start_time": 180,
            "end_time": 210,
            "transcription": " So there's no additional benefit to more complicated activations. So if we ask what type of things can this neuron compute, any function that it represents will have a one-dimensional input, x1, and it will always produce a one-dimensional output, y, and the mapping from input to output will be a linear function. So if we pick particular values for the weight and the bias, we can then draw the"
        },
        {
            "start_time": 210,
            "end_time": 240,
            "transcription": " the specific linear function that the neuron computes. So when w1 is 3 and b is minus 2, this neuron is computing the function y equals 3x1 minus 2. So how is this related to regression? Well, in regression, we are looking for some mapping from continuous inputs to continuous outputs. And since our"
        },
        {
            "start_time": 240,
            "end_time": 270,
            "transcription": " our neuron produces continuous outputs. This is a type of function we could use for regression. And then the regression problem would be to choose the specific linear function, that is the specific values for w1 and b, that best fits some data set. It's also worth thinking about what would it look like if this neuron had more inputs? Well, then we would be mapping a multi-dimensional input to a 1"
        },
        {
            "start_time": 270,
            "end_time": 300,
            "transcription": " one-dimensional output, but we would still have a linear function and could still use that for regression, but I would no longer be able to draw the plot. Here, our input is one dimension and our output is one dimension. So if we had, say, three inputs, then we would have a three-dimensional input mapped to a one-dimensional output, and we can definitely compute that, but it's hard to draw on the whiteboard. For this case, I've shown a neuron"
        },
        {
            "start_time": 300,
            "end_time": 330,
            "transcription": " with two inputs and a step function for its activation. Again, we start by calculating the weighted sum of inputs, and we can describe this sort of activation using a piecewise function. So our activation steps from zero to one when the weighted sum of inputs become"
        },
        {
            "start_time": 330,
            "end_time": 360,
            "transcription": " positive. Once again, if we pick example values for the weights and the bias, we could plot this on a two-dimensional graph because here we have two-dimensional inputs but our output is now just 0 or 1. So I will draw the boundary in the two-dimensional plane between the inputs where the neuron output 0 and the inputs for which the neuron outputs 1."
        },
        {
            "start_time": 360,
            "end_time": 390,
            "transcription": " That boundary happens where the weighted sum of inputs is equal to zero. So if we're using this for classification, then we are labeling the inputs on one side, zero, and labeling the inputs on the other side, one. So the machine learning problem here would be based on the data set to choose the best W1, W2, and B,"
        },
        {
            "start_time": 390,
            "end_time": 420,
            "transcription": " to classify the data. And once again, we can think about cases where the neuron has other numbers of inputs. If we had only one input here, then we could represent that single dimension with a line. And our function would be specifying some classification boundary where on one side we output zero and on the other side we output one. Thinking about how this generalizes with one dimensional"
        },
        {
            "start_time": 420,
            "end_time": 450,
            "transcription": " inputs, we are specifying some point on a line. With two-dimensional inputs, we specify some line in the plane. If we had three inputs, then our classifier would be specifying some plane that divides 3D space. And in general, we can think of this sort of classifier as splitting n-dimensional space into the half where we output 0 and the half where we output 1. Now that we have a sense of the"
        },
        {
            "start_time": 450,
            "end_time": 480,
            "transcription": " of functions we can represent with a single neuron. Let's consider how we can use those functions to perform regression or classification. For any supervised learning problem, we will have a data set that consists of many examples of what we think the function should output. Each of those examples has an input and an output, so we can think of our data set as a large collection of"
        },
        {
            "start_time": 480,
            "end_time": 510,
            "transcription": " input output pairs. The mathematical representation of the input and the output determines what sort of single neuron model we can use. First, if the output is binary zero or one, then we should use a step function for our activation. And if the output is continuous, then we should use a linear activation function. The input examples in the data set can in principle have very"
        },
        {
            "start_time": 510,
            "end_time": 540,
            "transcription": " and the dimension of the input examples will determine the number of inputs and therefore the number of weights in our single neuron model. So if our input examples were two-dimensional and our outputs were continuous, then we would pick a linear neuron with two inputs and therefore two weights. And the machine learning problem would be"
        },
        {
            "start_time": 540,
            "end_time": 570,
            "transcription": " to choose w1, w2, and b in a way that best represents the examples in the data set. On the other hand, if we had four-dimensional inputs and binary 0,1 outputs, then we would choose a step function for our activation, and we'd be trying to solve the machine learning problem of picking the best value for all five of the resulting parameters. The process of picking these parameters"
        },
        {
            "start_time": 570,
            "end_time": 600,
            "transcription": " to best represent a data set is called training. And to train a neural network or a single neuron, we need to define a loss function. The job of the loss function is to tell us for the current values of the parameters, how wrong is the model in terms of representing the data. If our data set is for one dimensional regression like this example, then we will have input output"
        },
        {
            "start_time": 600,
            "end_time": 630,
            "transcription": " pairs that we can plot on this graph. And the loss function should be based on how far is the prediction that the model makes for certain inputs from the value in the data set at those inputs. For our classification example here, our input points have both an x1"
        },
        {
            "start_time": 630,
            "end_time": 660,
            "transcription": " and an x2 value, and then we can represent whether the label should be 1 or 0 by the color here. And so our model is wrong when it puts points on the wrong side of the decision boundary. So the goal of the loss function is to tell us for the current parameter values how wrong is the model on the data set. And the first loss function we will consider is the sum of squares"
        },
        {
            "start_time": 660,
            "end_time": 690,
            "transcription": " that is for each data point in the model, we'll take the difference between what the model predicted and what we should have gotten, square that difference, and sum them up. And now that we've defined a loss function that describes how wrong our model is, our goal is to update the parameters."
        },
        {
            "start_time": 690,
            "end_time": 705.805375,
            "transcription": " in a way that reduces the loss, making our model less wrong. And the key idea for how we can update the parameters to reduce the loss is gradient descent, which will be the subject of our next video."
        }
    ],
    "Transformers and Self-Attention (DL 19)": [
        {
            "start_time": 0,
            "end_time": 30,
            "transcription": " Neural networks based on the transformer architecture have achieved state-of-the-art performance on language modeling and many other tasks. So let's take a look at some of the core ideas behind the transformers, including how they're built from self-attention blocks and how they combine some of the best aspects of their recurrent and residual network predecessors. Recurrent neural networks are good at text processing because they receive word embeddings as input and then will"
        },
        {
            "start_time": 30,
            "end_time": 60,
            "transcription": " a sequence of words, gradually building up a hidden state that over time will come to represent the information content of a document. And they can be trained to do this using readily available data, using simple unsupervised tasks like predict the next word in a sentence. But recurrent networks, even LSTMs, tend to struggle with long inputs,"
        },
        {
            "start_time": 60,
            "end_time": 90,
            "transcription": " because the data has to be passed through the layers so many times. On the other hand, residual networks are great at handling data that passes through many, many layers, because the residual connections mean that each block only needs to augment the input, and that gradients have a shorter path to follow, both of which simplify training of deep networks. Another advantage of"
        },
        {
            "start_time": 90,
            "end_time": 120,
            "transcription": " of residual networks when applied for image processing is that they can use convolution within the residual blocks, meaning that the architecture gives a leg up on the sorts of functions that are likely to be useful for image processing. So we'd like some way of combining the advantages in processing text of recurrent networks with the advantages in learning deep models of residual"
        },
        {
            "start_time": 120,
            "end_time": 150,
            "transcription": " and that leads us to the transformer. Much like a recurrent network, a transformer operates on word embeddings, but unlike a recurrent network that receives words one at a time, a transformer receives all of the embeddings for an entire document concatenated together into a matrix where every row is the embedding of a different word. And the"
        },
        {
            "start_time": 150,
            "end_time": 180,
            "transcription": " Like recurrent networks, a transformer can be trained on unsupervised prediction of missing words to produce an encoding of a document that can be used for all sorts of natural language processing tasks. But from residual networks, the transformer inherits skip connections that mean each block only needs to augment its predecessors, and that gradients can propagate quickly through even"
        },
        {
            "start_time": 180,
            "end_time": 210,
            "transcription": " large network. But to do text processing, we need a very different sort of architecture within the blocks. And the key idea for setting up the architecture within the blocks so that it's easy for the network to learn useful functions is known as self-attention. Self-attention is the idea that in text processing, to understand one word in a sentence, we have to pay attention to other words that might be far away."
        },
        {
            "start_time": 210,
            "end_time": 240,
            "transcription": " in the sentence, but maybe we can learn how to pay attention to the right things. As an example of what we mean, consider this sentence from one of my favorite deep learning papers on de-bias and word embeddings. When we read the sentence, the analogies generated from these embeddings spell out the bias implicit in the data on which they were trained. We encounter at the end of the sentence"
        },
        {
            "start_time": 240,
            "end_time": 270,
            "transcription": " the pronoun they. And grammatically, that pronoun can refer back to any of the noun phrases appearing earlier in the sentence. But as speakers of English with a little knowledge of deep learning, we can infer that since they are being trained, that refers back to the embeddings. And so understanding the end of the sentence requires us"
        },
        {
            "start_time": 270,
            "end_time": 300,
            "transcription": " to think back to context that was established earlier. And so for a neural network to do good natural language processing, we'd like to set up its architecture to help it with that sort of identification of what should it be paying attention to to figure out what's happening later in the sentence. So the idea of self-attention is for each word, we'd like to figure out which other words in the"
        },
        {
            "start_time": 300,
            "end_time": 330,
            "transcription": " the document should it be paying attention to to really understand the meaning. But as was the case when we talked about convolution, we're not going to explicitly engineer a specific attention function into the network. Rather, we are going to set up the architecture in a way that seems like it makes that type of function easy to learn. If it's possible to learn this sort of thing, where for each one of"
        },
        {
            "start_time": 330,
            "end_time": 360,
            "transcription": " we can figure out what other parts of the document are most important to understanding its meaning. And if that were the output of our blocks, then that would be a great way to enhance the word embeddings and produce a good encoding of the entire document. So it would be great if we could calculate this self-attention, but in general we don't know how to do that. But maybe we can set up the archival"
        },
        {
            "start_time": 360,
            "end_time": 390,
            "transcription": " architecture of our blocks in a way that facilitates learning this style of function. So let's think about the architecture within the first of these self-attention encoder blocks. The first block in the network receives all of the word embeddings as its input, and for each of those words for which it has an embedding, it computes three"
        },
        {
            "start_time": 390,
            "end_time": 420,
            "transcription": " different dense layers. Those dense layers are called Q, K, and V for query, key, and value. And we have the same set of three dense layers being applied to every word in the document. Like with convolution, all of the query layers for every single word share the same weights. They are just being applied to different"
        },
        {
            "start_time": 420,
            "end_time": 450,
            "transcription": " elements of the input sentence. From here, the idea is that we can take a dot product between the query and key vectors to assess similarity. As we know from linear algebra, when we take the dot product of two vectors, that value will be large if the vectors point in roughly the same direction, and the dot product will be small if those vectors"
        },
        {
            "start_time": 450,
            "end_time": 480,
            "transcription": " point in very different directions. So if the query and key vectors that we got from the dense layers are similar, then we will get a large value when we compute their dot product. Here I've shown the dot product between the query and key vectors for the same word, but we'll also take the dot product between the query vector for the"
        },
        {
            "start_time": 480,
            "end_time": 510,
            "transcription": " and the key vector for each of the other words. And so we'll get a vector of similarity scores between the query vector for this word and the key vectors for every other word. We don't know exactly what the Q and K vectors mean because they are the result of a dense layer calculation."
        },
        {
            "start_time": 510,
            "end_time": 540,
            "transcription": " that will be trained by gradient descent, but if the Q and K layers are extracting some kind of useful information about every word, then we can assess the similarity between this word's Q vector and all of the other Ks. Then we can apply a softmax to convert those similarity scores to values between 0 and 1 that emphasize the most similar of the vector."
        },
        {
            "start_time": 540,
            "end_time": 570,
            "transcription": " So this vector represents for all of the keys which are most similar to the query for the word analogies. And we can use that to say what should we pay attention to when evaluating the word analogies. And the way we actually pay attention is we'll use those softmax weights as multipliers on all of the value vectors"
        },
        {
            "start_time": 570,
            "end_time": 600,
            "transcription": " for every word in the document. So now we take each of the vectors that were the output of the value dense layers. And each of these vectors gets multiplied element-wise with the corresponding entry in the softmax vector. So whatever similarity score we"
        },
        {
            "start_time": 600,
            "end_time": 630,
            "transcription": " calculated between the query for analogies and the key for embeddings will be multiplied by the value vector for embeddings. So we are taking all of these value vectors and performing a weighted sum according to how much attention we want to pay. And this gives us an output vector that was computed from all of the values in the document."
        },
        {
            "start_time": 630,
            "end_time": 660,
            "transcription": " but weighted by how much we decided to pay attention to each of them on the word analogies. And this whole structure is being applied in parallel for each of the words. So we will also use the query vector for embeddings to compute similarities with all of the key vectors and then take a softmax and apply that to all of the key vectors."
        },
        {
            "start_time": 660,
            "end_time": 690,
            "transcription": " of the values to get our output vector for embeddings and an output vector for they and for all of the other words in the input. And the result when we squish all of these vectors together is a matrix where every row is calculated from the entire document according to how much attention we wanted to pay for that word. And we will"
        },
        {
            "start_time": 690,
            "end_time": 720,
            "transcription": " Now add this information that came from the entire document onto the original embedding. So we have now taken the original words and added to each word content that came from the entire document, but based on how we decided to pay attention to the different parts of the document. From there we'll apply a regular dense layer of matching shape, and this gives the"
        },
        {
            "start_time": 720,
            "end_time": 750,
            "transcription": " output of one attention head. But, much like we want many channels in a convolutional layer, we often want many attention heads within a given encoder block. And so, all of this gets duplicated for however many attention heads we have. And then, at the end, we can take the output of all the different attention heads."
        },
        {
            "start_time": 750,
            "end_time": 780,
            "transcription": " and add them up as our output of the block, which can then be combined with the skip connection that goes around the block. And so by taking a dot product of query and key vectors and applying a softmax to produce the weights that we will use to sum up the value vectors, we set the network up with an architecture that allows it to learn based on the"
        },
        {
            "start_time": 780,
            "end_time": 810,
            "transcription": " From the weights it finds for these dense layers, what it wants to pay attention to in each of the attention heads, and because it has several different attention heads, it can learn to pay attention to different things under different circumstances, and combine all of those different attention calculations to augment the input representation into a useful encoding of a text-to-speech."
        },
        {
            "start_time": 810,
            "end_time": 840,
            "transcription": " document. Once we produce this sort of encoding, there are lots of things we could do. Most straightforwardly, we could just apply some additional layers to the encoding to produce a classification output. But it turns out that this encoding often contains enough information that we can do way more, such as machine translation. If we use this network to"
        },
        {
            "start_time": 840,
            "end_time": 870,
            "transcription": " produce an encoding of a document in English, and we want to translate that to some other language, we can use this encoding as the input to another neural network. And this was in fact the problem for which the transformer architecture was originally proposed, and that paper described in addition to this sequence of encoder blocks, a sequence of additional self-attention blocks that serve"
        },
        {
            "start_time": 870,
            "end_time": 900,
            "transcription": " as the decoder to produce output in another language. And translation is a great task for training language models, because if we want to translate text from one language to another, we really have to understand a lot about the meaning of that text. And so the first transformers were trained on performing encoding in one language, and then decoding with similar self-attention layers in another language."
        },
        {
            "start_time": 900,
            "end_time": 930,
            "transcription": " But we can also do unsupervised training of transformers, much like we would with a recurrent architecture. Only with a transformer, we can't ask it to predict the next word because it sees the whole input at once. And the solution here is to train the network on documents where some of the words have been randomly blanked out."
        },
        {
            "start_time": 930,
            "end_time": 960,
            "transcription": " from the input, then, much like with recurrent networks, we can ask the model to produce those missing words as output, and by gradient descent training, force it to come up with an encoding that is good enough to make at least a reasonable guess as to what the missing words might have been. One issue that I've glossed over so far is that the architecture here has no inherent understanding of which"
        },
        {
            "start_time": 960,
            "end_time": 990,
            "transcription": " words are near one another in the sentence. And the ordering and proximity of words is often very important to understanding language. So as part of the input to the network, we need to give it some information about where the words are. And that is done via a positional encoding. The positional encoding will be a matrix of the same size as"
        },
        {
            "start_time": 990,
            "end_time": 1020,
            "transcription": " the input embedding, and when we add it to the input, it will modify each line in a way that is unique to that position in the text. And there are a number of different ways of producing positional embeddings, some of which have been manually engineered, and some of which have been learned. But as long as we give the network some useful information about position, then it can learn"
        },
        {
            "start_time": 1020,
            "end_time": 1050,
            "transcription": " things that are dependent on word ordering or word proximity. What I've shown you here, while plenty complicated on its own, is just an introduction to transformers. And there are lots of variations on transformers that use encoders or decoders separately, or both together, and train on all kinds of different data sets to solve tons of interesting problems, both in natural language processing and in"
        },
        {
            "start_time": 1050,
            "end_time": 1052.874,
            "transcription": " many other areas of deep learning."
        }
    ],
    "Making Neural Networks Fast with Vectorization (DL 10)": [
        {
            "start_time": 0,
            "end_time": 30,
            "transcription": " To understand what's going on under the hood of a neural network, it helps to dive down to the level of a single neuron and to think about the connections between individual neurons when we're performing a forward pass to compute activations or a backward pass to compute deltas. And while thinking about nodes and edges is great for building intuition, it's not at all how deep learning actually works. If we want to build large"
        },
        {
            "start_time": 30,
            "end_time": 60,
            "transcription": " neural networks that can be trained efficiently, we have to move to a higher level of abstraction and start thinking in terms of vectors, matrices, and tensors. Our first step in that direction is to represent a layer's activations as a vector. If our neural network is organized into layers, then we can collect the activations of a layer into a vector. Here, the vector AL stores all of the activations"
        },
        {
            "start_time": 60,
            "end_time": 90,
            "transcription": " for layer L, and so it has as many entries as the layer has nodes. Likewise, when we're performing back propagation, we can collect the deltas for a layer into a vector where each entry gives us the delta value for one of the nodes in the layer. And along the same lines, we can collect up a vector to represent a layer's biases or its inputs."
        },
        {
            "start_time": 90,
            "end_time": 120,
            "transcription": " And to translate the computations we've been performing into this notation, we can first think about how a node computes its weighted sum of inputs. The input x5 that goes into the activation function for node 5 is calculated as a weighted sum of previous layer activations plus a bias. And so if we've collected those previous layer activations into the vector ak, and we have a vector of"
        },
        {
            "start_time": 120,
            "end_time": 150,
            "transcription": " all of the weights coming into node 5, then the weighted sum of inputs is a dot product between those two vectors. And another way of writing the dot product is to transpose the first vector and multiply the row vector with the column vector. So we can express the input to node 5 as the weight vector coming into node 5 transpose times the activation"
        },
        {
            "start_time": 150,
            "end_time": 180,
            "transcription": " vector for the previous layer plus node 5's bias. But this vectorized notation can go much further and let us talk about calculating the entire vector of inputs to layer L at once. If we combine our row vector of weights for node 5 with row vectors of weights for each of the other neurons in that layer, the result is a"
        },
        {
            "start_time": 180,
            "end_time": 210,
            "transcription": " matrix containing all of the weights from layer K to layer L. This weight matrix has as many rows as there are nodes in layer L because each row is a vector of weights into one of the layer L neurons. And it has as many columns as there are nodes in the previous layer K because each column"
        },
        {
            "start_time": 210,
            "end_time": 240,
            "transcription": " of this matrix corresponds to a vector of weights coming out of one of the layer k nodes. And if we multiply this weight matrix times the activation vector for layer k, the result is equivalent to having performed each of the dot products with a row from the matrix and the vector of activations. So this will give us a"
        },
        {
            "start_time": 240,
            "end_time": 270,
            "transcription": " a three-element vector where each element is the weighted sum of inputs for one of the layer L nodes. And to get the activation function inputs, we need to add on each of the biases, but since we've collected the biases into a vector, that can be added to the three vector we get from this multiplication. And we've now expressed in terms of a matrix vector multiplication and a vector addition,"
        },
        {
            "start_time": 270,
            "end_time": 300,
            "transcription": " operations for calculating all of the inputs to this layer that used to take us a double for loop. Once we have this vector of inputs, we can think about getting the vector of activations for this layer. And each activation is computed by applying the activation function to that node's input. But if our activation function can be vectorized, that is broadcast,"
        },
        {
            "start_time": 300,
            "end_time": 330,
            "transcription": " to operate on each element of an array, then we can calculate all of the activations by a single call to our vectorized activation function. I will use the notation f dot, which is consistent with how Julia represents element wise operations to indicate that the function f is being applied to every element of the vector xl."
        },
        {
            "start_time": 330,
            "end_time": 360,
            "transcription": " and this produces each of the elements of the vector AL. And this means that an entire forward pass through a neural network can be performed by matrix vector multiplication, vector addition, and element-wise functions. So now we'd like to vectorize the backward pass where we're computing deltas. When thinking about one neuron at a time, the delta for a node in layer K was a weighted sum"
        },
        {
            "start_time": 360,
            "end_time": 390,
            "transcription": " of all of the deltas at the next layer times the derivative of that node's activation function. But once again, the weighted sum can be expressed as a dot product. And so we can think of that operation as multiplying a row vector of weights coming out of node three by the vector of deltas for layer L and then multiplying by the activation derivative. But as we did before, we'd like to think"
        },
        {
            "start_time": 390,
            "end_time": 420,
            "transcription": " collect up the calculation of all the deltas in a layer into a single vectorized operation. And again, we can use our weight matrix because it collects w3 and each of the other vectors of weights coming out of layer L neurons. And we're going to multiply it by the vector of deltas for layer L. And since that is a 3 vector, and we want to get a 4 vector"
        },
        {
            "start_time": 420,
            "end_time": 450,
            "transcription": " as output, we need to transpose this matrix so that we get a 4 by 3, which we can multiply by our 3 vector and get a 4 vector. So this multiplication is equivalent to doing 4 dot products. The first is between the delta L vector and the vector of weights coming out of node 1. The second is between delta L and the weight"
        },
        {
            "start_time": 450,
            "end_time": 480,
            "transcription": " 2 vector and the last two components correspond to nodes 3 and 4. But then for each of those nodes where we've done a dot product, we need to multiply the result by the activation derivative for that neuron. And we can use another vectorized function for the derivative of f. And since we think of our derivatives in terms of the activations, this is"
        },
        {
            "start_time": 480,
            "end_time": 510,
            "transcription": " evaluated on the a k vector. But now to get this delta vector for layer k, we want each element to correspond to multiplying the derivative by the result of the dot product. And so that means we want element one of this vector to be multiplied by element one of this vector, element two multiplied by element two, and so on to build up the delta k vector. And the name of the operation that we"
        },
        {
            "start_time": 510,
            "end_time": 540,
            "transcription": " does this element-wise multiplication between two vectors is the Hadamard product. I'll use this symbol with a dot inside a circle to indicate the Hadamard product, but the name here makes it sound way fancier than it is. To implement this operation in Julia, we can just do dot times to cause each corresponding element of the two vectors to be multiplied into the output vector. So now we've"
        },
        {
            "start_time": 540,
            "end_time": 570,
            "transcription": " we've got to vectorize the forward pass to compute activations and the backward pass to compute deltas, but we'd still like to vectorize the third pass over the neural network to update the weights. But before we get there, we can make our calculations even more efficient by vectorizing over another dimension, which stores the batch. If we want to compute activations on an entire batch of data, we're performing the same sequence of operations"
        },
        {
            "start_time": 570,
            "end_time": 600,
            "transcription": " on each of the input points. And so we can collect up the input vectors and combine them into an input matrix, and then replace our matrix vector multiplications with matrix matrix multiplication to operate on the entire batch at once. So now at each layer,"
        },
        {
            "start_time": 600,
            "end_time": 630,
            "transcription": " instead of having just a vector of the activations for a single data point, we will have a matrix where each column is a vector of activations for one data point, and we will have as many columns as there are data points in the batch. Now when we multiply the weight matrix by this matrix of activations, it will be like simultaneously doing the multiplication of the weight matrix times"
        },
        {
            "start_time": 630,
            "end_time": 660,
            "transcription": " the vector for each of these vectors. So now we can produce the matrix of inputs to layer L, where each column of this matrix corresponds to the input vector for a different data point by performing the matrix multiplication between the weights and the previous layer activations, and then adding the vector of biases."
        },
        {
            "start_time": 660,
            "end_time": 690,
            "transcription": " But my notation here indicates that we are adding a matrix plus a vector. And what we really want is to add this vector to each of the columns of the matrix, because before the input vector for each individual data point had the bias vector added on. And so we need to indicate that this is actually being added to each column of the matrix. And by analogy with our Hadamard product,"
        },
        {
            "start_time": 690,
            "end_time": 720,
            "transcription": " put a circle around the plus. This is not a standard symbol, but I hope it's clear what it means. And again, in Julia, we can implement this with a dot plus operation that will broadcast over the columns of the matrix. And we'd like to continue moving forward in terms of these batch matrices so we can think about the activations for the next layer by applying our element-wise activation function to the entire input matrix."
        },
        {
            "start_time": 720,
            "end_time": 750,
            "transcription": " Now that we have operations to go from the matrix of activations from one layer via the inputs to the matrix of activations for the next layer, we can perform a single forward pass to calculate the activations for an entire batch of data. And now we'd like to vectorize over the batch on our backward pass. And the idea is similar. We'll define"
        },
        {
            "start_time": 750,
            "end_time": 780,
            "transcription": " a matrix of deltas much like we defined a matrix of activations. Where again, all we've done here is collect up the delta vectors for individual data points into the columns of this matrix. So the matrix now represents all of the deltas on layer L for the entire"
        },
        {
            "start_time": 780,
            "end_time": 810,
            "transcription": " batch. And now we'd like to extend our calculation of the delta vector to get the delta matrix for the previous layer. Once again, our matrix version looks an awful lot like the vector version. Only we've extended out this vector of deltas into a matrix where each column is the deltas for a different data point."
        },
        {
            "start_time": 810,
            "end_time": 840,
            "transcription": " And we've also extended out this vector of activations into an activation matrix. And the result is that with a single matrix multiplication, we can calculate the weighted sum of next layer deltas for every node in layer k and every data point in the batch. And then we can element-wise multiply with our activation derivatives to get the deltas at layer k."
        },
        {
            "start_time": 840,
            "end_time": 870,
            "transcription": " we have just two more steps remaining. The first is that while we know how to propagate deltas backwards, we haven't yet shown how to calculate the output deltas. This calculation will be different for various combinations of output layer activations and loss function. And while all of them can be vectorized, that would take us too far afield in this video, and so I'll leave it as an exercise for you. But the last step is to vectorize our updated"
        },
        {
            "start_time": 870,
            "end_time": 900,
            "transcription": " of weights and biases. We know from previous videos that the update to each weight is based on an average of delta times activation for each point in the batch. But just like all of our other weighted sums, it can be expressed as a dot product. Here we have the vector that collects"
        },
        {
            "start_time": 900,
            "end_time": 930,
            "transcription": " all of the deltas for node 5 across the batch, and this corresponds to the first row of our matrix of deltas for layer L, and we're dotting that with the vector of all of the activations for node 3 across the batch, which corresponds to the third row of our AK matrix. This dot product adds up delta"
        },
        {
            "start_time": 930,
            "end_time": 960,
            "transcription": " times activation for each point in the batch. And so dividing by batch size gives us an average update. And then we can take a step in the minus gradient direction of size determined by our learning rate eta. And yet again, we can collect up this update for all of the weights in this matrix as a single vectorized operation."
        },
        {
            "start_time": 960,
            "end_time": 990,
            "transcription": " If we multiply our matrix of deltas for layer L by the transpose of our weight matrix for layer K, since this is an L by B matrix and the transpose here is B by K, the result will be an L by K matrix whose dimensions match our weight matrix. And the first entry in this matrix comes from multiple"
        },
        {
            "start_time": 990,
            "end_time": 1020,
            "transcription": " multiplying the top row of this matrix by the first column of this matrix and the first column of the transpose is the first row of this matrix. So that will be the dot product between the vector of activations across the batch for the first neuron in layer L and the vector of deltas across the"
        },
        {
            "start_time": 1020,
            "end_time": 1050,
            "transcription": " the batch for the first neuron in layer k. And so this top left entry is the vector of batch deltas for node 5 dotted with the vector of batch activations for node 1, which is exactly what we need to perform the update to the weight from 1 to 5."
        },
        {
            "start_time": 1050,
            "end_time": 1080,
            "transcription": " And as we move down the first column of this matrix, we get the other dot products with this vector of activations for node one, with respectively the deltas for node six and the deltas for node seven. The next column will contain all of the dot products involving the activations for node two and then three and four. So I'll now finish right now."
        },
        {
            "start_time": 1080,
            "end_time": 1110,
            "transcription": " this out and note that I've dropped the B superscript, but remember that each of these vectors has length equal to the number of data points in the batch. And so we can use this matrix to perform the updates to the entire matrix of weights in a single operation."
        },
        {
            "start_time": 1110,
            "end_time": 1140,
            "transcription": " where we divide all of these dot products by the size of the batch to turn them into averages, and then we take a step in the minus gradient direction of size equal to our learning rate. The update for the bias is simpler because we just have to take an average across the columns to get a single"
        },
        {
            "start_time": 1140,
            "end_time": 1170,
            "transcription": " delta vector averaged over the batch, and then we can subtract a learning rate multiple of that vector from the biases. And so now we can do all of our computations on a feed-forward, densely connected neural network using matrix operations. In the vast majority of programming languages, just translating into matrices will give us a huge speed up"
        },
        {
            "start_time": 1170,
            "end_time": 1183.8345,
            "transcription": " up, but the biggest performance gain comes from the fact that matrix operations can be ported to other hardware, like a graphics processor, that can dramatically accelerate these operations."
        }
    ],
    "Word Embeddings (DL 16)": [
        {
            "start_time": 0,
            "end_time": 30,
            "transcription": " Up to now the vast majority of the data that we've been operating on with neural networks has been image data. Starting this week, I'd like to broaden that a bit and think about ways that we can use neural networks for other types of problems. And we'll start by thinking about how we can give text data as input to neural networks. With image data, standard digital storage formats with arrays of red, green and blue pixels are quite convenient"
        },
        {
            "start_time": 30,
            "end_time": 60,
            "transcription": " as inputs to a neural network. This works so well because the arrays capture spatial relationships between the pixels, and because the pixel values capture color intensities that are directly relevant to the sorts of computer vision or image processing tasks that we're trying to solve. For text data, on the other hand, the way to represent a document as input to a neural network is much less obvious. The"
        },
        {
            "start_time": 60,
            "end_time": 90,
            "transcription": " The standard digital representation of text, where we convert characters to ASCII or other digital values, is much less directly relevant to how neural networks learn. We could imagine various ways of converting these ASCII values into something that would be at least valid input to a neural network, like taking the binary representation of the ASCII values,"
        },
        {
            "start_time": 90,
            "end_time": 120,
            "transcription": " or normalizing the range of characters in the alphabet that we're working with to lie between 0 and 1 and then constructing vectors of those character values for each word. But neither of these representations is meaningful in anything close to the same way that the array representation of an"
        },
        {
            "start_time": 120,
            "end_time": 150,
            "transcription": " image is. If we think about words that are similar under these representations, in the normalized vector version, cat and car are identical in all but one dimension where they differ by.04. Whereas in the binary vector version cat and sat differ by a single bit flip. And so a neural network that's trying to make generalizations based on these"
        },
        {
            "start_time": 150,
            "end_time": 180,
            "transcription": " numerical values is going to be fooled an awful lot by getting very similar numerical values that have very different semantic meanings. One alternative that might seem a little crazy at first would be to create giant vectors which are one-hot encodings of our entire vocabulary. So we have a vector with an enormous number of dimensions"
        },
        {
            "start_time": 180,
            "end_time": 210,
            "transcription": " that is mostly zeros representing all of the other words in our dictionary and then a one in the position of the specific word that we are trying to represent. The size of this vector is the number of words that we would ever expect to encounter. So think on the order of a million dimensions. This does solve"
        },
        {
            "start_time": 210,
            "end_time": 240,
            "transcription": " the problem of words that are semantically very different being represented very similarly in that every word is represented completely differently by being literally the only word to have a one in its particular dimension. If we gave documents where each word was represented this way as input to a neural network, then the network would be forced to learn different weights for each word because"
        },
        {
            "start_time": 240,
            "end_time": 270,
            "transcription": " because each word is a totally separate input neuron. But the problems here should be obvious that we've swung much too far in the other direction towards having words be dissimilar and towards having a gigantic expansion of the number of dimensions required to represent our data. So it's worth enumerating what we'd like our representation of text data to actually achieve. So it makes sense to have a"
        },
        {
            "start_time": 270,
            "end_time": 300,
            "transcription": " word representation, at least if we are representing English. And we want it to be the case that our vector representation of each word is not too gigantic in terms of the number of dimensions for reasons both of data storage and learnability. And we'd also love for this numerical representation to carry some amount of actual semantics."
        },
        {
            "start_time": 300,
            "end_time": 330,
            "transcription": " that is words with similar meaning have similar vector representations and words with completely different meanings are not represented by similar vectors. Finding a good numerical representation of text documents to do machine learning on has been a big problem in natural language processing for a long time. But in the last"
        },
        {
            "start_time": 330,
            "end_time": 360,
            "transcription": " In the last five years, there has been great advancement in this area by using neural networks to solve the problem of generating an appropriate input representation to our neural networks. A key prerequisite to this sort of training is extracting N grams from text data. An N gram is just a sequence of N words, so a 5 gram would be a sequence of N"
        },
        {
            "start_time": 360,
            "end_time": 390,
            "transcription": " of five words that we extracted from a text document. So from this example sentence, if we were to extract all of the five grams, the first five gram would be the blind application of machine. The second five gram would be blind application of machine learning. The third would be application of machine learning runs."
        },
        {
            "start_time": 390,
            "end_time": 420,
            "transcription": " So on."
        },
        {
            "start_time": 420,
            "end_time": 450,
            "transcription": " that we will use for text documents is that we can train using this one-hot dictionary representation as the input to the training and then pass that to a neural network which is trying to predict the one-hot encoding of the surrounding N-gram. So for example, we could make the output be"
        },
        {
            "start_time": 450,
            "end_time": 480,
            "transcription": " the other four words in the N-gram, represented by their one-hot dictionary vectors, all concatenated together. So for example, one of the inputs in our data set could be the 5 gram runs the risk of amplifying, and we'd train on the input risk and try to predict the output vector runs the of amplifying."
        },
        {
            "start_time": 480,
            "end_time": 510,
            "transcription": " And so by training the network to recognize nearby words using these N-grams, we're not expecting that the network will ever be able to perfectly solve this task. Obviously for the input risk, there are lots of other 5 grams that it could be a part of. But this should have the effect that semantically similar words should appear in similar N-grams and should receive similar"
        },
        {
            "start_time": 510,
            "end_time": 540,
            "transcription": " gradient feedback in our neural network training. And to the extent that a network can learn anything on this task, we would expect that it would tend to produce for any given input some of the more common contexts in which that word can appear. And in order to do that, it must be the case that by the time we get to the last of the hidden layers in this network, the activations"
        },
        {
            "start_time": 540,
            "end_time": 570,
            "transcription": " in that hidden layer on a given input word must be somehow a representation of that word and its common contexts that has actively proven itself useful for predicting words that are in actual practice appearing near the input word. And so to return to one of our favorite tricks, we can throw away the output layer and use"
        },
        {
            "start_time": 570,
            "end_time": 600,
            "transcription": " use the vector of activations at this last hidden layer on the particular input as a numerical encoding of that input. And the result is what is known as a word embedding. Once we've thrown away the output layer, this neural network takes as inputs, words in our dictionary, and produces as output a"
        },
        {
            "start_time": 600,
            "end_time": 630,
            "transcription": " vector representation of that word that we know was at least somewhat useful in terms of predicting its context in actual text. There are many variations on the general idea I've shown here for producing word embeddings. What I have presented is a rough sketch of the approach behind word to VEC, which was one of the first successful embeddings"
        },
        {
            "start_time": 630,
            "end_time": 660,
            "transcription": " produced by a neural network. But there are a number of newer and more popular approaches based on the same general idea. What we will do in practice is not to train our own word embeddings, but in the spirit of transfer learning to use embeddings that have been trained by others with more data and more computation to hand. Because once such a model has been trained, it's quite easy to generate a lookup table"
        },
        {
            "start_time": 660,
            "end_time": 690,
            "transcription": " that lets you translate an arbitrary text document into the word embedding. And so our approach to doing machine learning on text data will be to first pass our document through an existing embedding and then use the vectorized representation produced by the embedding as the input to our neural network."
        },
        {
            "start_time": 690,
            "end_time": 690.80525,
            "transcription": " you"
        }
    ],
    "Feed-Forward Neural Networks (DL 07)": [
        {
            "start_time": 0,
            "end_time": 30,
            "transcription": " In this video, we're leveling up from individual neurons to multilayer neural networks. My aim is to give you an idea of how neural networks compute their predictions and why we would even prefer a network over a single neuron. Every node in a neural net performs the same computation we've seen before, which we can split into two stages, taking the weighted sum of inputs and applying an activation function."
        },
        {
            "start_time": 30,
            "end_time": 60,
            "transcription": " Only now, a neuron's input can come from previous neuron's activations, and the activation it computes can be passed along as input to later nodes. In the case of neuron 8 in this network, it receives its inputs from neurons 5, 6, and 7. So the weighted sum of inputs that neuron 8 performs adds up the activations of those neurons"
        },
        {
            "start_time": 60,
            "end_time": 90,
            "transcription": " times the weights on the corresponding edges plus the bias. So our sum is over activations 5, 6, and 7 times, respectively, the weights from 5 to 8, 6 to 8, and 7 to 8. Then as always, that weighted sum of inputs is passed through the activation function for node 8. And then this output"
        },
        {
            "start_time": 90,
            "end_time": 120,
            "transcription": " from neuron 8 is fed in as one of the inputs to nodes 11 and 12. For this activation function, we saw with single neuron models that if the output was linear, we could use the model for regression. And if the output was a sigmoid activation, we could use the model for classification. But now that we have many neurons in our network that are not"
        },
        {
            "start_time": 120,
            "end_time": 150,
            "transcription": " not directly producing the output of the model, it can be very useful to have some additional activation functions at our disposal. And two of the most common additional activation functions are the hyperbolic tangent, which behaves much like the sigmoid smoothly approximating a step function, but in this case the outputs range from minus one to one instead of just zero to one. And also we have the rectified"
        },
        {
            "start_time": 150,
            "end_time": 180,
            "transcription": " linear unit, which behaves like a linear activation function when its input is positive, but if the weighted sum of inputs is negative, the neuron will output zero. And because we will eventually be training our neural networks with gradient descent, we need to know the derivative of each of these activation functions. And for each of these functions, I've formulated the derivative in terms of"
        },
        {
            "start_time": 180,
            "end_time": 210,
            "transcription": " of the activation that the function outputs. We could also write down the derivative in terms of the input, but as we'll see when we derive gradient descent, we'll need to be storing the activations anyway. And so if we can formulate the derivative in terms of those stored activations, we can make our computations easier. So now we want to think about how do we actually perform a computation using a neural network?"
        },
        {
            "start_time": 210,
            "end_time": 240,
            "transcription": " to make a prediction on some data point. Well, we begin by using the data point to set the activations of the input layer to the network. These nodes I've illustrated in the input layer aren't really neurons. They're not computing anything. Instead, they are just storing the values in the input vector. All of the sub-values"
        },
        {
            "start_time": 240,
            "end_time": 270,
            "transcription": " The subsequent nodes in the hidden layers and the output layer are neurons performing this sort of computation. Because the job of these input nodes is just to store the point on which we're making a prediction, the dimension of our data directly determines the number of inputs to our network. So the network I have drawn is one that operates on four-dimensional inputs. Likewise,"
        },
        {
            "start_time": 270,
            "end_time": 300,
            "transcription": " Because we have two output nodes, we know that this network produces two-dimensional outputs, and so this network would be used for making predictions on a data set with four-dimensional observations and two-dimensional targets. So the size of the input and output layers are part of our hypothesis space specification, but as we'll see, the complexity of function that the network can"
        },
        {
            "start_time": 300,
            "end_time": 330,
            "transcription": " represent will depend on the number and connectivity of nodes and layers between the inputs and outputs. These neurons in between are known as hidden neurons, and they are typically organized into layers. The layer structure means that if we compute all of the activations for one layer, then we know that all of the inputs for the"
        },
        {
            "start_time": 330,
            "end_time": 360,
            "transcription": " next layer are now available. And so we'll often describe our neural networks in terms of the number and size of hidden layers. In this case, we have two hidden layers that each contain three neurons. So to compute a prediction with this neural network, we start by setting the activation of the input layer neurons equal"
        },
        {
            "start_time": 360,
            "end_time": 390,
            "transcription": " to the components of some data point. Then we will loop through the layers of the network, and at each layer we will compute each neuron's activation. As we know that computation does a weighted sum of inputs and then applies an activation function. And along the way we will store the activation that each neuron computes, because when we get to the next layer,"
        },
        {
            "start_time": 390,
            "end_time": 420,
            "transcription": " we will need to reference the previous activations to compute our weighted sum of inputs, and it turns out we'll also need those activations when we're performing gradient descent to update the weights. So with a network like this one, we can map our four-dimensional inputs to two-dimensional outputs, but why is that any better than just using"
        },
        {
            "start_time": 420,
            "end_time": 450,
            "transcription": " using two single neuron models to produce the two outputs. In other words, what's the benefit of messing around with these hidden layers? Well, it turns out that if we use linear activations for all of our neurons, there's actually no benefit at all. And the reason that hidden layers don't help if we only use linear neurons comes from"
        },
        {
            "start_time": 450,
            "end_time": 480,
            "transcription": " theorem you may have seen in linear algebra. If we have two functions that are both linear and we compose those functions, the result is another linear function. And so if we compute some linear function with this neuron and then pass it through some other linear function, we could have achieved the same thing by just using different weights."
        },
        {
            "start_time": 480,
            "end_time": 510,
            "transcription": " on a single neuron. I have not stated this theorem particularly carefully and I'm not going to prove it, but the consequence is that if we want to get any value out of multi-layer networks, we need to use some nonlinear activation function. But then the question becomes how much can we represent if we use nonlinear activations? Well, if we think back to our"
        },
        {
            "start_time": 510,
            "end_time": 540,
            "transcription": " single neuron models with sigmoid activations, we use the sigmoid activation because it approximated a Boolean step function. And if we go back to that simplification and think about a step function outputting zero or one, we can think about what Boolean functions can be computed. And this leads us to a second theorem that you may have encountered in a class that covers Boolean logic, which is that"
        },
        {
            "start_time": 540,
            "end_time": 570,
            "transcription": " And if we can build and or and not, and then compose them together, we can represent any Boolean function. And because computers are built using Boolean logic, any Boolean function really means any function we could possibly compute. Once again, I'm not going to prove this theorem, but I do want to show you how we could make"
        },
        {
            "start_time": 570,
            "end_time": 600,
            "transcription": " neurons that represent and or are not. If we plot the AND function on the plane that shows x1 versus x2, there are four possible inputs. The combinations of 0 or 1 for x1 or x2 and the AND function outputs a 1 if its inputs are both 1 and otherwise it outputs 0. So how could we build a neuron that computes this function?"
        },
        {
            "start_time": 600,
            "end_time": 630,
            "transcription": " Well, a step function classifier will split the plane into a region where it outputs zero and a region where it outputs one. So let's choose the weights that put the decision boundary here. And to get this decision boundary, we can use the following weights. If the weights on x1 and x2 are both one, and the bias is negative one and a half, then the only"
        },
        {
            "start_time": 630,
            "end_time": 660,
            "transcription": " way for the weighted sum of inputs to be positive is when x1 and x2 are both 1 and otherwise the neuron will output 0. Likewise, for the OR function, we can represent that on the 2D plane, but now if either or both of the inputs is 1, we output a 1, and for this we can construct a very similar neuron. Our decision boundary has just moved down by 1, and so we get weights of"
        },
        {
            "start_time": 660,
            "end_time": 690,
            "transcription": " of 1 and 1 and a bias of negative 1 half. For negation, we can think of it as a function that takes one input, so we only have a one-dimensional classifier. And here, we want to put our decision boundary at 0.5, and we want to output 0 if we're above the boundary, and 1 if we're below it. With a weight of negative 1, our step function"
        },
        {
            "start_time": 690,
            "end_time": 720,
            "transcription": " would have a negative sum of inputs for 1 and a 0 sum of inputs for 0. And with the threshold at minus 0.5, it will output 1 if the input is 0 and 0 if the input is 1. So if we were using a step function for our activation, then we could build neurons that represent and or a not. And since the sigmoid approximates the step function, we could use it to approximate and or a not. And so in principle, by changing the"
        },
        {
            "start_time": 720,
            "end_time": 750,
            "transcription": " together neurons that represent and or a nod, we could build a neural network that corresponds to a circuit to compute just about any function. And while there are some very old research papers that think about neural networks by analogy to circuits, none of the neural networks that I've been drawing or that you're likely to see look at all like a Boolean circuit. And that's because when using neural networks, we don't actually want to design circuits by hand"
        },
        {
            "start_time": 750,
            "end_time": 780,
            "transcription": " Instead, we want to train our neural networks using gradient descent. And for that, it turns out that a great tool is a neural network architecture consisting of multiple layers of nodes that are each densely connected to the next. So from here, we'd like to use gradient descent on some data set to train the parameters of the neural network so that it represents some interaction"
        },
        {
            "start_time": 780,
            "end_time": 810,
            "transcription": " When we had a single neuron, our parameters were the weights and the bias. And now that we have a network composed of many neurons, the model's parameters are all of the weights and all of the biases in the entire network. So if we were to write down the parameter vector for this neural network, it would contain the weight from neuron one to neuron"
        },
        {
            "start_time": 810,
            "end_time": 840,
            "transcription": " and from neuron one to neuron six, and so on for all of the weights, and it would contain the bias for every single node. So in total, we have four times three plus three times three plus three times two weights in the network, and we have three plus three plus two biases, meaning that in total this network has 35 parameters."
        },
        {
            "start_time": 840,
            "end_time": 870,
            "transcription": " And when we compute gradients, we will have a partial derivative for each of those parameters. But those partial derivatives are defined based on the loss function. And so we need to think about what is the loss function when we have a neural network with multiple output neurons. In a single neuron model, we had a loss function that depended on the neural function"
        },
        {
            "start_time": 870,
            "end_time": 900,
            "transcription": " neurons activation and the target, but now we need a loss function that incorporates the activation and the target for every neuron in the output layer. The first approach we'll consider for this is to generalize the mean squared error for a single neuron by simply summing over all the output neurons. So for any point J in the data set, the loss of the model on that point"
        },
        {
            "start_time": 900,
            "end_time": 930,
            "transcription": " is the sum over the output neurons of the square difference between the target and the activation. And note here that I'm using the subscripts to refer to which neuron we're talking about, and the superscripts to refer to some point in the data set. To get the loss for the data set, we want to take the mean"
        },
        {
            "start_time": 930,
            "end_time": 960,
            "transcription": " of these squared errors for all of the points. So with this definition of the loss on the data set, we can compute the gradient of the loss evaluated at the current parameters, then take a step in the minus gradient direction to update all of the weights and"
        },
        {
            "start_time": 960,
            "end_time": 979.1274375,
            "transcription": " the biases in a way that reduces the loss of the network. When we do this repeatedly, we'll be performing gradient descent. And in the next video, we'll go into much more detail on the back propagation algorithm for performing gradient descent updates on a neural network."
        }
    ]
}